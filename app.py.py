# app.py â€” Smart Zameen Dost (Gradio + Urdu OCR + RAG)
# =============================================================================
# Deploy on Hugging Face Spaces (Gradio). Add keys in "Settings â†’ Variables & secrets":
#   - ROBOFLOW_API_KEY
#   - WEATHER_API_KEY
#   - GROQ_API_KEY
#
# System deps (installed via packages.txt):
#   tesseract-ocr, tesseract-ocr-urd, poppler-utils, ffmpeg
# =============================================================================

import os
import io
import re
import json
import tempfile
from datetime import datetime, timedelta

import gradio as gr
import requests

# Whisper + TTS
import whisper
from gtts import gTTS

# Roboflow inference client
from inference_sdk import InferenceHTTPClient

# RAG stack
import PyPDF2
import nltk
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# OCR stack
from pdf2image import convert_from_bytes
import pytesseract

# Groq client
from groq import Groq


# ===========================
# ğŸ” API keys (from Secrets)
# ===========================
ROBOFLOW_API_KEY = os.getenv("ROBOFLOW_API_KEY", "").strip()
WEATHER_API_KEY  = os.getenv("WEATHER_API_KEY", "").strip()
GROQ_API_KEY     = os.getenv("GROQ_API_KEY", "").strip()

# Optional: point pytesseract to the binary if not on PATH (usually not needed on Spaces)
TESSERACT_CMD = os.getenv("TESSERACT_CMD", "").strip()
if TESSERACT_CMD:
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD

# ===========================
# ğŸ”§ Clients / Endpoints
# ===========================
ROBOFLOW_API_URL = "https://serverless.roboflow.com"
WEATHER_URL_NOW  = "https://api.openweathermap.org/data/2.5/weather"
WEATHER_URL_3H   = "https://api.openweathermap.org/data/2.5/forecast"

rf_client = None
if ROBOFLOW_API_KEY:
    try:
        rf_client = InferenceHTTPClient(api_url=ROBOFLOW_API_URL, api_key=ROBOFLOW_API_KEY)
    except Exception as e:
        print(f"[WARN] Roboflow client init failed: {e}")

groq_client = None
if GROQ_API_KEY:
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
    except Exception as e:
        print(f"[WARN] Groq client init failed: {e}")

# ===========================
# ğŸ”§ Global limits
# ===========================
K_RETRIEVE = 3
PER_DOC_CHARS = 700
MAX_CONTEXT_CHARS = 4000
MAX_OUTPUT_TOKENS = 512

# ===========================
# ğŸ”‡ NLTK (quiet)
# ===========================
try:
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)
except Exception:
    pass

# ===========================
# ğŸ§© Helpers
# ===========================
ARABIC_DIGITS_MAP = str.maketrans("Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹", "0123456789")
ZERO_WIDTH_RE = re.compile(r"[\u200B-\u200F\u202A-\u202E]")

def safe_str(x) -> str:
    if x is None:
        return ""
    if isinstance(x, (bool, int, float)):
        return str(x)
    return str(x)

def normalize_mixed_text(text: str) -> str:
    s = safe_str(text)
    s = ZERO_WIDTH_RE.sub("", s)
    s = s.translate(ARABIC_DIGITS_MAP)
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"[^\u0600-\u06FF\u0750-\u077F\uFB50-\uFDFF\uFE70-\uFEFF\w\s.,;:!?()\-]", " ", s)
    return s

def _limit_chars(s: str, n: int) -> str:
    s = str(s or "")
    return s if len(s) <= n else (s[:n] + " â€¦")

def _clip_context(snippets, max_chars: int) -> str:
    out, used = [], 0
    for snip in snippets:
        snip = str(snip or "")
        if used + len(snip) > max_chars:
            snip = snip[: max(0, max_chars - used)]
        if snip:
            out.append(snip)
            used += len(snip)
        if used >= max_chars:
            break
    return "\n\n".join(out)

def extract_numerical_data(text: str):
    t = safe_str(text)
    info = {}
    prices = re.findall(r"[$Rs\.]\s*(\d+(?:,\d{3})*(?:\.\d{2})?)", t)
    if prices:
        info["prices"] = prices
    perc = re.findall(r"(\d+(?:\.\d+)?)\s*%", t)
    if perc:
        info["percentages"] = perc
    yields = re.findall(
        r"(\d+(?:\.\d+)?)\s*(tons?|kg|quintals?|maunds?)\s*(?:per|/)?\s*(acre|hectare|Ø§ÛŒÚ©Ú‘)",
        t,
        re.IGNORECASE,
    )
    if yields:
        info["yields"] = yields
    return info

# ===========================
# ğŸŒ Urdu translation helper
# ===========================
def translate_weather(description: str) -> str:
    mapping = {
        "clear sky": "ØµØ§Ù Ø¢Ø³Ù…Ø§Ù†",
        "few clouds": "ÛÙ„Ú©Û’ Ø¨Ø§Ø¯Ù„",
        "scattered clouds": "Ú†Ú¾Ù¹Ù¾Ù¹ Ø¨Ø§Ø¯Ù„",
        "broken clouds": "Ù¹ÙˆÙ¹Û’ ÛÙˆØ¦Û’ Ø¨Ø§Ø¯Ù„",
        "shower rain": "Ø¨ÙˆÙ†Ø¯Ø§ Ø¨Ø§Ù†Ø¯ÛŒ",
        "rain": "Ø¨Ø§Ø±Ø´",
        "moderate rain": "Ø¯Ø±Ù…ÛŒØ§Ù†ÛŒ Ø¨Ø§Ø±Ø´",
        "light rain": "ÛÙ„Ú©ÛŒ Ø¨Ø§Ø±Ø´",
        "thunderstorm": "Ø¢Ù†Ø¯Ú¾ÛŒ Ø§ÙˆØ± Ú¯Ø±Ø¬ Ú†Ù…Ú©",
        "snow": "Ø¨Ø±ÙØ¨Ø§Ø±ÛŒ",
        "mist": "Ø¯Ú¾Ù†Ø¯",
        "overcast clouds": "Ù…Ú©Ù…Ù„ Ø¨Ø§Ø¯Ù„",
    }
    return mapping.get(safe_str(description).lower(), description)

# ===========================
# ğŸ¦  Plant disease detection
# ===========================
def predict_disease(image_path: str):
    if not image_path:
        return "âŒ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ù¾ØªÛ’ Ú©ÛŒ ØªØµÙˆÛŒØ± Ø§Ù¾ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº"
    if not rf_client:
        return "âŒ Roboflow API key Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’"
    try:
        res = rf_client.infer(image_path, model_id="plant-disease-detection-v2-2nclk/1")
        preds = res.get("predictions", [])
        if not preds:
            return "âŒ Ú©ÙˆØ¦ÛŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù…Ø¹Ù„ÙˆÙ… Ù†ÛÛŒÚº ÛÙˆØ¦ÛŒ"
        top = preds[0]
        cls = top.get("class", "Ù†Ø§Ù…Ø¹Ù„ÙˆÙ…")
        conf = float(top.get("confidence", 0.0)) * 100
        return f"ğŸ¦  Ø¨ÛŒÙ…Ø§Ø±ÛŒ: {cls}\nØ§Ø¹ØªÙ…Ø§Ø¯: {conf:.2f}%"
    except Exception as e:
        return f"âŒ Ø®Ø±Ø§Ø¨ÛŒ: {e}"

# ===========================
# â˜ï¸ Weather (Urdu)
# ===========================
def get_current_weather(city: str):
    city = (city or "").strip()
    if not city:
        return "âŒ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù… Ø¯Ø±Ø¬ Ú©Ø±ÛŒÚº"
    if not WEATHER_API_KEY:
        return "âŒ Weather API key Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’"
    try:
        params = {"q": city, "appid": WEATHER_API_KEY, "units": "metric", "lang": "en"}
        r = requests.get(WEATHER_URL_NOW, params=params, timeout=20)
        data = r.json()
        if data.get("cod") != 200:
            return "âŒ Ù…ÙˆØ³Ù… Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø§ØµÙ„ Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©ÛŒÚº"
        desc_en   = data["weather"][0]["description"]
        desc_ur   = translate_weather(desc_en)
        temp      = data["main"]["temp"]
        feels     = data["main"]["feels_like"]
        humidity  = data["main"]["humidity"]
        wind      = data["wind"].get("speed", 0)
        return (
            f"ğŸŒ¤ï¸ Ù…ÙˆØ¬ÙˆØ¯Û Ù…ÙˆØ³Ù…: {desc_ur}\n"
            f"ğŸŒ¡ï¸ Ø¯Ø±Ø¬Û Ø­Ø±Ø§Ø±Øª: {temp}Â°C (Ù…Ø­Ø³ÙˆØ³: {feels}Â°C)\n"
            f"ğŸ’§ Ù†Ù…ÛŒ: {humidity}%\n"
            f"ğŸ’¨ ÛÙˆØ§ Ú©ÛŒ Ø±ÙØªØ§Ø±: {wind} m/s"
        )
    except Exception as e:
        return f"âŒ Ù…ÙˆØ³Ù… Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {e}"

def get_tomorrow_forecast(city: str):
    city = (city or "").strip()
    if not city:
        return "âŒ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù… Ø¯Ø±Ø¬ Ú©Ø±ÛŒÚº"
    if not WEATHER_API_KEY:
        return "âŒ Weather API key Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’"
    try:
        params = {"q": city, "appid": WEATHER_API_KEY, "units": "metric", "lang": "en"}
        r = requests.get(WEATHER_URL_3H, params=params, timeout=20)
        data = r.json()
        if data.get("cod") != "200":
            return "âŒ Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù… Ø¯Ø±Ø³Øª Ù†ÛÛŒÚº ÛŒØ§ Ù…ÙˆØ³Ù… Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ø³ØªÛŒØ§Ø¨ Ù†ÛÛŒÚº"
        tomorrow = (datetime.utcnow() + timedelta(days=1)).date()
        slots = []
        for entry in data.get("list", []):
            ts = datetime.utcfromtimestamp(entry["dt"])
            if ts.date() == tomorrow:
                desc_en = entry["weather"][0]["description"]
                slots.append({
                    "time": ts.strftime("%I:%M %p"),
                    "desc": translate_weather(desc_en),
                    "temp": entry["main"]["temp"],
                    "feels": entry["main"]["feels_like"],
                    "humidity": entry["main"]["humidity"]
                })
        if not slots:
            return "âŒ Ú©Ù„ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ¦ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ Ø¯Ø³ØªÛŒØ§Ø¨ Ù†ÛÛŒÚº"
        out = [f"ğŸ“… Ú©Ù„ ({tomorrow}) Ú©Û’ Ù…ÙˆØ³Ù… Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ:\n"]
        for s in slots:
            out.append(
                f"ğŸ•’ {s['time']} â€” {s['desc']}\n"
                f"   ğŸŒ¡ï¸ {s['temp']}Â°C (Ù…Ø­Ø³ÙˆØ³: {s['feels']}Â°C) | ğŸ’§ Ù†Ù…ÛŒ: {s['humidity']}%"
            )
        return "\n".join(out)
    except Exception as e:
        return f"âŒ Ø®Ø±Ø§Ø¨ÛŒ: {e}"

# ===========================
# ğŸ”— Combined function
# ===========================
def advisory_app(image, city):
    disease = predict_disease(image)
    now     = get_current_weather(city)
    tomo    = get_tomorrow_forecast(city)
    return disease, now, tomo

# =========================
# ğŸ–¼ï¸ OCR config
# =========================
OCR_LANGS = "urd+eng"
OCR_DPI = 300
MAX_OCR_PAGES = None

def ocr_pdf_bytes(pdf_content: bytes, dpi: int = OCR_DPI, langs: str = OCR_LANGS, max_pages=MAX_OCR_PAGES):
    images = convert_from_bytes(pdf_content, dpi=dpi)
    if max_pages is not None:
        images = images[:max_pages]
    out = []
    for i, img in enumerate(images, start=1):
        try:
            txt = pytesseract.image_to_string(img, lang=langs)
            txt = normalize_mixed_text(txt)
            out.append(f"\n--- OCR Page {i} ---\n{txt}\n" if txt else f"\n--- OCR Page {i} (no text) ---\n")
        except Exception as e:
            out.append(f"\n--- OCR Page {i} (error: {e}) ---\n")
    return "".join(out), len(images)

# =========================
# ğŸ“¥ Google Drive PDF Processor (with OCR fallback)
# =========================
class GoogleDrivePDFProcessor:
    @staticmethod
    def convert_gdrive_link(share_link: str):
        patterns = [r"/file/d/([a-zA-Z0-9\-_]+)", r"id=([a-zA-Z0-9\-_]+)", r"/d/([a-zA-Z0-9\-_]+)"]
        file_id = None
        link = safe_str(share_link)
        for pat in patterns:
            m = re.search(pat, link)
            if m:
                file_id = m.group(1); break
        if not file_id:
            return None
        return f"https://drive.google.com/uc?export=download&id={file_id}"

    @staticmethod
    def download_pdf_from_gdrive(gdrive_link: str):
        try:
            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)
            if not download_link:
                return None, "Invalid Google Drive link format"
            headers = {"User-Agent": "Mozilla/5.0"}
            resp = requests.get(download_link, headers=headers, stream=True, timeout=60)
            txt = safe_str(resp.text)
            if ("confirm=" in txt) or ("virus scan warning" in txt.lower()):
                token = re.search(r"confirm=([^&]+)", txt)
                if token:
                    confirmed = f"{download_link}&confirm={token.group(1)}"
                    resp = requests.get(confirmed, headers=headers, stream=True, timeout=60)
            if resp.status_code == 200:
                return resp.content, "Success"
            return None, f"Download failed: HTTP {resp.status_code}"
        except Exception as e:
            return None, f"Download error: {e}"

    @staticmethod
    def extract_text_from_pdf(pdf_content: bytes):
        text = ""; pages = 0
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
            pages = len(reader.pages)
            out = []
            for i in range(pages):
                try:
                    pg = reader.pages[i]
                    t = normalize_mixed_text(pg.extract_text() or "")
                    out.append(f"\n--- Page {i+1} ---\n{t}\n" if t else f"\n--- Page {i+1} (no text) ---\n")
                except Exception:
                    out.append(f"\n--- Page {i+1} (Error extracting) ---\n")
            text = "".join(out)
        except Exception as e:
            text = f"PDF text extraction error: {e}"
            pages = 0

        core = re.sub(r"\s+", "", safe_str(text))
        if (pages == 0) or (len(core) < 100):
            try:
                ocr_text, ocr_pages = ocr_pdf_bytes(pdf_content, dpi=OCR_DPI, langs=OCR_LANGS, max_pages=MAX_OCR_PAGES)
                if len(re.sub(r"\s+", "", ocr_text)) >= 20:
                    return ocr_text, (ocr_pages or pages or 0)
            except Exception:
                pass
        return text, pages

# =========================
# ğŸ§  Knowledge Base / RAG
# =========================
class AdvancedPakistaniAgriRAG:
    def __init__(self, predefined_links=None):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.vector_store = None
        self.gdrive = GoogleDrivePDFProcessor()
        self.processed_documents = []
        self._setup_seed_knowledge()
        if predefined_links:
            self._auto_process_predefined_pdfs(predefined_links)

    def _setup_seed_knowledge(self):
        seed = [
            {
                "content": """Punjab Wheat Varieties for Export:
                Ø§Ø¹Ù„ÛŒÙ° Ù‚Ø³Ù… Ú©ÛŒ Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ø§Ù‚Ø³Ø§Ù…:
                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton
                - Faisalabad-2008: High protein 12-14%, Premium export variety
                - Galaxy-2013: Disease resistant, Suitable for UAE market
                - Punjab-2011: Good for bread making, Export to Afghanistan
                Ø§Ø±Ø¯Ùˆ: ÛŒÛ Ø§Ù‚Ø³Ø§Ù… Ø¨Ø±Ø¢Ù…Ø¯ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ† ÛÛŒÚº Ø§ÙˆØ± Ø²ÛŒØ§Ø¯Û Ù‚ÛŒÙ…Øª Ù…Ù„ØªÛŒ ÛÛ’""",
                "metadata": {"type": "crop_varieties", "region": "Punjab", "crop": "wheat", "language": "mixed"},
            },
            {
                "content": """Rice Export Opportunities - Ú†Ø§ÙˆÙ„ Ú©ÛŒ Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª:
                Basmati Varieties with International Prices:
                - Super Basmati: 1000-1300/ton
                - IRRI-6: 700-850/ton (Middle East markets)
                Export Requirements:
                - Moisture: Maximum 14%
                - Broken grains: Less than 5%
                - Length: Minimum 6.0mm for Basmati
                Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº: Ø¨Ø³Ù…ØªÛŒ Ú†Ø§ÙˆÙ„ Ú©ÛŒ Ø¨Ø±Ø¢Ù…Ø¯ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù†Ø§ÙØ¹ Ø¨Ø®Ø´ ÛÛ’""",
                "metadata": {"type": "export_markets", "crop": "rice", "price_range": "450-1300", "language": "mixed"},
            },
            {
                "content": """Government Support Schemes - Ø­Ú©ÙˆÙ…ØªÛŒ Ø§Ø³Ú©ÛŒÙ…Ø²:
                Kisan Card Program:
                - 25% subsidy on fertilizers
                - 20% discount on certified seeds
                - Easy loan access through banks
                Solar Tube Well Scheme:
                - 60% government subsidy
                - Remaining 40% through easy installments
                - Electricity bill savings: Rs. 50,000+ annually
                Crop Insurance Program:
                - Premium: 5% of sum insured
                - Government pays 75% of premium
                - Coverage: Natural disasters, pest attacks
                Ú©Ø³Ø§Ù† ÚˆÙˆÛŒÙ„Ù¾Ù…Ù†Ù¹ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù… Ø³Û’ Ù…ÙØª ØªØ±Ø¨ÛŒØª Ø§ÙˆØ± Ù…Ø´ÙˆØ±Û’""",
                "metadata": {"type": "government_schemes", "schemes": "kisan_card,solar_tubewell,crop_insurance", "language": "mixed"},
            },
        ]

        docs = []
        for item in seed:
            content = normalize_mixed_text(item["content"])
            meta = dict(item.get("metadata") or {})
            nums = extract_numerical_data(content)
            if nums:
                meta.update(nums)
            docs.append(Document(page_content=content, metadata=meta))

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800, chunk_overlap=100,
            separators=["\n\n", "\n", "Û”", ".", ":", ";", " "],
            length_function=len,
        )
        pieces = splitter.split_documents(docs)
        self.vector_store = FAISS.from_documents(pieces, self.embeddings)
        print("âœ… Seed agricultural knowledge initialized with", len(pieces), "chunks.")

    def _auto_process_predefined_pdfs(self, links):
        print(f"ğŸš€ Auto-processing {len(links)} Google Drive PDF(s)...")
        ok = 0
        for i, link in enumerate(links, start=1):
            try:
                blob, msg = self.gdrive.download_pdf_from_gdrive(link)
                if blob is None:
                    print(f"âŒ Doc {i}: {msg}")
                    self.processed_documents.append({"id": i, "pages": 0, "chunks": 0, "source": link, "status": msg})
                    continue
                text, pages = self.gdrive.extract_text_from_pdf(blob)
                if "pdf text extraction error" in safe_str(text).lower():
                    print(f"âŒ Doc {i}: {text}")
                    self.processed_documents.append({"id": i, "pages": 0, "chunks": 0, "source": link, "status": text})
                    continue
                if len(safe_str(text).strip()) < 100:
                    print(f"â„¹ï¸ Doc {i}: low native text â€” OCR likely used.")
                processed = normalize_mixed_text(text)
                numbers = extract_numerical_data(processed)
                doc = Document(
                    page_content=processed,
                    metadata={
                        "type": "auto_processed_pdf",
                        "source": f"Auto PDF {i}",
                        "pages": pages,
                        "numerical_data": numbers,
                        "processing_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "original_link": link[:50] + "..." if len(link) > 50 else link,
                    },
                )
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800, chunk_overlap=100,
                    separators=["\n\n", "\n", "Û”", ".", ":", ";", " "]
                )
                chunks = splitter.split_documents([doc])
                if self.vector_store:
                    self.vector_store.add_documents(chunks)
                else:
                    self.vector_store = FAISS.from_documents(chunks, self.embeddings)
                self.processed_documents.append(
                    {"id": i, "pages": pages, "chunks": len(chunks), "source": doc.metadata["original_link"], "status": "âœ… Success"}
                )
                print(f"âœ… Doc {i}: {pages} pages â†’ {len(chunks)} chunks")
                ok += 1
            except Exception as e:
                self.processed_documents.append(
                    {"id": i, "pages": 0, "chunks": 0, "source": link[:50] + "..." if len(link) > 50 else link, "status": f"âŒ Error: {e}"}
                )
                print(f"âŒ Doc {i}: {e}")
        print(f"ğŸ‰ Finished: {ok}/{len(links)} document(s) processed.")

    def get_stats_html(self) -> str:
        if not self.processed_documents:
            return "ğŸ“Š Knowledge Base: Seed Pakistani agricultural data only (no PDFs yet)"
        total_chunks = sum(d.get("chunks", 0) for d in self.processed_documents)
        total_pages = sum(d.get("pages", 0) for d in self.processed_documents)
        return f"""ğŸ“Š Knowledge Base Statistics:

ğŸ—‚ï¸ Auto-processed Documents: {len(self.processed_documents)}
ğŸ“„ Total Pages Processed: {total_pages}
ğŸ§© Total Text Chunks: {total_chunks}
ğŸ“š Seed Knowledge: Pakistani agriculture (Urdu + English)
ğŸ” Search Capability: Multilingual (English + Urdu)
âœ… Status: Ready for queries
"""

    def get_relevant_info(self, query: str, k: int = K_RETRIEVE) -> str:
        if not self.vector_store:
            return "Knowledge base not available"
        try:
            q = normalize_mixed_text(query)
            hits = self.vector_store.similarity_search(q, k=k)
            snippets = []; nums_summary = []
            for i, doc in enumerate(hits, start=1):
                body = _limit_chars(doc.page_content, PER_DOC_CHARS)
                snippets.append(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª {i}: {body}")
                meta = doc.metadata or {}
                if not isinstance(meta, dict):
                    meta = {}
                nd = meta.get("numerical_data")
                if isinstance(nd, dict):
                    meta = {**meta, **nd}
                if isinstance(meta.get("prices"), list) and meta["prices"]:
                    nums_summary.append(f"ğŸ’° Ù‚ÛŒÙ…ØªÛŒÚº: {', '.join(map(safe_str, meta['prices']))}")
                if isinstance(meta.get("percentages"), list) and meta["percentages"]:
                    nums_summary.append(f"ğŸ“Š ÙÛŒØµØ¯: {', '.join(map(safe_str, meta['percentages']))}%")
                if isinstance(meta.get("yields"), list) and meta["yields"]:
                    y_fmt = []
                    for y in meta["yields"]:
                        try:
                            val, unit, per = y
                            y_fmt.append(f"{val} {unit} per {per}")
                        except Exception:
                            y_fmt.append(safe_str(y))
                    nums_summary.append(f"ğŸŒ¾ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø±: {', '.join(y_fmt)}")
            context = "\n\n".join(snippets)
            if nums_summary:
                context = "ğŸ“ˆ Ø§ÛÙ… Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø´Ù…Ø§Ø±:\n" + "\n".join(nums_summary) + "\n\n" + context
            return _clip_context([context], MAX_CONTEXT_CHARS) or "No relevant information found."
        except Exception as e:
            return f"Error retrieving information: {e}"

# ===========================
# ğŸ¤– Whisper model
# ===========================
print("ğŸ¤– Loading Whisper model...")
whisper_model = whisper.load_model("base")
print("âœ… Whisper model loaded.")

# ===========================
# ğŸ“„ Optional seed PDFs (can be empty)
# ===========================
PREDEFINED_PDF_LINKS = []

# ===========================
# ğŸš€ Initialize RAG
# ===========================
print("ğŸ§  Initializing Advanced Pakistani Agricultural Knowledge Base...")
pak_agri_rag = AdvancedPakistaniAgriRAG(predefined_links=PREDEFINED_PDF_LINKS)

# ===========================
# ğŸ™ï¸ Voice, ğŸŒ¦ï¸ Weather, ğŸ¤ AI
# ===========================
def voice_to_text(audio_file_path):
    if not audio_file_path:
        return ""
    try:
        result = whisper_model.transcribe(audio_file_path, language="ur")
        return normalize_mixed_text(result.get("text", ""))
    except Exception as e:
        return f"Ø¢ÙˆØ§Ø² Ø³Ù…Ø¬Ú¾ Ù†ÛÛŒÚº Ø¢Ø¦ÛŒ: {e}"

def get_weather_with_farming_advice(city="Lahore"):
    try:
        city = safe_str(city).strip() or "Lahore"
        if not WEATHER_API_KEY:
            return "Weather API key Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’"
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric"
        resp = requests.get(url, timeout=20)
        try:
            data = resp.json()
        except Exception:
            return "Ù…ÙˆØ³Ù…ÛŒ JSON Ø¯Ø±Ø³Øª Ù†ÛÛŒÚºÛ”"
        main = data.get("main") or {}
        wind = data.get("wind") or {}
        weather_l = data.get("weather") or [{}]
        temp = main.get("temp"); humidity = main.get("humidity")
        wind_speed = wind.get("speed"); description = weather_l[0].get("description", "")
        if any(v is None for v in (temp, humidity, wind_speed)):
            return "Ù…ÙˆØ³Ù…ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ú©Ù…Ù„ Ù†ÛÛŒÚº Ù…Ù„ Ø³Ú©ÛŒÚºÛ”"
        if temp > 35:
            advice = f"âš ï¸ Ø²ÛŒØ§Ø¯Û Ú¯Ø±Ù…ÛŒ ({temp}Â°C): ØµØ¨Ø­ 6-8 Ø¨Ø¬Û’ Ù¾Ø§Ù†ÛŒ Ø¯ÛŒÚºØŒ Ø¯ÙˆÙ¾ÛØ± Ù…ÛŒÚº Ù†ÛÛŒÚºÛ” Ù¾Ø§Ù†ÛŒ Ú©ÛŒ Ù…Ù‚Ø¯Ø§Ø± 20% Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚºÛ”"
        elif humidity > 80:
            advice = f"ğŸŒ§ï¸ Ø²ÛŒØ§Ø¯Û Ù†Ù…ÛŒ ({humidity}%): ÙÙ†Ú¯ÛŒØ³Ø§Ø¦ÛŒÚˆ Ø³Ù¾Ø±Û’ Ú©Ø±ÛŒÚºÛ” Mancozeb 2g/Ù„ÛŒÙ¹Ø± ÛŒØ§ Copper Oxychloride 3g/Ù„ÛŒÙ¹Ø±Û”"
        elif temp < 10:
            advice = f"â„ï¸ Ø³Ø±Ø¯ÛŒ ({temp}Â°C): Ù¾ÙˆØ¯ÙˆÚº Ú©Ùˆ ÚˆÚ¾Ø§Ù†Ù¾ÛŒÚºØŒ Ù¾Ø§Ù†ÛŒ 50% Ú©Ù… Ø¯ÛŒÚºÛ” Frost protection Ø¶Ø±ÙˆØ±ÛŒÛ”"
        elif wind_speed > 5:
            advice = f"ğŸ’¨ ØªÛŒØ² ÛÙˆØ§ ({wind_speed} m/s): Ú©ÛŒÚ‘Û’ Ù…Ø§Ø± Ø¯ÙˆØ§ Ú©Ø§ Ø³Ù¾Ø±Û’ Ù†Û Ú©Ø±ÛŒÚºÛ” Wind barriers Ù„Ú¯Ø§Ø¦ÛŒÚºÛ”"
        else:
            advice = f"âœ… Ù…ÙˆØ³Ù… Ø§Ú†Ú¾Ø§ ÛÛ’ ({temp}Â°C, {humidity}% Ù†Ù…ÛŒ): Ú©Ú¾ÛŒØªÛŒ Ú©Û’ Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”"
        return f"Ø¢Ø¬ {city} Ù…ÛŒÚº {temp}Â°CØŒ Ù†Ù…ÛŒ {humidity}%ØŒ ÛÙˆØ§ {wind_speed} m/sØŒ Ù…ÙˆØ³Ù… {description}\n\n{advice}"
    except Exception as e:
        return f"Ù…ÙˆØ³Ù…ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÛÛŒÚº Ù…Ù„ Ø³Ú©ÛŒÚº: {e}"

def text_to_voice(text):
    try:
        clean = normalize_mixed_text(text)
        if len(clean) > 500:
            clean = clean[:500] + "... Ù…Ú©Ù…Ù„ Ø¬ÙˆØ§Ø¨ Ø§ÙˆÙ¾Ø± Ù¾Ú‘Ú¾ÛŒÚº"
        tts = gTTS(text=clean, lang="ur", slow=False)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            tts.save(tmp.name)
            return tmp.name
    except Exception as e:
        print(f"TTS Error: {e}")
        return None

def get_enhanced_ai_response(user_message: str, location: str = "") -> str:
    relevant_context = pak_agri_rag.get_relevant_info(user_message)
    system_prompt = (
        "You are Zameen Dost, a Pakistani agriculture advisor. "
        "Answer in simple Urdu, start with 'Ø¨Ú¾Ø§Ø¦ÛŒ', use numbers when available, "
        "and keep it concise and actionable. If weather is included, integrate it. "
        "Only use the provided context; do not invent facts."
    )
    prompt_user = (
        f"Context:\n{relevant_context}\n\n"
        f"Location: {safe_str(location)}\n"
        f"Question: {safe_str(user_message)}"
    )
    if not groq_client:
        return "âš ï¸ GROQ_API_KEY Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ AI Ø¬ÙˆØ§Ø¨ Ù…Ø­Ø¯ÙˆØ¯ ÛÛ’Û”"
    try:
        chat = groq_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_user},
            ],
            model="llama-3.1-8b-instant",
            max_tokens=MAX_OUTPUT_TOKENS,
            temperature=0.5,
        )
        return chat.choices[0].message.content
    except Exception as e:
        msg = safe_str(e)
        if ("rate_limit" in msg) or ("tokens per minute" in msg) or ("Request too large" in msg):
            return "Ù…Ø¹Ø°Ø±ØªØŒ Ù¾ÛŒØºØ§Ù… Ø¨Ú‘Ø§ ØªÚ¾Ø§ ÛŒØ§ Ø±ÙØªØ§Ø± Ø­Ø¯ Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªÚ¾ÛŒÛ” Ø¨Ø±Ø§ÛÙ Ú©Ø±Ù… Ú†Ú¾ÙˆÙ¹Ø§ Ø³ÙˆØ§Ù„ Ú©Ø±ÛŒÚºØŒ ÛŒØ§ Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºÛ”"
        return f"Ù…Ø¹Ø°Ø±ØªØŒ AI Ø³Û’ Ø±Ø§Ø¨Ø·Û Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©Ø§: {e}"

# ===========================
# ğŸ’¬ Main chat handler
# ===========================
def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):
    user_message = ""; input_display = ""
    if audio_input:
        user_message = voice_to_text(audio_input)
        input_display = f"ğŸ’¬ Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: {user_message}"
    elif text_input:
        user_message = safe_str(text_input)
        input_display = f"ğŸ’¬ Ø¢Ù¾ Ù†Û’ Ù„Ú©Ú¾Ø§: {user_message}"
    if not isinstance(user_message, str) or not user_message.strip():
        return "Ú©Ø±Ù¾ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ†Ú¾ÛŒÚº", None, "âŒ Ú©ÙˆØ¦ÛŒ Ø³ÙˆØ§Ù„ Ù†ÛÛŒÚº Ù…Ù„Ø§"
    enhanced = user_message
    if focus_area and safe_str(focus_area) != "Ø¹Ø§Ù… Ø³ÙˆØ§Ù„":
        enhanced += f" (Ú©Ø³Ø§Ù† Ú©ÛŒ Ø¯Ù„Ú†Ø³Ù¾ÛŒ: {focus_area})"
    terms = ["Ù…ÙˆØ³Ù…", "Ø¨Ø§Ø±Ø´", "Ù¾Ø§Ù†ÛŒ", "weather", "irrigation", "spray", "Ø³Ù¾Ø±Û’"]
    if isinstance(user_message, str) and any(t in user_message for t in terms):
        weather_info = get_weather_with_farming_advice(city_name or "Lahore")
        enhanced += f"\n\nÙ…ÙˆØ³Ù…ÛŒ Ø­Ø§Ù„Ø§Øª: {weather_info}"
    ai_response = get_enhanced_ai_response(enhanced, city_name or "")
    voice_response = text_to_voice(ai_response)
    return input_display, voice_response, ai_response

# =========================
# ğŸ–¥ï¸ Professional Gradio UI
# =========================
CUSTOM_CSS = """
:root {
  --brand:#2E8B57; --brand-2:#1f6f45; --bg:#f6faf9; --card:#ffffff; --muted:#6b7280; --ring: rgba(46,139,87,0.2);
}
.gradio-container {
  background: radial-gradient(1200px 600px at 10% -10%, #eaf7f0 0%, transparent 50%), var(--bg);
  font-family: ui-sans-serif, system-ui, -apple-system, "Noto Nastaliq Urdu", "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
  color: #0f172a;
}
a { color: var(--brand); }
.header {
  background: linear-gradient(135deg, #ffffff 0%, #f0faf5 100%);
  border: 1px solid #e6f2ea; border-left: 4px solid var(--brand);
  border-radius: 14px; padding: 18px 20px; box-shadow: 0 2px 10px rgba(16,24,40,0.04);
  margin: 8px 0 18px;
}
.header h1 { display:flex; align-items:center; gap:12px; margin:0; font-size: 26px; }
.header .tag { background: #eaf6f0; color: var(--brand-2); border-radius: 999px; padding: 4px 10px; font-size: 12px; }
.header p { margin: 6px 0 0; color: var(--muted); }
.card {
  background: var(--card); border: 1px solid #eef3ef; border-radius: 14px;
  box-shadow: 0 2px 12px rgba(15,23,42,0.04); padding: 16px; margin-bottom: 14px;
}
.card h3 { margin: 0 0 10px; font-size: 16px; color: #0f172a; }
.card .hint { color: var(--muted); font-size: 13px; }
button, .gr-button-primary { border-radius: 10px !important; box-shadow: 0 1px 2px rgba(0,0,0,0.04); }
button.primary, .gr-button-primary { background: var(--brand) !important; border: 1px solid #1f6f45 !important; }
button.primary:hover, .gr-button-primary:hover { background: var(--brand-2) !important; }
input:focus, textarea:focus, .gr-textbox:focus-within, .gr-textbox textarea:focus { outline: none !important; box-shadow: 0 0 0 4px var(--ring) !important; }
.footer { color: var(--muted); font-size: 12px; text-align: center; margin-top: 8px; }
"""

def _kb_stats_html():
    try:
        return pak_agri_rag.get_stats_html()
    except Exception:
        return "ğŸ“Š Knowledge Base: (not initialized in this session)"

with gr.Blocks(
    title="Smart Zameen Dost - Ø²Ù…ÛŒÙ† Ø¯ÙˆØ³Øª",
    theme=gr.themes.Soft(primary_hue="green", neutral_hue="gray"),
    css=CUSTOM_CSS
) as app:
    gr.HTML("""
        <div class="header">
          <h1>ğŸŒ¾ Ø²Ù…ÛŒÙ† Ø¯ÙˆØ³Øª <span class="tag">Ø§Ø±Ø¯Ùˆ + Ø¢Ø± Ø§Û’ Ø¬ÛŒ + Ø§Ùˆ Ø³ÛŒ Ø¢Ø±</span></h1>
          <p>Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ú©Ø³Ø§Ù†ÙˆÚº Ú©Û’ Ù„ÛŒÛ’: Ø°ÛÛŒÙ† Ù…Ø´ÙˆØ±ÛØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©ÛŒ ØªØ´Ø®ÛŒØµØŒ Ø§ÙˆØ± Ù…ÙˆØ³Ù… Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ</p>
        </div>
    """)

    with gr.Tabs():
        with gr.TabItem("Ú†ÛŒÙ¹ Ø§Ø³Ø³Ù¹Ù†Ù¹"):
            with gr.Row():
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ú©Ø±ÛŒÚº")
                        audio_input = gr.Audio(sources=["microphone"], type="filepath", label="Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ù¾ÙˆÚ†Ú¾ÛŒÚº", interactive=True)
                        text_input = gr.Textbox(label="ÛŒØ§ ÛŒÛØ§Úº Ù„Ú©Ú¾ÛŒÚº (Ø§Ø±Ø¯Ùˆ / English)", placeholder="Ù…Ø«Ø§Ù„: Ø¨Ø±Ø¢Ù…Ø¯ÛŒ Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ø§Ù‚Ø³Ø§Ù… Ú©ÛŒØ§ ÛÛŒÚºØŸ", lines=3)
                        with gr.Row():
                            city_input = gr.Textbox(label="Ø¢Ù¾ Ú©Ø§ Ø´ÛØ±", placeholder="Lahore, Karachi, Faisalabad", value="Lahore")
                            focus_area = gr.Dropdown(
                                label="Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ú©Ø§ Ø´Ø¹Ø¨Û",
                                choices=["Ø¹Ø§Ù… Ø³ÙˆØ§Ù„","Ø¨Ø±Ø¢Ù…Ø¯ÛŒ ÙØµÙ„ÛŒÚº","Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ú©Ø§Ø´Øª","Ú†Ø§ÙˆÙ„ Ú©ÛŒ Ú©Ø§Ø´Øª","Ú©Ù¾Ø§Ø³ Ú©ÛŒ Ú©Ø§Ø´Øª","Ø³Ø¨Ø²ÛŒÙˆÚº Ú©ÛŒ Ú©Ø§Ø´Øª","Ù¾Ú¾Ù„ÙˆÚº Ú©ÛŒ Ú©Ø§Ø´Øª","Ú©Ú¾Ø§Ø¯ Ø§ÙˆØ± Ø¨ÛŒØ¬","Ø¨ÛŒÙ…Ø§Ø±ÛŒÙˆÚº Ú©Ø§ Ø¹Ù„Ø§Ø¬","Ø­Ú©ÙˆÙ…ØªÛŒ Ø§Ø³Ú©ÛŒÙ…Ø²","Ù…Ù†ÚˆÛŒ Ú©ÛŒ Ù‚ÛŒÙ…ØªÛŒÚº"],
                                value="Ø¹Ø§Ù… Ø³ÙˆØ§Ù„",
                            )
                        chat_btn = gr.Button("Ø¬ÙˆØ§Ø¨ Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº", variant="primary", elem_classes=["primary"])
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### Ø°ÛÛŒÙ† Ø¬ÙˆØ§Ø¨")
                        input_display = gr.Textbox(label="Ø¢Ù¾ Ú©Ø§ Ø³ÙˆØ§Ù„", lines=2, interactive=False)
                        audio_output = gr.Audio(label="Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨")
                        text_output = gr.Textbox(label="ØªÙØµÛŒÙ„ÛŒ Ø¬ÙˆØ§Ø¨", lines=12, interactive=False, show_copy_button=True)

            with gr.Row():
                with gr.Column():
                    gr.HTML(value=_kb_stats_html(), elem_classes=["card"])

            chat_btn.click(zameen_dost_advanced_chat, inputs=[audio_input, text_input, city_input, focus_area], outputs=[input_display, audio_output, text_output])

        with gr.TabItem("Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©ÛŒ Ø´Ù†Ø§Ø®Øª"):
            with gr.Row():
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### ØªØµÙˆÛŒØ± Ø§Ù¾ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº")
                        image_input = gr.Image(label="Ù¾ØªÛ’/Ù¾ÙˆØ¯Û’ Ú©ÛŒ ØªØµÙˆÛŒØ±", type="filepath", height=280)
                        run_disease = gr.Button("ğŸ” Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±ÛŒÚº", variant="primary", elem_classes=["primary"])
                        gr.Markdown('<div class="hint">ğŸ“Œ ÙˆØ§Ø¶Ø­ØŒ Ù‚Ø±ÛŒØ¨ Ø³Û’ Ù„ÛŒ Ú¯Ø¦ÛŒ ØªØµÙˆÛŒØ± Ø¨ÛØªØ± Ù†ØªØ§Ø¦Ø¬ Ø¯ÛŒØªÛŒ ÛÛ’</div>')
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### Ù†ØªÛŒØ¬Û")
                        disease_out = gr.Textbox(label="Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ", lines=6, interactive=False)

            run_disease.click(lambda img: predict_disease(img), inputs=[image_input], outputs=[disease_out])

        with gr.TabItem("Ù…ÙˆØ³Ù…"):
            with gr.Row():
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### Ø´ÛØ± Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº")
                        city_input2 = gr.Textbox(label="Ø¢Ù¾ Ú©Ø§ Ø´ÛØ±", placeholder="Lahore, Karachi, Faisalabad", value="Lahore")
                        with gr.Row():
                            run_now = gr.Button("Ù…ÙˆØ¬ÙˆØ¯Û Ù…ÙˆØ³Ù…", variant="primary", elem_classes=["primary"])
                            run_tomo = gr.Button("Ú©Ù„ Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ", variant="secondary")
                with gr.Column(scale=6):
                    with gr.Group(elem_classes=["card"]):
                        gr.Markdown("### Ù…ÙˆØ³Ù…ÛŒ ØªÙØµÛŒÙ„")
                        now_out = gr.Textbox(label="Ù…ÙˆØ¬ÙˆØ¯Û Ù…ÙˆØ³Ù…", lines=6, interactive=False)
                        tomo_out = gr.Textbox(label="Ú©Ù„ Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ", lines=10, interactive=False)

            run_now.click(lambda c: get_current_weather(c), inputs=[city_input2], outputs=[now_out])
            run_tomo.click(lambda c: get_tomorrow_forecast(c), inputs=[city_input2], outputs=[tomo_out])

    gr.HTML('<div class="footer">Â© Zameen Dost â€¢ Built for Pakistani farmers â€¢ Urdu-first UX</div>')

# Spaces will run the script, but launching locally is fine too.
if __name__ == "__main__":
    app.launch()
