{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gradio==4.44.0 \\\n",
        "    openai-whisper \\\n",
        "    gTTS \\\n",
        "    groq \\\n",
        "    requests \\\n",
        "    pandas \\\n",
        "    PyPDF2 \\\n",
        "    python-docx \\\n",
        "    nltk \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    torch \\\n",
        "    transformers \\\n",
        "    langchain==0.2.16 \\\n",
        "    langchain-community==0.2.10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eOTs7vhI1XgE",
        "outputId": "bd248671-b91c-46b1-9fa8-3f2bd0bfeb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio==4.44.0\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Collecting langchain==0.2.16\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community==0.2.10\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.44.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (4.10.0)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.6.1)\n",
            "Collecting gradio-client==1.3.0 (from gradio==4.44.0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.34.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.44.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (25.0)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==4.44.0)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.12.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.44.0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (4.14.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.0) (0.35.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.16) (2.0.42)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.16) (3.12.15)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain==0.2.16)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.16)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.16)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<3.0,>=1.0 (from gradio==4.44.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.16) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.10)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.3.0->gradio==4.44.0) (2025.3.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio==4.44.0)\n",
            "  Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0->gradio==4.44.0) (0.47.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.44.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.0) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (1.1.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16) (1.33)\n",
            "Collecting packaging (from gradio==4.44.0)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.0) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.16) (3.2.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.0) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain==0.2.16) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=f834a273ca57071497528691ba40048e4e3850db3261c339812e1ff1686f3714\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: websockets, tomlkit, python-docx, PyPDF2, pillow, packaging, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mypy-extensions, markupsafe, click, aiofiles, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, gTTS, faiss-cpu, nvidia-cusolver-cu12, langsmith, groq, gradio-client, dataclasses-json, langchain-core, gradio, openai-whisper, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.12\n",
            "    Uninstalling langsmith-0.4.12:\n",
            "      Successfully uninstalled langsmith-0.4.12\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.0\n",
            "    Uninstalling gradio_client-1.11.0:\n",
            "      Successfully uninstalled gradio_client-1.11.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.72\n",
            "    Uninstalling langchain-core-0.3.72:\n",
            "      Successfully uninstalled langchain-core-0.3.72\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.41.0\n",
            "    Uninstalling gradio-5.41.0:\n",
            "      Successfully uninstalled gradio-5.41.0\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.9\n",
            "    Uninstalling langchain-text-splitters-0.3.9:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.9\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.28.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 aiofiles-23.2.1 click-8.1.8 dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 gTTS-2.5.4 gradio-4.44.0 gradio-client-1.3.0 groq-0.31.0 langchain-0.2.16 langchain-community-0.2.10 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 markupsafe-2.1.5 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625 packaging-24.2 pillow-10.4.0 python-docx-1.2.0 tomlkit-0.12.0 typing-inspect-0.9.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "fc059163d4e54ef79f036d4539437a90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"gradio==4.44.1\"\n",
        "# (gradio_client will update to a compatible version automatically)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mxY4MZiAPTK-",
        "outputId": "88444ea4-9b55-456c-c0e8-74cdff4428ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio==4.44.1\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (4.10.0)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.34.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.12.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (4.14.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (2025.3.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.1) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0->gradio==4.44.1) (0.40.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.1) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.1) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.1) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.1) (1.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.1) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.44.1) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.44.1) (3.4.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 4.44.0\n",
            "    Uninstalling gradio-4.44.0:\n",
            "      Successfully uninstalled gradio-4.44.0\n",
            "Successfully installed gradio-4.44.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gradio"
                ]
              },
              "id": "fdd7218f80ff4ab8b06ccfb97b0081b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from groq import Groq\n",
        "\n",
        "# 🔐 API Key\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\"\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# 📎 PDF Drive Links\n",
        "drive_links = {\n",
        "    \"PDF 1\": \"https://drive.google.com/file/d/16VRkuegHXbhQPPH6kB3jTcdlg1eh95Og/view?usp=sharing\",\n",
        "    \"PDF 2\": \"https://drive.google.com/file/d/1e4Zi9vYXHEtuU_mkpBKDjZ-s0fIFqdzO/view?usp=sharing\",\n",
        "    \"PDF 3\": \"https://drive.google.com/file/d/149Js-w01KO085cRibqXyra9_oPNRAYqZ/view?usp=sharing\"\n",
        "}\n",
        "\n",
        "# 📥 Download PDF\n",
        "def download_pdf_from_drive(drive_link):\n",
        "    try:\n",
        "        file_id = drive_link.split(\"/d/\")[1].split(\"/\")[0]\n",
        "        url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        if response.content[:4] != b\"%PDF\":\n",
        "            raise ValueError(\"Invalid PDF format\")\n",
        "        return BytesIO(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# 🧼 Clean OCR text\n",
        "def clean_ocr_text(text):\n",
        "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# 🧠 Extract Urdu text from all PDFs\n",
        "def extract_all_texts_from_drive(drive_links, max_pages=2):\n",
        "    all_text = \"\"\n",
        "    for title, url in drive_links.items():\n",
        "        file = download_pdf_from_drive(url)\n",
        "        if file:\n",
        "            try:\n",
        "                images = convert_from_bytes(file.read())[:max_pages]\n",
        "                for img in images:\n",
        "                    all_text += pytesseract.image_to_string(img, lang=\"urd\") + \"\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"❌ OCR failed for {title}: {e}\")\n",
        "    return clean_ocr_text(all_text)\n",
        "\n",
        "# 📚 Chunk text\n",
        "def chunk_text(text):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    return splitter.create_documents([text])\n",
        "\n",
        "# 💾 Create FAISS index\n",
        "def create_faiss_index(docs):\n",
        "    if not docs:\n",
        "        raise ValueError(\"❌ No chunks created from text.\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    return FAISS.from_documents(docs, embedding=embeddings)\n",
        "\n",
        "# 🤖 Query with Groq\n",
        "def query_vector_db(query, db):\n",
        "    results = db.similarity_search(query, k=3)\n",
        "    if not results:\n",
        "        return \"❌ No relevant information found.\"\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "    prompt = f\"\"\"Use the following Urdu text to answer the user's question. Provide your answer in BOTH English and Urdu.\n",
        "\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error from Groq: {e}\"\n",
        "\n",
        "# 🚀 Preprocessing once\n",
        "print(\"📦 Processing Urdu PDFs...\")\n",
        "extracted_text = extract_all_texts_from_drive(drive_links)\n",
        "print(f\"✅ Extracted {len(extracted_text)} characters of Urdu text\")\n",
        "\n",
        "documents = chunk_text(extracted_text)\n",
        "print(f\"📄 {len(documents)} chunks created\")\n",
        "\n",
        "vector_db = create_faiss_index(documents)\n",
        "print(\"✅ FAISS index created\")\n",
        "\n",
        "# 🎛 Gradio UI\n",
        "def answer_question(query):\n",
        "    return query_vector_db(query, vector_db)\n",
        "\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(label=\"💬 Ask your question (in English or Roman Urdu)\"),\n",
        "    outputs=gr.Textbox(label=\"📘 Answer\"),\n",
        "    title=\"📚 Urdu PDF QnA (Groq + FAISS)\",\n",
        "    description=\"Ask questions from 3 Urdu PDFs using OCR, FAISS, and Groq's LLaMA model.\",\n",
        "    theme=\"default\"\n",
        ").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "BTkQtxyObKoo",
        "outputId": "3861ebdf-c335-4751-e54e-79a32f7e29e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Processing Urdu PDFs...\n",
            "✅ Extracted 14642 characters of Urdu text\n",
            "📄 59 chunks created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://87ce9fc6545328eee8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://87ce9fc6545328eee8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"pydantic==2.10.6\"\n",
        "# (optional if it fights you)\n",
        "# !pip install --force-reinstall --no-cache-dir \"pydantic==2.7.1\"\n",
        "\n",
        "# IMPORTANT: restart runtime so the new version is used\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLRt7G17pdcS",
        "outputId": "e88e11cc-e231-4d1f-bebc-ecdd242ac8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==2.10.6\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.10.6) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.10.6) (4.14.1)\n",
            "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.28.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.10.6 pydantic-core-2.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pydantic; print(pydantic.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ0vFBFgp68H",
        "outputId": "7219cd93-0ef3-444a-dd6f-5e699ecac452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System deps for pdf2image & Tesseract OCR\n",
        "!apt-get -y install poppler-utils tesseract-ocr tesseract-ocr-urd\n",
        "\n",
        "# Python packages\n",
        "!pip install -U gradio==4.44.1 groq pdf2image pytesseract gTTS \\\n",
        "  sentence-transformers transformers faiss-cpu \\\n",
        "  langchain==0.2.* langchain-community==0.2.* langchain-huggingface==0.0.* \\\n",
        "  openai-whisper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4KqBcJAqVQh",
        "outputId": "928f6d39-90d3-4190-a0ff-676155673bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils tesseract-ocr-urd\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,186 kB of archives.\n",
            "After this operation, 2,110 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-urd all 1:4.00~git30-7274cfa-1.1 [1,000 kB]\n",
            "Fetched 1,186 kB in 1s (1,395 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Selecting previously unselected package tesseract-ocr-urd.\n",
            "Preparing to unpack .../tesseract-ocr-urd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gradio==4.44.1\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.31.0)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: langchain==0.2.* in /usr/local/lib/python3.11/dist-packages (0.2.16)\n",
            "Collecting langchain==0.2.*\n",
            "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-community==0.2.* in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Collecting langchain-community==0.2.*\n",
            "  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-huggingface==0.0.*\n",
            "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (4.10.0)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.34.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.12.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (4.14.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.44.1) (0.35.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (2.0.42)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (3.12.15)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (0.2.43)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (0.1.147)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.*) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.*) (0.6.7)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface==0.0.*) (0.21.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (2025.3.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.10.0)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.*) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.1) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.*) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.*) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0->gradio==4.44.1) (0.47.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.1) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.44.1) (1.1.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.43->langchain==0.2.*) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.*) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.44.1) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.*) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.*) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain==0.2.*) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.44.1) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (2.19.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.*) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.19-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytesseract, pdf2image, gradio, sentence-transformers, langchain-huggingface, langchain, langchain-community\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 4.44.0\n",
            "    Uninstalling gradio-4.44.0:\n",
            "      Successfully uninstalled gradio-4.44.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.0.0\n",
            "    Uninstalling sentence-transformers-5.0.0:\n",
            "      Successfully uninstalled sentence-transformers-5.0.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.16\n",
            "    Uninstalling langchain-0.2.16:\n",
            "      Successfully uninstalled langchain-0.2.16\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.10\n",
            "    Uninstalling langchain-community-0.2.10:\n",
            "      Successfully uninstalled langchain-community-0.2.10\n",
            "Successfully installed gradio-4.44.1 langchain-0.2.17 langchain-community-0.2.19 langchain-huggingface-0.0.3 pdf2image-1.17.0 pytesseract-0.3.13 sentence-transformers-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Standard library\n",
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "\n",
        "# --- Third-party\n",
        "import requests\n",
        "import gradio as gr\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "from groq import Groq\n",
        "\n",
        "# --- LangChain (v0.2.x layout)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "GYDpHEvOqZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "tg8V08dW2SQn",
        "outputId": "94165b0d-b0fe-4900-f92f-d95a9acd44e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.28.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ef1ab371d70386f049.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ef1ab371d70386f049.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py — Smart Zameen Dost (updated for LangChain 0.2.x)\n",
        "# --------------------------------------------------------\n",
        "# - Uses langchain_core.documents.Document\n",
        "# - Uses langchain_huggingface for embeddings\n",
        "# - Robust membership tests, metadata handling\n",
        "# - Safe Google Drive PDF download + PyPDF2 extraction\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from gtts import gTTS\n",
        "import whisper\n",
        "import torch  # noqa: F401\n",
        "\n",
        "from groq import Groq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# ----------------------------\n",
        "# Keys (⚠️ hard-code only if you accept the risk)\n",
        "# ----------------------------\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\"  # paste locally\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"WEATHER_API_KEY\"] = WEATHER_API_KEY\n",
        "\n",
        "if not GROQ_API_KEY.strip():\n",
        "    raise RuntimeError(\"GROQ_API_KEY not set.\")\n",
        "if not WEATHER_API_KEY.strip():\n",
        "    raise RuntimeError(\"WEATHER_API_KEY not set.\")\n",
        "\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# ----------------------------\n",
        "# NLTK (quiet)\n",
        "# ----------------------------\n",
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Models / Embeddings\n",
        "# ----------------------------\n",
        "print(\"🤖 Loading Whisper model...\")\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "print(\"✅ Whisper model loaded.\")\n",
        "\n",
        "print(\"🔤 Loading multilingual sentence embeddings...\")\n",
        "multilingual_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "print(\"✅ Multilingual embeddings loaded.\")\n",
        "\n",
        "# ----------------------------\n",
        "# Predefined Google Drive PDFs\n",
        "# ----------------------------\n",
        "PREDEFINED_PDF_LINKS = [\n",
        "    \"https://drive.google.com/file/d/1H7b-1PG2SLB99gjogfSl7QTOmLd1iGX0/view?usp=sharing\",\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def safe_str(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (bool, int, float)):\n",
        "        return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def normalize_mixed_text(text: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", safe_str(text)).strip()\n",
        "    return re.sub(\n",
        "        r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()\\-]\",\n",
        "        \" \",\n",
        "        s,\n",
        "    )\n",
        "\n",
        "def extract_numerical_data(text: str):\n",
        "    t = safe_str(text)\n",
        "    info = {}\n",
        "    prices = re.findall(r\"[\\$Rs\\.]\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", t)\n",
        "    if prices:\n",
        "        info[\"prices\"] = prices\n",
        "    perc = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*%\", t)\n",
        "    if perc:\n",
        "        info[\"percentages\"] = perc\n",
        "    yields = re.findall(\n",
        "        r\"(\\d+(?:\\.\\d+)?)\\s*(tons?|kg|quintals?|maunds?)\\s*(?:per|/)?\\s*(acre|hectare|ایکڑ)\",\n",
        "        t,\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "    if yields:\n",
        "        info[\"yields\"] = yields\n",
        "    return info\n",
        "\n",
        "# ----------------------------\n",
        "# Google Drive PDF Processor\n",
        "# ----------------------------\n",
        "class GoogleDrivePDFProcessor:\n",
        "    @staticmethod\n",
        "    def convert_gdrive_link(share_link: str):\n",
        "        patterns = [r\"/file/d/([a-zA-Z0-9\\-_]+)\", r\"id=([a-zA-Z0-9\\-_]+)\", r\"/d/([a-zA-Z0-9\\-_]+)\"]\n",
        "        file_id = None\n",
        "        link = safe_str(share_link)\n",
        "        for pat in patterns:\n",
        "            m = re.search(pat, link)\n",
        "            if m:\n",
        "                file_id = m.group(1)\n",
        "                break\n",
        "        if not file_id:\n",
        "            return None\n",
        "        return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pdf_from_gdrive(gdrive_link: str):\n",
        "        try:\n",
        "            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)\n",
        "            if not download_link:\n",
        "                return None, \"Invalid Google Drive link format\"\n",
        "\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "            resp = requests.get(download_link, headers=headers, stream=True, timeout=60)\n",
        "            txt = safe_str(resp.text)\n",
        "\n",
        "            if (\"confirm=\" in txt) or (\"virus scan warning\" in txt.lower()):\n",
        "                token = re.search(r\"confirm=([^&]+)\", txt)\n",
        "                if token:\n",
        "                    confirmed = f\"{download_link}&confirm={token.group(1)}\"\n",
        "                    resp = requests.get(confirmed, headers=headers, stream=True, timeout=60)\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                return resp.content, \"Success\"\n",
        "            return None, f\"Download failed: HTTP {resp.status_code}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Download error: {e}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_content: bytes):\n",
        "        try:\n",
        "            reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n",
        "            pages = len(reader.pages)\n",
        "            out = []\n",
        "            for i in range(pages):\n",
        "                try:\n",
        "                    pg = reader.pages[i]\n",
        "                    t = (pg.extract_text() or \"\").strip()\n",
        "                    if t:\n",
        "                        out.append(f\"\\n--- Page {i+1} ---\\n{t}\\n\")\n",
        "                except Exception:\n",
        "                    out.append(f\"\\n--- Page {i+1} (Error extracting) ---\\n\")\n",
        "            return \"\".join(out), pages\n",
        "        except Exception as e:\n",
        "            return f\"PDF text extraction error: {e}\", 0\n",
        "\n",
        "# ----------------------------\n",
        "# Knowledge Base / RAG\n",
        "# ----------------------------\n",
        "class AdvancedPakistaniAgriRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = multilingual_embeddings\n",
        "        self.vector_store = None\n",
        "        self.gdrive = GoogleDrivePDFProcessor()\n",
        "        self.processed_documents = []\n",
        "        self._setup_seed_knowledge()\n",
        "        self._auto_process_predefined_pdfs()\n",
        "\n",
        "    def _setup_seed_knowledge(self):\n",
        "        seed = [\n",
        "            {\n",
        "                \"content\": \"\"\"Punjab Wheat Varieties for Export:\n",
        "                اعلیٰ قسم کی گندم کی اقسام:\n",
        "                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton\n",
        "                - Faisalabad-2008: High protein 12-14%, Premium export variety\n",
        "                - Galaxy-2013: Disease resistant, Suitable for UAE market\n",
        "                - Punjab-2011: Good for bread making, Export to Afghanistan\n",
        "                Urdu: یہ اقسام برآمد کے لیے بہترین ہیں اور زیادہ قیمت ملتی ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"crop_varieties\", \"region\": \"Punjab\", \"crop\": \"wheat\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Rice Export Opportunities - چاول کی برآمدات:\n",
        "                Basmati Varieties with International Prices:\n",
        "                - Super Basmati: $900-1200/ton (UAE, Saudi Arabia)\n",
        "                - Basmati 385: Premium grade, $1000-1300/ton\n",
        "                - IRRI-6: $450-550/ton (Philippines, Malaysia)\n",
        "                - Kainaat: $700-850/ton (Middle East markets)\n",
        "\n",
        "                Export Requirements:\n",
        "                - Moisture: Maximum 14%\n",
        "                - Broken grains: Less than 5%\n",
        "                - Length: Minimum 6.0mm for Basmati\n",
        "\n",
        "                اردو میں: بسمتی چاول کی برآمد سب سے زیادہ منافع بخش ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"export_markets\", \"crop\": \"rice\", \"price_range\": \"450-1300\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Government Support Schemes - حکومتی اسکیمز:\n",
        "                Kisan Card Program:\n",
        "                - 25% subsidy on fertilizers\n",
        "                - 20% discount on certified seeds\n",
        "                - Easy loan access through banks\n",
        "\n",
        "                Solar Tube Well Scheme:\n",
        "                - 60% government subsidy\n",
        "                - Remaining 40% through easy installments\n",
        "                - Electricity bill savings: Rs. 50,000+ annually\n",
        "\n",
        "                Crop Insurance Program:\n",
        "                - Premium: 5% of sum insured\n",
        "                - Government pays 75% of premium\n",
        "                - Coverage: Natural disasters, pest attacks\n",
        "\n",
        "                کسان ڈویلپمنٹ پروگرام سے مفت تربیت اور مشورے\"\"\",\n",
        "                \"metadata\": {\"type\": \"government_schemes\", \"schemes\": \"kisan_card,solar_tubewell,crop_insurance\", \"language\": \"mixed\"},\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        docs = []\n",
        "        for item in seed:\n",
        "            content = normalize_mixed_text(item[\"content\"])\n",
        "            meta = dict(item.get(\"metadata\") or {})\n",
        "            nums = extract_numerical_data(content)\n",
        "            if nums:\n",
        "                meta.update(nums)\n",
        "            docs.append(Document(page_content=content, metadata=meta))\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"], length_function=len\n",
        "        )\n",
        "        pieces = splitter.split_documents(docs)\n",
        "        self.vector_store = FAISS.from_documents(pieces, self.embeddings)\n",
        "        print(\"✅ Seed agricultural knowledge initialized with\", len(pieces), \"chunks.\")\n",
        "\n",
        "    def _auto_process_predefined_pdfs(self):\n",
        "        if not PREDEFINED_PDF_LINKS:\n",
        "            print(\"ℹ️ No predefined PDFs configured.\")\n",
        "            return\n",
        "\n",
        "        print(f\"🚀 Auto-processing {len(PREDEFINED_PDF_LINKS)} Google Drive PDF(s)...\")\n",
        "        ok = 0\n",
        "        for i, link in enumerate(PREDEFINED_PDF_LINKS, start=1):\n",
        "            try:\n",
        "                blob, msg = self.gdrive.download_pdf_from_gdrive(link)\n",
        "                if blob is None:\n",
        "                    print(f\"❌ Doc {i}: {msg}\")\n",
        "                    continue\n",
        "\n",
        "                text, pages = self.gdrive.extract_text_from_pdf(blob)\n",
        "                if \"pdf text extraction error\" in safe_str(text).lower():\n",
        "                    print(f\"❌ Doc {i}: {text}\")\n",
        "                    continue\n",
        "\n",
        "                if len(safe_str(text).strip()) < 100:\n",
        "                    print(f\"⚠️ Doc {i}: likely image-based or encrypted; minimal text.\")\n",
        "\n",
        "                processed = normalize_mixed_text(text)\n",
        "                numbers = extract_numerical_data(processed)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=processed,\n",
        "                    metadata={\n",
        "                        \"type\": \"auto_processed_pdf\",\n",
        "                        \"source\": f\"Auto PDF {i}\",\n",
        "                        \"pages\": pages,\n",
        "                        \"numerical_data\": numbers,\n",
        "                        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                        \"original_link\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"]\n",
        "                )\n",
        "                chunks = splitter.split_documents([doc])\n",
        "\n",
        "                if self.vector_store:\n",
        "                    self.vector_store.add_documents(chunks)\n",
        "                else:\n",
        "                    self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": pages, \"chunks\": len(chunks), \"source\": doc.metadata[\"original_link\"], \"status\": \"✅ Success\"}\n",
        "                )\n",
        "                print(f\"✅ Doc {i}: {pages} pages → {len(chunks)} chunks\")\n",
        "                ok += 1\n",
        "            except Exception as e:\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": 0, \"chunks\": 0, \"source\": link[:50] + \"...\" if len(link) > 50 else link, \"status\": f\"❌ Error: {e}\"}\n",
        "                )\n",
        "                print(f\"❌ Doc {i}: {e}\")\n",
        "\n",
        "        print(f\"🎉 Finished: {ok}/{len(PREDEFINED_PDF_LINKS)} document(s) processed.\")\n",
        "\n",
        "    def get_stats_html(self) -> str:\n",
        "        if not self.processed_documents:\n",
        "            return \"📊 Knowledge Base: Seed Pakistani agricultural data only (no PDFs yet)\"\n",
        "        total_chunks = sum(d.get(\"chunks\", 0) for d in self.processed_documents)\n",
        "        total_pages = sum(d.get(\"pages\", 0) for d in self.processed_documents)\n",
        "        return f\"\"\"📊 Knowledge Base Statistics:\n",
        "\n",
        "🗂️ Auto-processed Documents: {len(self.processed_documents)}\n",
        "📄 Total Pages Processed: {total_pages}\n",
        "🧩 Total Text Chunks: {total_chunks}\n",
        "📚 Seed Knowledge: Pakistani agriculture (Urdu + English)\n",
        "🔍 Search Capability: Multilingual (English + Urdu)\n",
        "✅ Status: Ready for queries\n",
        "\"\"\"\n",
        "\n",
        "    def get_relevant_info(self, query: str, k: int = 4) -> str:\n",
        "        if not self.vector_store:\n",
        "            return \"Knowledge base not available\"\n",
        "        try:\n",
        "            q = safe_str(query)\n",
        "            hits = self.vector_store.similarity_search(q, k=k)\n",
        "\n",
        "            context = \"\"\n",
        "            nums_summary = []\n",
        "\n",
        "            for i, doc in enumerate(hits, start=1):\n",
        "                context += f\"معلومات {i}: {doc.page_content}\\n\\n\"\n",
        "\n",
        "                meta = doc.metadata or {}\n",
        "                if not isinstance(meta, dict):\n",
        "                    meta = {}\n",
        "\n",
        "                nd = meta.get(\"numerical_data\")\n",
        "                if isinstance(nd, dict):\n",
        "                    meta = {**meta, **nd}\n",
        "\n",
        "                if isinstance(meta.get(\"prices\"), list) and meta[\"prices\"]:\n",
        "                    nums_summary.append(f\"💰 قیمتیں: {', '.join(map(safe_str, meta['prices']))}\")\n",
        "                if isinstance(meta.get(\"percentages\"), list) and meta[\"percentages\"]:\n",
        "                    nums_summary.append(f\"📊 فیصد: {', '.join(map(safe_str, meta['percentages']))}%\")\n",
        "                if isinstance(meta.get(\"yields\"), list) and meta[\"yields\"]:\n",
        "                    y_fmt = []\n",
        "                    for y in meta[\"yields\"]:\n",
        "                        try:\n",
        "                            val, unit, per = y\n",
        "                            y_fmt.append(f\"{val} {unit} per {per}\")\n",
        "                        except Exception:\n",
        "                            y_fmt.append(safe_str(y))\n",
        "                    nums_summary.append(f\"🌾 پیداوار: {', '.join(y_fmt)}\")\n",
        "\n",
        "            if nums_summary:\n",
        "                context = \"📈 اہم اعداد و شمار:\\n\" + \"\\n\".join(nums_summary) + \"\\n\\n\" + context\n",
        "            return context or \"No relevant information found.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving information: {e}\"\n",
        "\n",
        "# Initialize RAG\n",
        "print(\"🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\")\n",
        "pak_agri_rag = AdvancedPakistaniAgriRAG()\n",
        "\n",
        "# ----------------------------\n",
        "# Voice, Weather, AI\n",
        "# ----------------------------\n",
        "def voice_to_text(audio_file_path):\n",
        "    if not audio_file_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_file_path, language=\"ur\")\n",
        "        return normalize_mixed_text(result.get(\"text\", \"\"))\n",
        "    except Exception as e:\n",
        "        return f\"آواز سمجھ نہیں آئی: {e}\"\n",
        "\n",
        "def get_weather_with_farming_advice(city=\"Lahore\"):\n",
        "    try:\n",
        "        city = safe_str(city).strip() or \"Lahore\"\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric\"\n",
        "        resp = requests.get(url, timeout=20)\n",
        "        try:\n",
        "            data = resp.json()\n",
        "        except Exception:\n",
        "            return \"मوسمی JSON درست نہیں۔\"\n",
        "\n",
        "        main = data.get(\"main\") or {}\n",
        "        wind = data.get(\"wind\") or {}\n",
        "        weather_l = data.get(\"weather\") or [{}]\n",
        "\n",
        "        temp = main.get(\"temp\")\n",
        "        humidity = main.get(\"humidity\")\n",
        "        wind_speed = wind.get(\"speed\")\n",
        "        description = weather_l[0].get(\"description\", \"\")\n",
        "\n",
        "        if any(v is None for v in (temp, humidity, wind_speed)):\n",
        "            return \"موسمی معلومات مکمل نہیں مل سکیں۔\"\n",
        "\n",
        "        if temp > 35:\n",
        "            advice = f\"⚠️ زیادہ گرمی ({temp}°C): صبح 6-8 بجے پانی دیں، دوپہر میں نہیں۔ پانی کی مقدار 20% بڑھائیں۔\"\n",
        "        elif humidity > 80:\n",
        "            advice = f\"🌧️ زیادہ نمی ({humidity}%): فنگیسائیڈ سپرے کریں۔ Mancozeb 2g/لیٹر یا Copper Oxychloride 3g/لیٹر۔\"\n",
        "        elif temp < 10:\n",
        "            advice = f\"❄️ سردی ({temp}°C): پودوں کو ڈھانپیں، پانی 50% کم دیں۔ Frost protection ضروری۔\"\n",
        "        elif wind_speed > 5:\n",
        "            advice = f\"💨 تیز ہوا ({wind_speed} m/s): کیڑے مار دوا کا سپرے نہ کریں۔ Wind barriers لگائیں۔\"\n",
        "        else:\n",
        "            advice = f\"✅ موسم اچھا ہے ({temp}°C, {humidity}% نمی): کھیتی کے کام کر سکتے ہیں۔\"\n",
        "\n",
        "        return f\"آج {city} میں {temp}°C، نمی {humidity}%، ہوا {wind_speed} m/s، موسم {description}\\n\\n{advice}\"\n",
        "    except Exception as e:\n",
        "        return f\"موسمی معلومات نہیں مل سکیں: {e}\"\n",
        "\n",
        "def text_to_voice(text):\n",
        "    try:\n",
        "        clean = normalize_mixed_text(text)\n",
        "        if len(clean) > 500:\n",
        "            clean = clean[:500] + \"... مکمل جواب اوپر پڑھیں\"\n",
        "        tts = gTTS(text=clean, lang=\"ur\", slow=False)\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n",
        "            tts.save(tmp.name)\n",
        "            return tmp.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_enhanced_ai_response(user_message: str, location: str = \"\") -> str:\n",
        "    relevant_context = pak_agri_rag.get_relevant_info(user_message)\n",
        "    system_prompt = f\"\"\"\n",
        "آپ \"زمین دوست\" ہیں - پاکستانی کسانوں کے ماہر مشیر۔\n",
        "\n",
        "آپ کے پاس پاکستانی زراعت کی معلومات (English اور Urdu میں) ہیں:\n",
        "{relevant_context}\n",
        "\n",
        "کسان کا علاقہ: {safe_str(location)}\n",
        "\n",
        "آپ کا کام:\n",
        "1) پاکستانی حالات کے مطابق مشورہ دینا\n",
        "2) برآمدی فصلوں کی تجویز دینا (numerical data کے ساتھ)\n",
        "3) مقامی اقسام اور قیمتوں کا ذکر کرنا\n",
        "4) حکومتی اسکیموں کی معلومات دینا\n",
        "5) نقصان سے بچاؤ کے طریقے بتانا\n",
        "6) اعداد و شمار استعمال کرنا (prices, yields, percentages)\n",
        "\n",
        "Guidelines:\n",
        "- ہمیشہ \"بھائی\" کہہ کر شروع کریں\n",
        "- آسان اردو استعمال کریں\n",
        "- Numbers اور prices ضرور بتائیں\n",
        "- Export opportunities highlight کریں\n",
        "- Government schemes mention کریں\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        chat = groq_client.chat.completions.create(\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                      {\"role\": \"user\", \"content\": safe_str(user_message)}],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            max_tokens=1200,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        return chat.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"معذرت، AI سے رابطہ نہیں ہو سکا: {e}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Main chat handler (robust)\n",
        "# ----------------------------\n",
        "def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):\n",
        "    user_message = \"\"\n",
        "    input_display = \"\"\n",
        "\n",
        "    if audio_input:\n",
        "        user_message = voice_to_text(audio_input)\n",
        "        input_display = f\"💬 آپ نے کہا: {user_message}\"\n",
        "    elif text_input:\n",
        "        user_message = safe_str(text_input)\n",
        "        input_display = f\"💬 آپ نے لکھا: {user_message}\"\n",
        "\n",
        "    if not isinstance(user_message, str) or not user_message.strip():\n",
        "        return \"کرپیا کوئی سوال پوچھیں\", None, \"❌ کوئی سوال نہیں ملا\"\n",
        "\n",
        "    enhanced = user_message\n",
        "    if focus_area and safe_str(focus_area) != \"عام سوال\":\n",
        "        enhanced += f\" (کسان کی دلچسپی: {focus_area})\"\n",
        "\n",
        "    terms = [\"موسم\", \"بارش\", \"پانی\", \"weather\", \"irrigation\", \"spray\", \"سپرے\"]\n",
        "    if isinstance(user_message, str) and any(t in user_message for t in terms):\n",
        "        weather_info = get_weather_with_farming_advice(city_name or \"Lahore\")\n",
        "        enhanced += f\"\\n\\nموسمی حالات: {weather_info}\"\n",
        "\n",
        "    ai_response = get_enhanced_ai_response(enhanced, city_name or \"\")\n",
        "    voice_response = text_to_voice(ai_response)\n",
        "    return input_display, voice_response, ai_response\n",
        "\n",
        "# ----------------------------\n",
        "# UI\n",
        "# ----------------------------\n",
        "with gr.Blocks(\n",
        "    title=\"Smart Zameen Dost - زمین دوست\",\n",
        "    theme=gr.themes.Base(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container { background: linear-gradient(135deg, #f8fdff 0%, #e8f7f8 100%); font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }\n",
        "    .header-box { background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin: 10px 0; border-left: 4px solid #2E8B57; }\n",
        "    .stats-box { background: linear-gradient(45deg, #e8f5e8, #f0f8e8); padding: 15px; border-radius: 8px; border: 1px solid #c8e6c9; margin: 10px 0; font-size: 0.9em; }\n",
        "    \"\"\"\n",
        ") as app:\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class='header-box'>\n",
        "          <div style='text-align: center;'>\n",
        "            <h1 style='color: #2E8B57; font-size: 2.2em; margin: 0 0 8px 0;'>🌾 Smart Zameen Dost</h1>\n",
        "            <p style='color: #666; font-size: 1.1em; margin: 0;'>پاکستانی کسانوں کا ذہین مشیر</p>\n",
        "          </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎤 اپنا سوال پوچھیں\")\n",
        "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"آواز میں پوچھیں\")\n",
        "            text_input = gr.Textbox(label=\"یا یہاں لکھیں (اردو/English)\", placeholder=\"مثال: کون سی فصل زیادہ منافع دے گی؟\", lines=2)\n",
        "            with gr.Row():\n",
        "                city_input = gr.Textbox(label=\"آپ کا شہر\", placeholder=\"Lahore, Karachi, Faisalabad\", value=\"Lahore\", scale=1)\n",
        "                focus_area = gr.Dropdown(\n",
        "                    label=\"دلچسپی کا شعبہ\",\n",
        "                    choices=[\"عام سوال\",\"برآمدی فصلیں\",\"گندم کی کاشت\",\"چاول کی کاشت\",\"کپاس کی کاشت\",\"سبزیوں کی کاشت\",\"پھلوں کی کاشت\",\"کھاد اور بیج\",\"بیماریوں کا علاج\",\"حکومتی اسکیمز\",\"منڈی کی قیمتیں\"],\n",
        "                    value=\"عام سوال\",\n",
        "                    scale=1,\n",
        "                )\n",
        "            chat_btn = gr.Button(\"🚀 جواب حاصل کریں\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🧠 ذہین جواب\")\n",
        "            input_display = gr.Textbox(label=\"آپ کا سوال\", lines=2, interactive=False)\n",
        "            audio_output = gr.Audio(label=\"🔊 آواز میں جواب\")\n",
        "            text_output = gr.Textbox(label=\"📝 تفصیلی جواب\", lines=10, interactive=False, show_copy_button=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        kb_stats = gr.HTML(value=pak_agri_rag.get_stats_html(), elem_classes=[\"stats-box\"])\n",
        "\n",
        "    chat_btn.click(\n",
        "        zameen_dost_advanced_chat,\n",
        "        inputs=[audio_input, text_input, city_input, focus_area],\n",
        "        outputs=[input_display, audio_output, text_output],\n",
        "    )\n",
        "\n",
        "print(\"🎉 App ready!\")\n",
        "print(f\"✅ Auto-processed {len(PREDEFINED_PDF_LINKS)} Google Drive PDF link(s)\")\n",
        "print(\"🔍 Multilingual RAG + Voice + Weather integrated\")\n",
        "\n",
        "gr.close_all()\n",
        "app.launch(share=True, debug=True, show_api=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "Lr0xIuuIJ8t6",
        "outputId": "fd40a2e7-8fb6-4dcf-a55f-1b67987d0cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Loading Whisper model...\n",
            "✅ Whisper model loaded.\n",
            "🔤 Loading multilingual sentence embeddings...\n",
            "✅ Multilingual embeddings loaded.\n",
            "🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\n",
            "✅ Seed agricultural knowledge initialized with 3 chunks.\n",
            "🚀 Auto-processing 1 Google Drive PDF(s)...\n",
            "✅ Doc 1: 322 pages → 1516 chunks\n",
            "🎉 Finished: 1/1 document(s) processed.\n",
            "🎉 App ready!\n",
            "✅ Auto-processed 1 Google Drive PDF link(s)\n",
            "🔍 Multilingual RAG + Voice + Weather integrated\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://aff99ca7b6b3e43a3c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aff99ca7b6b3e43a3c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://aff99ca7b6b3e43a3c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py — Smart Zameen Dost (trimmed context to avoid 413/TPM)\n",
        "# --------------------------------------------------------------\n",
        "# - Uses langchain_core.documents.Document (LangChain 0.2.x)\n",
        "# - Caps retrieval + context length to stay under Groq limits\n",
        "# - Robust membership tests, metadata handling\n",
        "# - Safe Google Drive PDF download + PyPDF2 extraction\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from gtts import gTTS\n",
        "import whisper\n",
        "import torch  # noqa: F401\n",
        "\n",
        "from groq import Groq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# =========================\n",
        "# 🔑 Keys (⚠️ hard-code only if you accept the risk)\n",
        "# =========================\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\"\"  # paste locally\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"WEATHER_API_KEY\"] = WEATHER_API_KEY\n",
        "\n",
        "if not GROQ_API_KEY.strip():\n",
        "    raise RuntimeError(\"GROQ_API_KEY not set.\")\n",
        "if not WEATHER_API_KEY.strip():\n",
        "    raise RuntimeError(\"WEATHER_API_KEY not set.\")\n",
        "\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# =========================\n",
        "# 🔧 Global limits to keep prompt small\n",
        "# =========================\n",
        "K_RETRIEVE = 3               # fewer chunks retrieved\n",
        "PER_DOC_CHARS = 700          # per-chunk char cap (~175 tokens)\n",
        "MAX_CONTEXT_CHARS = 4000     # total context cap\n",
        "MAX_OUTPUT_TOKENS = 512      # shorter generations\n",
        "\n",
        "def _limit_chars(s: str, n: int) -> str:\n",
        "    s = str(s or \"\")\n",
        "    return s if len(s) <= n else (s[:n] + \" …\")\n",
        "\n",
        "def _clip_context(snippets, max_chars: int) -> str:\n",
        "    out, used = [], 0\n",
        "    for snip in snippets:\n",
        "        snip = str(snip or \"\")\n",
        "        if used + len(snip) > max_chars:\n",
        "            snip = snip[: max(0, max_chars - used)]\n",
        "        if snip:\n",
        "            out.append(snip)\n",
        "            used += len(snip)\n",
        "        if used >= max_chars:\n",
        "            break\n",
        "    return \"\\n\\n\".join(out)\n",
        "\n",
        "# =========================\n",
        "# 🔇 NLTK (quiet)\n",
        "# =========================\n",
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# 🤖 Models / Embeddings\n",
        "# =========================\n",
        "print(\"🤖 Loading Whisper model...\")\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "print(\"✅ Whisper model loaded.\")\n",
        "\n",
        "print(\"🔤 Loading multilingual sentence embeddings...\")\n",
        "multilingual_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "print(\"✅ Multilingual embeddings loaded.\")\n",
        "\n",
        "# =========================\n",
        "# 📄 Predefined Google Drive PDFs\n",
        "# =========================\n",
        "PREDEFINED_PDF_LINKS = [\n",
        "    \"https://drive.google.com/file/d/1H7b-1PG2SLB99gjogfSl7QTOmLd1iGX0/view?usp=sharing\",\n",
        "\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 🧰 Helpers\n",
        "# =========================\n",
        "def safe_str(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (bool, int, float)):\n",
        "        return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def normalize_mixed_text(text: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", safe_str(text)).strip()\n",
        "    return re.sub(\n",
        "        r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()\\-]\",\n",
        "        \" \",\n",
        "        s,\n",
        "    )\n",
        "\n",
        "def extract_numerical_data(text: str):\n",
        "    t = safe_str(text)\n",
        "    info = {}\n",
        "    prices = re.findall(r\"[\\$Rs\\.]\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", t)\n",
        "    if prices:\n",
        "        info[\"prices\"] = prices\n",
        "    perc = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*%\", t)\n",
        "    if perc:\n",
        "        info[\"percentages\"] = perc\n",
        "    yields = re.findall(\n",
        "        r\"(\\d+(?:\\.\\d+)?)\\s*(tons?|kg|quintals?|maunds?)\\s*(?:per|/)?\\s*(acre|hectare|ایکڑ)\",\n",
        "        t,\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "    if yields:\n",
        "        info[\"yields\"] = yields\n",
        "    return info\n",
        "\n",
        "# =========================\n",
        "# 📥 Google Drive PDF Processor\n",
        "# =========================\n",
        "class GoogleDrivePDFProcessor:\n",
        "    @staticmethod\n",
        "    def convert_gdrive_link(share_link: str):\n",
        "        patterns = [r\"/file/d/([a-zA-Z0-9\\-_]+)\", r\"id=([a-zA-Z0-9\\-_]+)\", r\"/d/([a-zA-Z0-9\\-_]+)\"]\n",
        "        file_id = None\n",
        "        link = safe_str(share_link)\n",
        "        for pat in patterns:\n",
        "            m = re.search(pat, link)\n",
        "            if m:\n",
        "                file_id = m.group(1)\n",
        "                break\n",
        "        if not file_id:\n",
        "            return None\n",
        "        return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pdf_from_gdrive(gdrive_link: str):\n",
        "        try:\n",
        "            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)\n",
        "            if not download_link:\n",
        "                return None, \"Invalid Google Drive link format\"\n",
        "\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "            resp = requests.get(download_link, headers=headers, stream=True, timeout=60)\n",
        "            txt = safe_str(resp.text)\n",
        "\n",
        "            if (\"confirm=\" in txt) or (\"virus scan warning\" in txt.lower()):\n",
        "                token = re.search(r\"confirm=([^&]+)\", txt)\n",
        "                if token:\n",
        "                    confirmed = f\"{download_link}&confirm={token.group(1)}\"\n",
        "                    resp = requests.get(confirmed, headers=headers, stream=True, timeout=60)\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                return resp.content, \"Success\"\n",
        "            return None, f\"Download failed: HTTP {resp.status_code}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Download error: {e}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_content: bytes):\n",
        "        try:\n",
        "            reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n",
        "            pages = len(reader.pages)\n",
        "            out = []\n",
        "            for i in range(pages):\n",
        "                try:\n",
        "                    pg = reader.pages[i]\n",
        "                    t = (pg.extract_text() or \"\").strip()\n",
        "                    if t:\n",
        "                        out.append(f\"\\n--- Page {i+1} ---\\n{t}\\n\")\n",
        "                except Exception:\n",
        "                    out.append(f\"\\n--- Page {i+1} (Error extracting) ---\\n\")\n",
        "            return \"\".join(out), pages\n",
        "        except Exception as e:\n",
        "            return f\"PDF text extraction error: {e}\", 0\n",
        "\n",
        "# =========================\n",
        "# 🧠 Knowledge Base / RAG\n",
        "# =========================\n",
        "class AdvancedPakistaniAgriRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = multilingual_embeddings\n",
        "        self.vector_store = None\n",
        "        self.gdrive = GoogleDrivePDFProcessor()\n",
        "        self.processed_documents = []\n",
        "        self._setup_seed_knowledge()\n",
        "        self._auto_process_predefined_pdfs()\n",
        "\n",
        "    def _setup_seed_knowledge(self):\n",
        "        seed = [\n",
        "            {\n",
        "                \"content\": \"\"\"Punjab Wheat Varieties for Export:\n",
        "                اعلیٰ قسم کی گندم کی اقسام:\n",
        "                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton\n",
        "                - Faisalabad-2008: High protein 12-14%, Premium export variety\n",
        "                - Galaxy-2013: Disease resistant, Suitable for UAE market\n",
        "                - Punjab-2011: Good for bread making, Export to Afghanistan\n",
        "                Urdu: یہ اقسام برآمد کے لیے بہترین ہیں اور زیادہ قیمت ملتی ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"crop_varieties\", \"region\": \"Punjab\", \"crop\": \"wheat\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Rice Export Opportunities - چاول کی برآمدات:\n",
        "                Basmati Varieties with International Prices:\n",
        "                - Super Basmati: $900-1200/ton (UAE, Saudi Arabia)\n",
        "                - Basmati 385: Premium grade, $1000-1300/ton\n",
        "                - IRRI-6: $450-550/ton (Philippines, Malaysia)\n",
        "                - Kainaat: $700-850/ton (Middle East markets)\n",
        "\n",
        "                Export Requirements:\n",
        "                - Moisture: Maximum 14%\n",
        "                - Broken grains: Less than 5%\n",
        "                - Length: Minimum 6.0mm for Basmati\n",
        "\n",
        "                اردو میں: بسمتی چاول کی برآمد سب سے زیادہ منافع بخش ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"export_markets\", \"crop\": \"rice\", \"price_range\": \"450-1300\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Government Support Schemes - حکومتی اسکیمز:\n",
        "                Kisan Card Program:\n",
        "                - 25% subsidy on fertilizers\n",
        "                - 20% discount on certified seeds\n",
        "                - Easy loan access through banks\n",
        "\n",
        "                Solar Tube Well Scheme:\n",
        "                - 60% government subsidy\n",
        "                - Remaining 40% through easy installments\n",
        "                - Electricity bill savings: Rs. 50,000+ annually\n",
        "\n",
        "                Crop Insurance Program:\n",
        "                - Premium: 5% of sum insured\n",
        "                - Government pays 75% of premium\n",
        "                - Coverage: Natural disasters, pest attacks\n",
        "\n",
        "                کسان ڈویلپمنٹ پروگرام سے مفت تربیت اور مشورے\"\"\",\n",
        "                \"metadata\": {\"type\": \"government_schemes\", \"schemes\": \"kisan_card,solar_tubewell,crop_insurance\", \"language\": \"mixed\"},\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        docs = []\n",
        "        for item in seed:\n",
        "            content = normalize_mixed_text(item[\"content\"])\n",
        "            meta = dict(item.get(\"metadata\") or {})\n",
        "            nums = extract_numerical_data(content)\n",
        "            if nums:\n",
        "                meta.update(nums)\n",
        "            docs.append(Document(page_content=content, metadata=meta))\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"], length_function=len\n",
        "        )\n",
        "        pieces = splitter.split_documents(docs)\n",
        "        self.vector_store = FAISS.from_documents(pieces, self.embeddings)\n",
        "        print(\"✅ Seed agricultural knowledge initialized with\", len(pieces), \"chunks.\")\n",
        "\n",
        "    def _auto_process_predefined_pdfs(self):\n",
        "        if not PREDEFINED_PDF_LINKS:\n",
        "            print(\"ℹ️ No predefined PDFs configured.\")\n",
        "            return\n",
        "\n",
        "        print(f\"🚀 Auto-processing {len(PREDEFINED_PDF_LINKS)} Google Drive PDF(s)...\")\n",
        "        ok = 0\n",
        "        for i, link in enumerate(PREDEFINED_PDF_LINKS, start=1):\n",
        "            try:\n",
        "                blob, msg = self.gdrive.download_pdf_from_gdrive(link)\n",
        "                if blob is None:\n",
        "                    print(f\"❌ Doc {i}: {msg}\")\n",
        "                    continue\n",
        "\n",
        "                text, pages = self.gdrive.extract_text_from_pdf(blob)\n",
        "                if \"pdf text extraction error\" in safe_str(text).lower():\n",
        "                    print(f\"❌ Doc {i}: {text}\")\n",
        "                    continue\n",
        "\n",
        "                if len(safe_str(text).strip()) < 100:\n",
        "                    print(f\"⚠️ Doc {i}: likely image-based or encrypted; minimal text.\")\n",
        "\n",
        "                processed = normalize_mixed_text(text)\n",
        "                numbers = extract_numerical_data(processed)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=processed,\n",
        "                    metadata={\n",
        "                        \"type\": \"auto_processed_pdf\",\n",
        "                        \"source\": f\"Auto PDF {i}\",\n",
        "                        \"pages\": pages,\n",
        "                        \"numerical_data\": numbers,\n",
        "                        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                        \"original_link\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"]\n",
        "                )\n",
        "                chunks = splitter.split_documents([doc])\n",
        "\n",
        "                if self.vector_store:\n",
        "                    self.vector_store.add_documents(chunks)\n",
        "                else:\n",
        "                    self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": pages, \"chunks\": len(chunks), \"source\": doc.metadata[\"original_link\"], \"status\": \"✅ Success\"}\n",
        "                )\n",
        "                print(f\"✅ Doc {i}: {pages} pages → {len(chunks)} chunks\")\n",
        "                ok += 1\n",
        "            except Exception as e:\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": 0, \"chunks\": 0, \"source\": link[:50] + \"...\" if len(link) > 50 else link, \"status\": f\"❌ Error: {e}\"}\n",
        "                )\n",
        "                print(f\"❌ Doc {i}: {e}\")\n",
        "\n",
        "        print(f\"🎉 Finished: {ok}/{len(PREDEFINED_PDF_LINKS)} document(s) processed.\")\n",
        "\n",
        "    def get_stats_html(self) -> str:\n",
        "        if not self.processed_documents:\n",
        "            return \"📊 Knowledge Base: Seed Pakistani agricultural data only (no PDFs yet)\"\n",
        "        total_chunks = sum(d.get(\"chunks\", 0) for d in self.processed_documents)\n",
        "        total_pages = sum(d.get(\"pages\", 0) for d in self.processed_documents)\n",
        "        return f\"\"\"📊 Knowledge Base Statistics:\n",
        "\n",
        "🗂️ Auto-processed Documents: {len(self.processed_documents)}\n",
        "📄 Total Pages Processed: {total_pages}\n",
        "🧩 Total Text Chunks: {total_chunks}\n",
        "📚 Seed Knowledge: Pakistani agriculture (Urdu + English)\n",
        "🔍 Search Capability: Multilingual (English + Urdu)\n",
        "✅ Status: Ready for queries\n",
        "\"\"\"\n",
        "\n",
        "    def get_relevant_info(self, query: str, k: int = K_RETRIEVE) -> str:\n",
        "        if not self.vector_store:\n",
        "            return \"Knowledge base not available\"\n",
        "        try:\n",
        "            q = safe_str(query)\n",
        "            hits = self.vector_store.similarity_search(q, k=k)\n",
        "\n",
        "            snippets = []\n",
        "            nums_summary = []\n",
        "\n",
        "            for i, doc in enumerate(hits, start=1):\n",
        "                body = _limit_chars(doc.page_content, PER_DOC_CHARS)\n",
        "                snippets.append(f\"معلومات {i}: {body}\")\n",
        "\n",
        "                meta = doc.metadata or {}\n",
        "                if not isinstance(meta, dict):\n",
        "                    meta = {}\n",
        "                nd = meta.get(\"numerical_data\")\n",
        "                if isinstance(nd, dict):\n",
        "                    meta = {**meta, **nd}\n",
        "\n",
        "                if isinstance(meta.get(\"prices\"), list) and meta[\"prices\"]:\n",
        "                    nums_summary.append(f\"💰 قیمتیں: {', '.join(map(safe_str, meta['prices']))}\")\n",
        "                if isinstance(meta.get(\"percentages\"), list) and meta[\"percentages\"]:\n",
        "                    nums_summary.append(f\"📊 فیصد: {', '.join(map(safe_str, meta['percentages']))}%\")\n",
        "                if isinstance(meta.get(\"yields\"), list) and meta[\"yields\"]:\n",
        "                    y_fmt = []\n",
        "                    for y in meta[\"yields\"]:\n",
        "                        try:\n",
        "                            val, unit, per = y\n",
        "                            y_fmt.append(f\"{val} {unit} per {per}\")\n",
        "                        except Exception:\n",
        "                            y_fmt.append(safe_str(y))\n",
        "                    nums_summary.append(f\"🌾 پیداوار: {', '.join(y_fmt)}\")\n",
        "\n",
        "            context = \"\\n\\n\".join(snippets)\n",
        "            if nums_summary:\n",
        "                context = \"📈 اہم اعداد و شمار:\\n\" + \"\\n\".join(nums_summary) + \"\\n\\n\" + context\n",
        "\n",
        "            return _clip_context([context], MAX_CONTEXT_CHARS) or \"No relevant information found.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving information: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 🚀 Initialize RAG\n",
        "# =========================\n",
        "print(\"🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\")\n",
        "pak_agri_rag = AdvancedPakistaniAgriRAG()\n",
        "\n",
        "# =========================\n",
        "# 🎙️ Voice, 🌦️ Weather, 🤝 AI\n",
        "# =========================\n",
        "def voice_to_text(audio_file_path):\n",
        "    if not audio_file_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_file_path, language=\"ur\")\n",
        "        return normalize_mixed_text(result.get(\"text\", \"\"))\n",
        "    except Exception as e:\n",
        "        return f\"آواز سمجھ نہیں آئی: {e}\"\n",
        "\n",
        "def get_weather_with_farming_advice(city=\"Lahore\"):\n",
        "    try:\n",
        "        city = safe_str(city).strip() or \"Lahore\"\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric\"\n",
        "        resp = requests.get(url, timeout=20)\n",
        "        try:\n",
        "            data = resp.json()\n",
        "        except Exception:\n",
        "            return \"موسمی JSON درست نہیں۔\"\n",
        "\n",
        "        main = data.get(\"main\") or {}\n",
        "        wind = data.get(\"wind\") or {}\n",
        "        weather_l = data.get(\"weather\") or [{}]\n",
        "\n",
        "        temp = main.get(\"temp\")\n",
        "        humidity = main.get(\"humidity\")\n",
        "        wind_speed = wind.get(\"speed\")\n",
        "        description = weather_l[0].get(\"description\", \"\")\n",
        "\n",
        "        if any(v is None for v in (temp, humidity, wind_speed)):\n",
        "            return \"موسمی معلومات مکمل نہیں مل سکیں۔\"\n",
        "\n",
        "        if temp > 35:\n",
        "            advice = f\"⚠️ زیادہ گرمی ({temp}°C): صبح 6-8 بجے پانی دیں، دوپہر میں نہیں۔ پانی کی مقدار 20% بڑھائیں۔\"\n",
        "        elif humidity > 80:\n",
        "            advice = f\"🌧️ زیادہ نمی ({humidity}%): فنگیسائیڈ سپرے کریں۔ Mancozeb 2g/لیٹر یا Copper Oxychloride 3g/لیٹر۔\"\n",
        "        elif temp < 10:\n",
        "            advice = f\"❄️ سردی ({temp}°C): پودوں کو ڈھانپیں، پانی 50% کم دیں۔ Frost protection ضروری۔\"\n",
        "        elif wind_speed > 5:\n",
        "            advice = f\"💨 تیز ہوا ({wind_speed} m/s): کیڑے مار دوا کا سپرے نہ کریں۔ Wind barriers لگائیں۔\"\n",
        "        else:\n",
        "            advice = f\"✅ موسم اچھا ہے ({temp}°C, {humidity}% نمی): کھیتی کے کام کر سکتے ہیں۔\"\n",
        "\n",
        "        return f\"آج {city} میں {temp}°C، نمی {humidity}%، ہوا {wind_speed} m/s، موسم {description}\\n\\n{advice}\"\n",
        "    except Exception as e:\n",
        "        return f\"موسمی معلومات نہیں مل سکیں: {e}\"\n",
        "\n",
        "def text_to_voice(text):\n",
        "    try:\n",
        "        clean = normalize_mixed_text(text)\n",
        "        if len(clean) > 500:\n",
        "            clean = clean[:500] + \"... مکمل جواب اوپر پڑھیں\"\n",
        "        tts = gTTS(text=clean, lang=\"ur\", slow=False)\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n",
        "            tts.save(tmp.name)\n",
        "            return tmp.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_enhanced_ai_response(user_message: str, location: str = \"\") -> str:\n",
        "    relevant_context = pak_agri_rag.get_relevant_info(user_message)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are Zameen Dost, a Pakistani agriculture advisor. \"\n",
        "        \"Answer in simple Urdu, start with 'بھائی', use numbers when available, \"\n",
        "        \"and keep it concise and actionable. If weather is included, integrate it. \"\n",
        "        \"Only use the provided context; do not invent facts.\"\n",
        "    )\n",
        "\n",
        "    prompt_user = (\n",
        "        f\"Context:\\n{relevant_context}\\n\\n\"\n",
        "        f\"Location: {safe_str(location)}\\n\"\n",
        "        f\"Question: {safe_str(user_message)}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        chat = groq_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt_user},\n",
        "            ],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            max_tokens=MAX_OUTPUT_TOKENS,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        return chat.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        msg = safe_str(e)\n",
        "        if (\"rate_limit\" in msg) or (\"tokens per minute\" in msg) or (\"Request too large\" in msg):\n",
        "            return \"معذرت، پیغام بڑا تھا یا رفتار حد سے زیادہ تھی۔ براہِ کرم چھوٹا سوال کریں، یا دوبارہ کوشش کریں۔\"\n",
        "        return f\"معذرت، AI سے رابطہ نہیں ہو سکا: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 💬 Main chat handler\n",
        "# =========================\n",
        "def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):\n",
        "    user_message = \"\"\n",
        "    input_display = \"\"\n",
        "\n",
        "    if audio_input:\n",
        "        user_message = voice_to_text(audio_input)\n",
        "        input_display = f\"💬 آپ نے کہا: {user_message}\"\n",
        "    elif text_input:\n",
        "        user_message = safe_str(text_input)\n",
        "        input_display = f\"💬 آپ نے لکھا: {user_message}\"\n",
        "\n",
        "    if not isinstance(user_message, str) or not user_message.strip():\n",
        "        return \"کرپیا کوئی سوال پوچھیں\", None, \"❌ کوئی سوال نہیں ملا\"\n",
        "\n",
        "    enhanced = user_message\n",
        "    if focus_area and safe_str(focus_area) != \"عام سوال\":\n",
        "        enhanced += f\" (کسان کی دلچسپی: {focus_area})\"\n",
        "\n",
        "    terms = [\"موسم\", \"بارش\", \"پانی\", \"weather\", \"irrigation\", \"spray\", \"سپرے\"]\n",
        "    if isinstance(user_message, str) and any(t in user_message for t in terms):\n",
        "        weather_info = get_weather_with_farming_advice(city_name or \"Lahore\")\n",
        "        enhanced += f\"\\n\\nموسمی حالات: {weather_info}\"\n",
        "\n",
        "    ai_response = get_enhanced_ai_response(enhanced, city_name or \"\")\n",
        "    voice_response = text_to_voice(ai_response)\n",
        "    return input_display, voice_response, ai_response\n",
        "\n",
        "# =========================\n",
        "# 🖥️ UI\n",
        "# =========================\n",
        "with gr.Blocks(\n",
        "    title=\"Smart Zameen Dost - زمین دوست\",\n",
        "    theme=gr.themes.Base(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container { background: linear-gradient(135deg, #f8fdff 0%, #e8f7f8 100%); font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }\n",
        "    .header-box { background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin: 10px 0; border-left: 4px solid #2E8B57; }\n",
        "    .stats-box { background: linear-gradient(45deg, #e8f5e8, #f0f8e8); padding: 15px; border-radius: 8px; border: 1px solid #c8e6c9; margin: 10px 0; font-size: 0.9em; }\n",
        "    \"\"\"\n",
        ") as app:\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class='header-box'>\n",
        "          <div style='text-align: center;'>\n",
        "            <h1 style='color: #2E8B57; font-size: 2.2em; margin: 0 0 8px 0;'>🌾 Smart Zameen Dost</h1>\n",
        "            <p style='color: #666; font-size: 1.1em; margin: 0;'>پاکستانی کسانوں کا ذہین مشیر</p>\n",
        "          </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎤 اپنا سوال پوچھیں\")\n",
        "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"آواز میں پوچھیں\")\n",
        "            text_input = gr.Textbox(label=\"یا یہاں لکھیں (اردو/English)\", placeholder=\"مثال: کون سی فصل زیادہ منافع دے گی؟\", lines=2)\n",
        "            with gr.Row():\n",
        "                city_input = gr.Textbox(label=\"آپ کا شہر\", placeholder=\"Lahore, Karachi, Faisalabad\", value=\"Lahore\", scale=1)\n",
        "                focus_area = gr.Dropdown(\n",
        "                    label=\"دلچسپی کا شعبہ\",\n",
        "                    choices=[\"عام سوال\",\"برآمدی فصلیں\",\"گندم کی کاشت\",\"چاول کی کاشت\",\"کپاس کی کاشت\",\"سبزیوں کی کاشت\",\"پھلوں کی کاشت\",\"کھاد اور بیج\",\"بیماریوں کا علاج\",\"حکومتی اسکیمز\",\"منڈی کی قیمتیں\"],\n",
        "                    value=\"عام سوال\",\n",
        "                    scale=1,\n",
        "                )\n",
        "            chat_btn = gr.Button(\"🚀 جواب حاصل کریں\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🧠 ذہین جواب\")\n",
        "            input_display = gr.Textbox(label=\"آپ کا سوال\", lines=2, interactive=False)\n",
        "            audio_output = gr.Audio(label=\"🔊 آواز میں جواب\")\n",
        "            text_output = gr.Textbox(label=\"📝 تفصیلی جواب\", lines=10, interactive=False, show_copy_button=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        kb_stats = gr.HTML(value=pak_agri_rag.get_stats_html(), elem_classes=[\"stats-box\"])\n",
        "\n",
        "    chat_btn.click(\n",
        "        zameen_dost_advanced_chat,\n",
        "        inputs=[audio_input, text_input, city_input, focus_area],\n",
        "        outputs=[input_display, audio_output, text_output],\n",
        "    )\n",
        "\n",
        "print(\"🎉 App ready!\")\n",
        "print(f\"✅ Auto-processed {len(PREDEFINED_PDF_LINKS)} Google Drive PDF link(s)\")\n",
        "print(\"🔍 Multilingual RAG + Voice + Weather integrated\")\n",
        "\n",
        "gr.close_all()\n",
        "app.launch(share=True, debug=True, show_api=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "5RUCe2gEsQUX",
        "outputId": "a1aea076-3719-410c-8f3a-5cb9e806620b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Loading Whisper model...\n",
            "✅ Whisper model loaded.\n",
            "🔤 Loading multilingual sentence embeddings...\n",
            "✅ Multilingual embeddings loaded.\n",
            "🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\n",
            "✅ Seed agricultural knowledge initialized with 3 chunks.\n",
            "🚀 Auto-processing 1 Google Drive PDF(s)...\n",
            "✅ Doc 1: 322 pages → 1516 chunks\n",
            "🎉 Finished: 1/1 document(s) processed.\n",
            "🎉 App ready!\n",
            "✅ Auto-processed 1 Google Drive PDF link(s)\n",
            "🔍 Multilingual RAG + Voice + Weather integrated\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://2663e74ed3d1b0e2b5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2663e74ed3d1b0e2b5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://2663e74ed3d1b0e2b5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py — Smart Zameen Dost (trimmed context to avoid 413/TPM)\n",
        "# --------------------------------------------------------------\n",
        "# - Uses langchain_core.documents.Document (LangChain 0.2.x)\n",
        "# - Caps retrieval + context length to stay under Groq limits\n",
        "# - Robust membership tests, metadata handling\n",
        "# - Safe Google Drive PDF download + PyPDF2 extraction\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from gtts import gTTS\n",
        "import whisper\n",
        "import torch  # noqa: F401\n",
        "\n",
        "from groq import Groq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# =========================\n",
        "# 🔑 Keys (⚠️ hard-code only if you accept the risk)\n",
        "# =========================\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\" # paste locally\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"WEATHER_API_KEY\"] = WEATHER_API_KEY\n",
        "\n",
        "if not GROQ_API_KEY.strip():\n",
        "    raise RuntimeError(\"GROQ_API_KEY not set.\")\n",
        "if not WEATHER_API_KEY.strip():\n",
        "    raise RuntimeError(\"WEATHER_API_KEY not set.\")\n",
        "\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# =========================\n",
        "# 🔧 Global limits to keep prompt small\n",
        "# =========================\n",
        "K_RETRIEVE = 3               # fewer chunks retrieved\n",
        "PER_DOC_CHARS = 700          # per-chunk char cap (~175 tokens)\n",
        "MAX_CONTEXT_CHARS = 4000     # total context cap\n",
        "MAX_OUTPUT_TOKENS = 512      # shorter generations\n",
        "\n",
        "def _limit_chars(s: str, n: int) -> str:\n",
        "    s = str(s or \"\")\n",
        "    return s if len(s) <= n else (s[:n] + \" …\")\n",
        "\n",
        "def _clip_context(snippets, max_chars: int) -> str:\n",
        "    out, used = [], 0\n",
        "    for snip in snippets:\n",
        "        snip = str(snip or \"\")\n",
        "        if used + len(snip) > max_chars:\n",
        "            snip = snip[: max(0, max_chars - used)]\n",
        "        if snip:\n",
        "            out.append(snip)\n",
        "            used += len(snip)\n",
        "        if used >= max_chars:\n",
        "            break\n",
        "    return \"\\n\\n\".join(out)\n",
        "\n",
        "# =========================\n",
        "# 🔇 NLTK (quiet)\n",
        "# =========================\n",
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# 🤖 Models / Embeddings\n",
        "# =========================\n",
        "print(\"🤖 Loading Whisper model...\")\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "print(\"✅ Whisper model loaded.\")\n",
        "\n",
        "print(\"🔤 Loading multilingual sentence embeddings...\")\n",
        "multilingual_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "print(\"✅ Multilingual embeddings loaded.\")\n",
        "\n",
        "# =========================\n",
        "# 📄 Predefined Google Drive PDFs\n",
        "# =========================\n",
        "PREDEFINED_PDF_LINKS = [\n",
        "    \"https://drive.google.com/file/d/1H7b-1PG2SLB99gjogfSl7QTOmLd1iGX0/view?usp=sharing\",\n",
        "\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 🧰 Helpers\n",
        "# =========================\n",
        "def safe_str(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (bool, int, float)):\n",
        "        return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def normalize_mixed_text(text: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", safe_str(text)).strip()\n",
        "    return re.sub(\n",
        "        r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()\\-]\",\n",
        "        \" \",\n",
        "        s,\n",
        "    )\n",
        "\n",
        "def extract_numerical_data(text: str):\n",
        "    t = safe_str(text)\n",
        "    info = {}\n",
        "    prices = re.findall(r\"[\\$Rs\\.]\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", t)\n",
        "    if prices:\n",
        "        info[\"prices\"] = prices\n",
        "    perc = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*%\", t)\n",
        "    if perc:\n",
        "        info[\"percentages\"] = perc\n",
        "    yields = re.findall(\n",
        "        r\"(\\d+(?:\\.\\d+)?)\\s*(tons?|kg|quintals?|maunds?)\\s*(?:per|/)?\\s*(acre|hectare|ایکڑ)\",\n",
        "        t,\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "    if yields:\n",
        "        info[\"yields\"] = yields\n",
        "    return info\n",
        "\n",
        "# =========================\n",
        "# 📥 Google Drive PDF Processor\n",
        "# =========================\n",
        "class GoogleDrivePDFProcessor:\n",
        "    @staticmethod\n",
        "    def convert_gdrive_link(share_link: str):\n",
        "        patterns = [r\"/file/d/([a-zA-Z0-9\\-_]+)\", r\"id=([a-zA-Z0-9\\-_]+)\", r\"/d/([a-zA-Z0-9\\-_]+)\"]\n",
        "        file_id = None\n",
        "        link = safe_str(share_link)\n",
        "        for pat in patterns:\n",
        "            m = re.search(pat, link)\n",
        "            if m:\n",
        "                file_id = m.group(1)\n",
        "                break\n",
        "        if not file_id:\n",
        "            return None\n",
        "        return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pdf_from_gdrive(gdrive_link: str):\n",
        "        try:\n",
        "            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)\n",
        "            if not download_link:\n",
        "                return None, \"Invalid Google Drive link format\"\n",
        "\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "            resp = requests.get(download_link, headers=headers, stream=True, timeout=60)\n",
        "            txt = safe_str(resp.text)\n",
        "\n",
        "            if (\"confirm=\" in txt) or (\"virus scan warning\" in txt.lower()):\n",
        "                token = re.search(r\"confirm=([^&]+)\", txt)\n",
        "                if token:\n",
        "                    confirmed = f\"{download_link}&confirm={token.group(1)}\"\n",
        "                    resp = requests.get(confirmed, headers=headers, stream=True, timeout=60)\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                return resp.content, \"Success\"\n",
        "            return None, f\"Download failed: HTTP {resp.status_code}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Download error: {e}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_content: bytes):\n",
        "        try:\n",
        "            reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n",
        "            pages = len(reader.pages)\n",
        "            out = []\n",
        "            for i in range(pages):\n",
        "                try:\n",
        "                    pg = reader.pages[i]\n",
        "                    t = (pg.extract_text() or \"\").strip()\n",
        "                    if t:\n",
        "                        out.append(f\"\\n--- Page {i+1} ---\\n{t}\\n\")\n",
        "                except Exception:\n",
        "                    out.append(f\"\\n--- Page {i+1} (Error extracting) ---\\n\")\n",
        "            return \"\".join(out), pages\n",
        "        except Exception as e:\n",
        "            return f\"PDF text extraction error: {e}\", 0\n",
        "\n",
        "# =========================\n",
        "# 🧠 Knowledge Base / RAG\n",
        "# =========================\n",
        "class AdvancedPakistaniAgriRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = multilingual_embeddings\n",
        "        self.vector_store = None\n",
        "        self.gdrive = GoogleDrivePDFProcessor()\n",
        "        self.processed_documents = []\n",
        "        self._setup_seed_knowledge()\n",
        "        self._auto_process_predefined_pdfs()\n",
        "\n",
        "    def _setup_seed_knowledge(self):\n",
        "        seed = [\n",
        "            {\n",
        "                \"content\": \"\"\"Punjab Wheat Varieties for Export:\n",
        "                اعلیٰ قسم کی گندم کی اقسام:\n",
        "                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton\n",
        "                - Faisalabad-2008: High protein 12-14%, Premium export variety\n",
        "                - Galaxy-2013: Disease resistant, Suitable for UAE market\n",
        "                - Punjab-2011: Good for bread making, Export to Afghanistan\n",
        "                Urdu: یہ اقسام برآمد کے لیے بہترین ہیں اور زیادہ قیمت ملتی ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"crop_varieties\", \"region\": \"Punjab\", \"crop\": \"wheat\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Rice Export Opportunities - چاول کی برآمدات:\n",
        "                Basmati Varieties with International Prices:\n",
        "                - Super Basmati: $900-1200/ton (UAE, Saudi Arabia)\n",
        "                - Basmati 385: Premium grade, $1000-1300/ton\n",
        "                - IRRI-6: $450-550/ton (Philippines, Malaysia)\n",
        "                - Kainaat: $700-850/ton (Middle East markets)\n",
        "\n",
        "                Export Requirements:\n",
        "                - Moisture: Maximum 14%\n",
        "                - Broken grains: Less than 5%\n",
        "                - Length: Minimum 6.0mm for Basmati\n",
        "\n",
        "                اردو میں: بسمتی چاول کی برآمد سب سے زیادہ منافع بخش ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"export_markets\", \"crop\": \"rice\", \"price_range\": \"450-1300\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Government Support Schemes - حکومتی اسکیمز:\n",
        "                Kisan Card Program:\n",
        "                - 25% subsidy on fertilizers\n",
        "                - 20% discount on certified seeds\n",
        "                - Easy loan access through banks\n",
        "\n",
        "                Solar Tube Well Scheme:\n",
        "                - 60% government subsidy\n",
        "                - Remaining 40% through easy installments\n",
        "                - Electricity bill savings: Rs. 50,000+ annually\n",
        "\n",
        "                Crop Insurance Program:\n",
        "                - Premium: 5% of sum insured\n",
        "                - Government pays 75% of premium\n",
        "                - Coverage: Natural disasters, pest attacks\n",
        "\n",
        "                کسان ڈویلپمنٹ پروگرام سے مفت تربیت اور مشورے\"\"\",\n",
        "                \"metadata\": {\"type\": \"government_schemes\", \"schemes\": \"kisan_card,solar_tubewell,crop_insurance\", \"language\": \"mixed\"},\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        docs = []\n",
        "        for item in seed:\n",
        "            content = normalize_mixed_text(item[\"content\"])\n",
        "            meta = dict(item.get(\"metadata\") or {})\n",
        "            nums = extract_numerical_data(content)\n",
        "            if nums:\n",
        "                meta.update(nums)\n",
        "            docs.append(Document(page_content=content, metadata=meta))\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"], length_function=len\n",
        "        )\n",
        "        pieces = splitter.split_documents(docs)\n",
        "        self.vector_store = FAISS.from_documents(pieces, self.embeddings)\n",
        "        print(\"✅ Seed agricultural knowledge initialized with\", len(pieces), \"chunks.\")\n",
        "\n",
        "    def _auto_process_predefined_pdfs(self):\n",
        "        if not PREDEFINED_PDF_LINKS:\n",
        "            print(\"ℹ️ No predefined PDFs configured.\")\n",
        "            return\n",
        "\n",
        "        print(f\"🚀 Auto-processing {len(PREDEFINED_PDF_LINKS)} Google Drive PDF(s)...\")\n",
        "        ok = 0\n",
        "        for i, link in enumerate(PREDEFINED_PDF_LINKS, start=1):\n",
        "            try:\n",
        "                blob, msg = self.gdrive.download_pdf_from_gdrive(link)\n",
        "                if blob is None:\n",
        "                    print(f\"❌ Doc {i}: {msg}\")\n",
        "                    continue\n",
        "\n",
        "                text, pages = self.gdrive.extract_text_from_pdf(blob)\n",
        "                if \"pdf text extraction error\" in safe_str(text).lower():\n",
        "                    print(f\"❌ Doc {i}: {text}\")\n",
        "                    continue\n",
        "\n",
        "                if len(safe_str(text).strip()) < 100:\n",
        "                    print(f\"⚠️ Doc {i}: likely image-based or encrypted; minimal text.\")\n",
        "\n",
        "                processed = normalize_mixed_text(text)\n",
        "                numbers = extract_numerical_data(processed)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=processed,\n",
        "                    metadata={\n",
        "                        \"type\": \"auto_processed_pdf\",\n",
        "                        \"source\": f\"Auto PDF {i}\",\n",
        "                        \"pages\": pages,\n",
        "                        \"numerical_data\": numbers,\n",
        "                        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                        \"original_link\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"]\n",
        "                )\n",
        "                chunks = splitter.split_documents([doc])\n",
        "\n",
        "                if self.vector_store:\n",
        "                    self.vector_store.add_documents(chunks)\n",
        "                else:\n",
        "                    self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": pages, \"chunks\": len(chunks), \"source\": doc.metadata[\"original_link\"], \"status\": \"✅ Success\"}\n",
        "                )\n",
        "                print(f\"✅ Doc {i}: {pages} pages → {len(chunks)} chunks\")\n",
        "                ok += 1\n",
        "            except Exception as e:\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": 0, \"chunks\": 0, \"source\": link[:50] + \"...\" if len(link) > 50 else link, \"status\": f\"❌ Error: {e}\"}\n",
        "                )\n",
        "                print(f\"❌ Doc {i}: {e}\")\n",
        "\n",
        "        print(f\"🎉 Finished: {ok}/{len(PREDEFINED_PDF_LINKS)} document(s) processed.\")\n",
        "\n",
        "    def get_stats_html(self) -> str:\n",
        "        if not self.processed_documents:\n",
        "            return \"📊 Knowledge Base: Seed Pakistani agricultural data only (no PDFs yet)\"\n",
        "        total_chunks = sum(d.get(\"chunks\", 0) for d in self.processed_documents)\n",
        "        total_pages = sum(d.get(\"pages\", 0) for d in self.processed_documents)\n",
        "        return f\"\"\"📊 Knowledge Base Statistics:\n",
        "\n",
        "🗂️ Auto-processed Documents: {len(self.processed_documents)}\n",
        "📄 Total Pages Processed: {total_pages}\n",
        "🧩 Total Text Chunks: {total_chunks}\n",
        "📚 Seed Knowledge: Pakistani agriculture (Urdu + English)\n",
        "🔍 Search Capability: Multilingual (English + Urdu)\n",
        "✅ Status: Ready for queries\n",
        "\"\"\"\n",
        "\n",
        "    def get_relevant_info(self, query: str, k: int = K_RETRIEVE) -> str:\n",
        "        if not self.vector_store:\n",
        "            return \"Knowledge base not available\"\n",
        "        try:\n",
        "            q = safe_str(query)\n",
        "            hits = self.vector_store.similarity_search(q, k=k)\n",
        "\n",
        "            snippets = []\n",
        "            nums_summary = []\n",
        "\n",
        "            for i, doc in enumerate(hits, start=1):\n",
        "                body = _limit_chars(doc.page_content, PER_DOC_CHARS)\n",
        "                snippets.append(f\"معلومات {i}: {body}\")\n",
        "\n",
        "                meta = doc.metadata or {}\n",
        "                if not isinstance(meta, dict):\n",
        "                    meta = {}\n",
        "                nd = meta.get(\"numerical_data\")\n",
        "                if isinstance(nd, dict):\n",
        "                    meta = {**meta, **nd}\n",
        "\n",
        "                if isinstance(meta.get(\"prices\"), list) and meta[\"prices\"]:\n",
        "                    nums_summary.append(f\"💰 قیمتیں: {', '.join(map(safe_str, meta['prices']))}\")\n",
        "                if isinstance(meta.get(\"percentages\"), list) and meta[\"percentages\"]:\n",
        "                    nums_summary.append(f\"📊 فیصد: {', '.join(map(safe_str, meta['percentages']))}%\")\n",
        "                if isinstance(meta.get(\"yields\"), list) and meta[\"yields\"]:\n",
        "                    y_fmt = []\n",
        "                    for y in meta[\"yields\"]:\n",
        "                        try:\n",
        "                            val, unit, per = y\n",
        "                            y_fmt.append(f\"{val} {unit} per {per}\")\n",
        "                        except Exception:\n",
        "                            y_fmt.append(safe_str(y))\n",
        "                    nums_summary.append(f\"🌾 پیداوار: {', '.join(y_fmt)}\")\n",
        "\n",
        "            context = \"\\n\\n\".join(snippets)\n",
        "            if nums_summary:\n",
        "                context = \"📈 اہم اعداد و شمار:\\n\" + \"\\n\".join(nums_summary) + \"\\n\\n\" + context\n",
        "\n",
        "            return _clip_context([context], MAX_CONTEXT_CHARS) or \"No relevant information found.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving information: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 🚀 Initialize RAG\n",
        "# =========================\n",
        "print(\"🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\")\n",
        "pak_agri_rag = AdvancedPakistaniAgriRAG()\n",
        "\n",
        "# =========================\n",
        "# 🎙️ Voice, 🌦️ Weather, 🤝 AI\n",
        "# =========================\n",
        "def voice_to_text(audio_file_path):\n",
        "    if not audio_file_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_file_path, language=\"ur\")\n",
        "        return normalize_mixed_text(result.get(\"text\", \"\"))\n",
        "    except Exception as e:\n",
        "        return f\"آواز سمجھ نہیں آئی: {e}\"\n",
        "\n",
        "def get_weather_with_farming_advice(city=\"Lahore\"):\n",
        "    try:\n",
        "        city = safe_str(city).strip() or \"Lahore\"\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric\"\n",
        "        resp = requests.get(url, timeout=20)\n",
        "        try:\n",
        "            data = resp.json()\n",
        "        except Exception:\n",
        "            return \"موسمی JSON درست نہیں۔\"\n",
        "\n",
        "        main = data.get(\"main\") or {}\n",
        "        wind = data.get(\"wind\") or {}\n",
        "        weather_l = data.get(\"weather\") or [{}]\n",
        "\n",
        "        temp = main.get(\"temp\")\n",
        "        humidity = main.get(\"humidity\")\n",
        "        wind_speed = wind.get(\"speed\")\n",
        "        description = weather_l[0].get(\"description\", \"\")\n",
        "\n",
        "        if any(v is None for v in (temp, humidity, wind_speed)):\n",
        "            return \"موسمی معلومات مکمل نہیں مل سکیں۔\"\n",
        "\n",
        "        if temp > 35:\n",
        "            advice = f\"⚠️ زیادہ گرمی ({temp}°C): صبح 6-8 بجے پانی دیں، دوپہر میں نہیں۔ پانی کی مقدار 20% بڑھائیں۔\"\n",
        "        elif humidity > 80:\n",
        "            advice = f\"🌧️ زیادہ نمی ({humidity}%): فنگیسائیڈ سپرے کریں۔ Mancozeb 2g/لیٹر یا Copper Oxychloride 3g/لیٹر۔\"\n",
        "        elif temp < 10:\n",
        "            advice = f\"❄️ سردی ({temp}°C): پودوں کو ڈھانپیں، پانی 50% کم دیں۔ Frost protection ضروری۔\"\n",
        "        elif wind_speed > 5:\n",
        "            advice = f\"💨 تیز ہوا ({wind_speed} m/s): کیڑے مار دوا کا سپرے نہ کریں۔ Wind barriers لگائیں۔\"\n",
        "        else:\n",
        "            advice = f\"✅ موسم اچھا ہے ({temp}°C, {humidity}% نمی): کھیتی کے کام کر سکتے ہیں۔\"\n",
        "\n",
        "        return f\"آج {city} میں {temp}°C، نمی {humidity}%، ہوا {wind_speed} m/s، موسم {description}\\n\\n{advice}\"\n",
        "    except Exception as e:\n",
        "        return f\"موسمی معلومات نہیں مل سکیں: {e}\"\n",
        "\n",
        "def text_to_voice(text):\n",
        "    try:\n",
        "        clean = normalize_mixed_text(text)\n",
        "        if len(clean) > 500:\n",
        "            clean = clean[:500] + \"... مکمل جواب اوپر پڑھیں\"\n",
        "        tts = gTTS(text=clean, lang=\"ur\", slow=False)\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n",
        "            tts.save(tmp.name)\n",
        "            return tmp.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_enhanced_ai_response(user_message: str, location: str = \"\") -> str:\n",
        "    relevant_context = pak_agri_rag.get_relevant_info(user_message)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are Zameen Dost, a Pakistani agriculture advisor. \"\n",
        "        \"Answer in simple Urdu, start with 'بھائی', use numbers when available, \"\n",
        "        \"and keep it concise and actionable. If weather is included, integrate it. \"\n",
        "        \"Only use the provided context; do not invent facts.\"\n",
        "    )\n",
        "\n",
        "    prompt_user = (\n",
        "        f\"Context:\\n{relevant_context}\\n\\n\"\n",
        "        f\"Location: {safe_str(location)}\\n\"\n",
        "        f\"Question: {safe_str(user_message)}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        chat = groq_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt_user},\n",
        "            ],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            max_tokens=MAX_OUTPUT_TOKENS,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        return chat.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        msg = safe_str(e)\n",
        "        if (\"rate_limit\" in msg) or (\"tokens per minute\" in msg) or (\"Request too large\" in msg):\n",
        "            return \"معذرت، پیغام بڑا تھا یا رفتار حد سے زیادہ تھی۔ براہِ کرم چھوٹا سوال کریں، یا دوبارہ کوشش کریں۔\"\n",
        "        return f\"معذرت، AI سے رابطہ نہیں ہو سکا: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 💬 Main chat handler\n",
        "# =========================\n",
        "def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):\n",
        "    user_message = \"\"\n",
        "    input_display = \"\"\n",
        "\n",
        "    if audio_input:\n",
        "        user_message = voice_to_text(audio_input)\n",
        "        input_display = f\"💬 آپ نے کہا: {user_message}\"\n",
        "    elif text_input:\n",
        "        user_message = safe_str(text_input)\n",
        "        input_display = f\"💬 آپ نے لکھا: {user_message}\"\n",
        "\n",
        "    if not isinstance(user_message, str) or not user_message.strip():\n",
        "        return \"کرپیا کوئی سوال پوچھیں\", None, \"❌ کوئی سوال نہیں ملا\"\n",
        "\n",
        "    enhanced = user_message\n",
        "    if focus_area and safe_str(focus_area) != \"عام سوال\":\n",
        "        enhanced += f\" (کسان کی دلچسپی: {focus_area})\"\n",
        "\n",
        "    terms = [\"موسم\", \"بارش\", \"پانی\", \"weather\", \"irrigation\", \"spray\", \"سپرے\"]\n",
        "    if isinstance(user_message, str) and any(t in user_message for t in terms):\n",
        "        weather_info = get_weather_with_farming_advice(city_name or \"Lahore\")\n",
        "        enhanced += f\"\\n\\nموسمی حالات: {weather_info}\"\n",
        "\n",
        "    ai_response = get_enhanced_ai_response(enhanced, city_name or \"\")\n",
        "    voice_response = text_to_voice(ai_response)\n",
        "    return input_display, voice_response, ai_response\n",
        "\n",
        "# =========================\n",
        "# 🖥️ UI\n",
        "# =========================\n",
        "with gr.Blocks(\n",
        "    title=\"Smart Zameen Dost - زمین دوست\",\n",
        "    theme=gr.themes.Base(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container { background: linear-gradient(135deg, #f8fdff 0%, #e8f7f8 100%); font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }\n",
        "    .header-box { background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin: 10px 0; border-left: 4px solid #2E8B57; }\n",
        "    .stats-box { background: linear-gradient(45deg, #e8f5e8, #f0f8e8); padding: 15px; border-radius: 8px; border: 1px solid #c8e6c9; margin: 10px 0; font-size: 0.9em; }\n",
        "    \"\"\"\n",
        ") as app:\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class='header-box'>\n",
        "          <div style='text-align: center;'>\n",
        "            <h1 style='color: #2E8B57; font-size: 2.2em; margin: 0 0 8px 0;'>🌾 Smart Zameen Dost</h1>\n",
        "            <p style='color: #666; font-size: 1.1em; margin: 0;'>پاکستانی کسانوں کا ذہین مشیر</p>\n",
        "          </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎤 اپنا سوال پوچھیں\")\n",
        "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"آواز میں پوچھیں\")\n",
        "            text_input = gr.Textbox(label=\"یا یہاں لکھیں (اردو/English)\", placeholder=\"مثال: کون سی فصل زیادہ منافع دے گی؟\", lines=2)\n",
        "            with gr.Row():\n",
        "                city_input = gr.Textbox(label=\"آپ کا شہر\", placeholder=\"Lahore, Karachi, Faisalabad\", value=\"Lahore\", scale=1)\n",
        "                focus_area = gr.Dropdown(\n",
        "                    label=\"دلچسپی کا شعبہ\",\n",
        "                    choices=[\"عام سوال\",\"برآمدی فصلیں\",\"گندم کی کاشت\",\"چاول کی کاشت\",\"کپاس کی کاشت\",\"سبزیوں کی کاشت\",\"پھلوں کی کاشت\",\"کھاد اور بیج\",\"بیماریوں کا علاج\",\"حکومتی اسکیمز\",\"منڈی کی قیمتیں\"],\n",
        "                    value=\"عام سوال\",\n",
        "                    scale=1,\n",
        "                )\n",
        "            chat_btn = gr.Button(\"🚀 جواب حاصل کریں\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🧠 ذہین جواب\")\n",
        "            input_display = gr.Textbox(label=\"آپ کا سوال\", lines=2, interactive=False)\n",
        "            audio_output = gr.Audio(label=\"🔊 آواز میں جواب\")\n",
        "            text_output = gr.Textbox(label=\"📝 تفصیلی جواب\", lines=10, interactive=False, show_copy_button=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        kb_stats = gr.HTML(value=pak_agri_rag.get_stats_html(), elem_classes=[\"stats-box\"])\n",
        "\n",
        "    chat_btn.click(\n",
        "        zameen_dost_advanced_chat,\n",
        "        inputs=[audio_input, text_input, city_input, focus_area],\n",
        "        outputs=[input_display, audio_output, text_output],\n",
        "    )\n",
        "\n",
        "print(\"🎉 App ready!\")\n",
        "print(f\"✅ Auto-processed {len(PREDEFINED_PDF_LINKS)} Google Drive PDF link(s)\")\n",
        "print(\"🔍 Multilingual RAG + Voice + Weather integrated\")\n",
        "\n",
        "gr.close_all()\n",
        "app.launch(share=True, debug=True, show_api=False)\n"
      ],
      "metadata": {
        "id": "Q8nMdJtKytp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from groq import Groq\n",
        "\n",
        "# 🔊 NEW: STT & TTS\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "\n",
        "# =========================\n",
        "# 🔐 API Key (use env var)\n",
        "# =========================\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\"\n",
        "\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "# =========================\n",
        "# 🎙️ Load Whisper STT model\n",
        "# =========================\n",
        "WHISPER_MODEL_NAME = os.getenv(\"WHISPER_MODEL\", \"base\")\n",
        "try:\n",
        "    stt_model = whisper.load_model(WHISPER_MODEL_NAME)\n",
        "except Exception:\n",
        "    stt_model = whisper.load_model(\"tiny\")  # fallback if base isn't available\n",
        "\n",
        "# 📎 PDF Drive Links\n",
        "drive_links = {\n",
        "    \"PDF 1\": \"https://drive.google.com/file/d/16VRkuegHXbhQPPH6kB3jTcdlg1eh95Og/view?usp=sharing\",\n",
        "    \"PDF 2\": \"https://drive.google.com/file/d/1e4Zi9vYXHEtuU_mkpBKDjZ-s0fIFqdzO/view?usp=sharing\",\n",
        "    \"PDF 3\": \"https://drive.google.com/file/d/149Js-w01KO085cRibqXyra9_oPNRAYqZ/view?usp=sharing\"\n",
        "}\n",
        "\n",
        "# 📥 Download PDF\n",
        "def download_pdf_from_drive(drive_link):\n",
        "    try:\n",
        "        file_id = drive_link.split(\"/d/\")[1].split(\"/\")[0]\n",
        "        url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        if response.content[:4] != b\"%PDF\":\n",
        "            raise ValueError(\"Invalid PDF format\")\n",
        "        return BytesIO(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# 🧼 Clean OCR text\n",
        "def clean_ocr_text(text):\n",
        "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# 🧠 Extract Urdu text from all PDFs\n",
        "def extract_all_texts_from_drive(drive_links, max_pages=2):\n",
        "    all_text = \"\"\n",
        "    for title, url in drive_links.items():\n",
        "        file = download_pdf_from_drive(url)\n",
        "        if file:\n",
        "            try:\n",
        "                images = convert_from_bytes(file.read())[:max_pages]\n",
        "                for img in images:\n",
        "                    all_text += pytesseract.image_to_string(img, lang=\"urd\") + \"\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"❌ OCR failed for {title}: {e}\")\n",
        "    return clean_ocr_text(all_text)\n",
        "\n",
        "# 📚 Chunk text\n",
        "def chunk_text(text):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    return splitter.create_documents([text])\n",
        "\n",
        "# 💾 Create FAISS index\n",
        "def create_faiss_index(docs):\n",
        "    if not docs:\n",
        "        raise ValueError(\"❌ No chunks created from text.\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    return FAISS.from_documents(docs, embedding=embeddings)\n",
        "\n",
        "# 🤖 Query with Groq — returns *bilingual* with clear section headers\n",
        "def query_vector_db(query, db):\n",
        "    results = db.similarity_search(query, k=3)\n",
        "    if not results:\n",
        "        return \"❌ No relevant information found.\"\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use ONLY the context to answer.\n",
        "Return your reply in EXACTLY this format:\n",
        "\n",
        "English:\n",
        "<2–4 sentences in clear English.>\n",
        "\n",
        "Urdu:\n",
        "<2–4 sentences in Urdu script.>\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error from Groq: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 🎙️ Speech-to-Text helper\n",
        "# =========================\n",
        "def transcribe_audio(audio_path: str) -> str:\n",
        "    \"\"\"Transcribe spoken question (Urdu/English) with Whisper.\"\"\"\n",
        "    if not audio_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        result = stt_model.transcribe(audio_path, task=\"transcribe\")\n",
        "        return (result.get(\"text\") or \"\").strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ STT error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# =========================\n",
        "# 🔊 Text-to-Speech helper\n",
        "# =========================\n",
        "def tts_to_file(text: str, lang_code: str):\n",
        "    \"\"\"Synthesize TTS and return a temporary .mp3 file path or None.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return None\n",
        "    try:\n",
        "        tmp = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False)\n",
        "        tts = gTTS(text=text.strip(), lang=lang_code, slow=False)\n",
        "        tts.save(tmp.name)\n",
        "        return tmp.name\n",
        "    except Exception as e:\n",
        "        print(f\"❌ TTS error ({lang_code}): {e}\")\n",
        "        return None\n",
        "\n",
        "def split_bilingual(answer_text: str):\n",
        "    \"\"\"Extract English/Urdu parts from the structured answer.\"\"\"\n",
        "    en, ur = \"\", \"\"\n",
        "    m_en = re.search(r\"English:\\s*(.+?)(?:\\n\\s*Urdu:|$)\", answer_text, flags=re.S)\n",
        "    m_ur = re.search(r\"Urdu:\\s*(.+)$\", answer_text, flags=re.S)\n",
        "    if m_en: en = m_en.group(1).strip()\n",
        "    if m_ur: ur = m_ur.group(1).strip()\n",
        "    if not en and not ur:\n",
        "        en = answer_text.strip()\n",
        "    return en, ur\n",
        "\n",
        "# 🚀 Preprocessing once\n",
        "print(\"📦 Processing Urdu PDFs...\")\n",
        "extracted_text = extract_all_texts_from_drive(drive_links)\n",
        "print(f\"✅ Extracted {len(extracted_text)} characters of Urdu text\")\n",
        "\n",
        "documents = chunk_text(extracted_text)\n",
        "print(f\"📄 {len(documents)} chunks created\")\n",
        "\n",
        "vector_db = create_faiss_index(documents)\n",
        "print(\"✅ FAISS index created\")\n",
        "\n",
        "# =========================\n",
        "# 🧩 Gradio pipeline (voice + text)\n",
        "# =========================\n",
        "def answer_with_voice(audio_path, typed_query, speak_answer, speak_lang_choice):\n",
        "    \"\"\"\n",
        "    - If audio provided, transcribe it to form the query (auto language).\n",
        "    - If typed text provided, it takes precedence.\n",
        "    - Query the vector DB + Groq.\n",
        "    - Optionally return English and/or Urdu TTS.\n",
        "    \"\"\"\n",
        "    query = \"\"\n",
        "    if audio_path:\n",
        "        query = transcribe_audio(audio_path)\n",
        "    if typed_query and typed_query.strip():\n",
        "        query = typed_query.strip()\n",
        "\n",
        "    if not query:\n",
        "        return \"❌ Please ask a question (type or record).\", None, None\n",
        "\n",
        "    answer = query_vector_db(query, vector_db)\n",
        "    en_text, ur_text = split_bilingual(answer)\n",
        "\n",
        "    en_audio = ur_audio = None\n",
        "    if speak_answer:\n",
        "        if speak_lang_choice in (\"Both\", \"English only\") and en_text:\n",
        "            en_audio = tts_to_file(en_text, \"en\")\n",
        "        if speak_lang_choice in (\"Both\", \"Urdu only\") and ur_text:\n",
        "            ur_audio = tts_to_file(ur_text, \"ur\")\n",
        "\n",
        "    return answer, en_audio, ur_audio\n",
        "\n",
        "# 🎛 Gradio UI (Blocks)\n",
        "with gr.Blocks(title=\"📚 Urdu PDF QnA (Groq + FAISS + Voice)\") as demo:\n",
        "    gr.Markdown(\"## 📚 Urdu PDF QnA — Ask by voice or text\\nUses OCR ➜ FAISS ➜ Groq ➜ optional voice answer.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_in = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"🎙️ Speak or upload audio\")\n",
        "        text_in = gr.Textbox(label=\"💬 Or type your question (English or Roman Urdu)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        speak_toggle = gr.Checkbox(value=True, label=\"🔊 Speak the answer\")\n",
        "        speak_lang = gr.Radio(\n",
        "            choices=[\"Both\", \"English only\", \"Urdu only\"],\n",
        "            value=\"Both\",\n",
        "            label=\"Audio language\"\n",
        "        )\n",
        "\n",
        "    ask_btn = gr.Button(\"Ask\")\n",
        "    answer_out = gr.Textbox(label=\"📘 Answer\", lines=8)\n",
        "    en_audio_out = gr.Audio(label=\"🔊 English audio\", type=\"filepath\")\n",
        "    ur_audio_out = gr.Audio(label=\"🔊 Urdu audio\", type=\"filepath\")\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=answer_with_voice,\n",
        "        inputs=[audio_in, text_in, speak_toggle, speak_lang],\n",
        "        outputs=[answer_out, en_audio_out, ur_audio_out]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7jlXZs0Mwo_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py — Smart Zameen Dost (trimmed context + OCR fallback)\n",
        "# -----------------------------------------------------------\n",
        "# - LangChain 0.2.x (langchain_core.documents.Document)\n",
        "# - Caps retrieval + context to avoid Groq 413/TPM\n",
        "# - OCR fallback (pdf2image + pytesseract) for image-based PDFs\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from gtts import gTTS\n",
        "import whisper\n",
        "import torch  # noqa: F401\n",
        "\n",
        "from groq import Groq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# NEW: OCR libs\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "\n",
        "# =========================\n",
        "# 🔑 Keys (⚠️ hard-code only if you accept the risk)\n",
        "# =========================\n",
        "GROQ_API_KEY = \"YOUR_GROQ_KEY_HERE\"            # paste locally\n",
        "WEATHER_API_KEY = \"YOUR_OPENWEATHER_KEY_HERE\"\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"WEATHER_API_KEY\"] = WEATHER_API_KEY\n",
        "\n",
        "if not GROQ_API_KEY.strip():\n",
        "    raise RuntimeError(\"GROQ_API_KEY not set.\")\n",
        "if not WEATHER_API_KEY.strip():\n",
        "    raise RuntimeError(\"WEATHER_API_KEY not set.\")\n",
        "\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# =========================\n",
        "# 🔧 Global limits to keep prompt small\n",
        "# =========================\n",
        "K_RETRIEVE = 3\n",
        "PER_DOC_CHARS = 700\n",
        "MAX_CONTEXT_CHARS = 4000\n",
        "MAX_OUTPUT_TOKENS = 512\n",
        "\n",
        "def _limit_chars(s: str, n: int) -> str:\n",
        "    s = str(s or \"\")\n",
        "    return s if len(s) <= n else (s[:n] + \" …\")\n",
        "\n",
        "def _clip_context(snippets, max_chars: int) -> str:\n",
        "    out, used = [], 0\n",
        "    for snip in snippets:\n",
        "        snip = str(snip or \"\")\n",
        "        if used + len(snip) > max_chars:\n",
        "            snip = snip[: max(0, max_chars - used)]\n",
        "        if snip:\n",
        "            out.append(snip)\n",
        "            used += len(snip)\n",
        "        if used >= max_chars:\n",
        "            break\n",
        "    return \"\\n\\n\".join(out)\n",
        "\n",
        "# =========================\n",
        "# 🔇 NLTK (quiet)\n",
        "# =========================\n",
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# 🤖 Models / Embeddings\n",
        "# =========================\n",
        "print(\"🤖 Loading Whisper model...\")\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "print(\"✅ Whisper model loaded.\")\n",
        "\n",
        "print(\"🔤 Loading multilingual sentence embeddings...\")\n",
        "multilingual_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "print(\"✅ Multilingual embeddings loaded.\")\n",
        "\n",
        "# =========================\n",
        "# 📄 Predefined Google Drive PDFs\n",
        "# =========================\n",
        "PREDEFINED_PDF_LINKS = [\n",
        "    \"https://drive.google.com/file/d/1H7b-1PG2SLB99gjogfSl7QTOmLd1iGX0/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/16VRkuegHXbhQPPH6kB3jTcdlg1eh95Og/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1e4Zi9vYXHEtuU_mkpBKDjZ-s0fIFqdzO/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/149Js-w01KO085cRibqXyra9_oPNRAYqZ/view?usp=sharing\"\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 🧰 Helpers\n",
        "# =========================\n",
        "def safe_str(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, (bool, int, float)):\n",
        "        return str(x)\n",
        "    return str(x)\n",
        "\n",
        "def normalize_mixed_text(text: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", safe_str(text)).strip()\n",
        "    return re.sub(\n",
        "        r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()\\-]\",\n",
        "        \" \",\n",
        "        s,\n",
        "    )\n",
        "\n",
        "def extract_numerical_data(text: str):\n",
        "    t = safe_str(text)\n",
        "    info = {}\n",
        "    prices = re.findall(r\"[\\$Rs\\.]\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", t)\n",
        "    if prices:\n",
        "        info[\"prices\"] = prices\n",
        "    perc = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*%\", t)\n",
        "    if perc:\n",
        "        info[\"percentages\"] = perc\n",
        "    yields = re.findall(\n",
        "        r\"(\\d+(?:\\.\\d+)?)\\s*(tons?|kg|quintals?|maunds?)\\s*(?:per|/)?\\s*(acre|hectare|ایکڑ)\",\n",
        "        t,\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "    if yields:\n",
        "        info[\"yields\"] = yields\n",
        "    return info\n",
        "\n",
        "# =========================\n",
        "# 🖼️ OCR config\n",
        "# =========================\n",
        "OCR_LANGS = \"urd+eng\"   # requires tesseract-ocr-urd installed\n",
        "OCR_DPI = 300\n",
        "MAX_OCR_PAGES = None    # set an int to cap pages for speed (e.g., 10)\n",
        "\n",
        "def ocr_pdf_bytes(pdf_content: bytes, dpi: int = OCR_DPI, langs: str = OCR_LANGS, max_pages=MAX_OCR_PAGES):\n",
        "    \"\"\"OCR full/partial PDF bytes -> text, pages_processed.\"\"\"\n",
        "    images = convert_from_bytes(pdf_content, dpi=dpi)\n",
        "    if max_pages is not None:\n",
        "        images = images[:max_pages]\n",
        "    out = []\n",
        "    for i, img in enumerate(images, start=1):\n",
        "        try:\n",
        "            txt = pytesseract.image_to_string(img, lang=langs)\n",
        "            txt = (txt or \"\").strip()\n",
        "            if txt:\n",
        "                out.append(f\"\\n--- OCR Page {i} ---\\n{txt}\\n\")\n",
        "            else:\n",
        "                out.append(f\"\\n--- OCR Page {i} (no text) ---\\n\")\n",
        "        except Exception as e:\n",
        "            out.append(f\"\\n--- OCR Page {i} (error: {e}) ---\\n\")\n",
        "    return \"\".join(out), len(images)\n",
        "\n",
        "# =========================\n",
        "# 📥 Google Drive PDF Processor (with OCR fallback)\n",
        "# =========================\n",
        "class GoogleDrivePDFProcessor:\n",
        "    @staticmethod\n",
        "    def convert_gdrive_link(share_link: str):\n",
        "        patterns = [r\"/file/d/([a-zA-Z0-9\\-_]+)\", r\"id=([a-zA-Z0-9\\-_]+)\", r\"/d/([a-zA-Z0-9\\-_]+)\"]\n",
        "        file_id = None\n",
        "        link = safe_str(share_link)\n",
        "        for pat in patterns:\n",
        "            m = re.search(pat, link)\n",
        "            if m:\n",
        "                file_id = m.group(1)\n",
        "                break\n",
        "        if not file_id:\n",
        "            return None\n",
        "        return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pdf_from_gdrive(gdrive_link: str):\n",
        "        try:\n",
        "            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)\n",
        "            if not download_link:\n",
        "                return None, \"Invalid Google Drive link format\"\n",
        "\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "            resp = requests.get(download_link, headers=headers, stream=True, timeout=60)\n",
        "            txt = safe_str(resp.text)\n",
        "\n",
        "            if (\"confirm=\" in txt) or (\"virus scan warning\" in txt.lower()):\n",
        "                token = re.search(r\"confirm=([^&]+)\", txt)\n",
        "                if token:\n",
        "                    confirmed = f\"{download_link}&confirm={token.group(1)}\"\n",
        "                    resp = requests.get(confirmed, headers=headers, stream=True, timeout=60)\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                return resp.content, \"Success\"\n",
        "            return None, f\"Download failed: HTTP {resp.status_code}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Download error: {e}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_content: bytes):\n",
        "        \"\"\"\n",
        "        Extract text with PyPDF2; if too little text, fallback to OCR.\n",
        "        Returns (text, page_count).\n",
        "        \"\"\"\n",
        "        # 1) Try text extraction\n",
        "        try:\n",
        "            reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n",
        "            pages = len(reader.pages)\n",
        "            out = []\n",
        "            for i in range(pages):\n",
        "                try:\n",
        "                    pg = reader.pages[i]\n",
        "                    t = (pg.extract_text() or \"\").strip()\n",
        "                    if t:\n",
        "                        out.append(f\"\\n--- Page {i+1} ---\\n{t}\\n\")\n",
        "                except Exception:\n",
        "                    out.append(f\"\\n--- Page {i+1} (Error extracting) ---\\n\")\n",
        "            text = \"\".join(out)\n",
        "        except Exception as e:\n",
        "            text = f\"PDF text extraction error: {e}\"\n",
        "            pages = 0\n",
        "\n",
        "        # 2) Decide whether to OCR\n",
        "        # If extraction failed or text is tiny, OCR the PDF\n",
        "        if (pages == 0) or (len(re.sub(r\"\\s+\", \"\", safe_str(text))) < 100):\n",
        "            try:\n",
        "                ocr_text, ocr_pages = ocr_pdf_bytes(pdf_content, dpi=OCR_DPI, langs=OCR_LANGS, max_pages=MAX_OCR_PAGES)\n",
        "                if len(re.sub(r\"\\s+\", \"\", ocr_text)) >= 20:\n",
        "                    return ocr_text, (ocr_pages or pages or 0)\n",
        "            except Exception as _:\n",
        "                pass\n",
        "\n",
        "        return text, pages\n",
        "\n",
        "# =========================\n",
        "# 🧠 Knowledge Base / RAG\n",
        "# =========================\n",
        "class AdvancedPakistaniAgriRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = multilingual_embeddings\n",
        "        self.vector_store = None\n",
        "        self.gdrive = GoogleDrivePDFProcessor()\n",
        "        self.processed_documents = []\n",
        "        self._setup_seed_knowledge()\n",
        "        self._auto_process_predefined_pdfs()\n",
        "\n",
        "    def _setup_seed_knowledge(self):\n",
        "        seed = [\n",
        "            {\n",
        "                \"content\": \"\"\"Punjab Wheat Varieties for Export:\n",
        "                اعلیٰ قسم کی گندم کی اقسام:\n",
        "                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton\n",
        "                - Faisalabad-2008: High protein 12-14%, Premium export variety\n",
        "                - Galaxy-2013: Disease resistant, Suitable for UAE market\n",
        "                - Punjab-2011: Good for bread making, Export to Afghanistan\n",
        "                Urdu: یہ اقسام برآمد کے لیے بہترین ہیں اور زیادہ قیمت ملتی ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"crop_varieties\", \"region\": \"Punjab\", \"crop\": \"wheat\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Rice Export Opportunities - چاول کی برآمدات:\n",
        "                Basmati Varieties with International Prices:\n",
        "                - Super Basmati: $900-1200/ton (UAE, Saudi Arabia)\n",
        "                - Basmati 385: Premium grade, $1000-1300/ton\n",
        "                - IRRI-6: $450-550/ton (Philippines, Malaysia)\n",
        "                - Kainaat: $700-850/ton (Middle East markets)\n",
        "\n",
        "                Export Requirements:\n",
        "                - Moisture: Maximum 14%\n",
        "                - Broken grains: Less than 5%\n",
        "                - Length: Minimum 6.0mm for Basmati\n",
        "\n",
        "                اردو میں: بسمتی چاول کی برآمد سب سے زیادہ منافع بخش ہے\"\"\",\n",
        "                \"metadata\": {\"type\": \"export_markets\", \"crop\": \"rice\", \"price_range\": \"450-1300\", \"language\": \"mixed\"},\n",
        "            },\n",
        "            {\n",
        "                \"content\": \"\"\"Government Support Schemes - حکومتی اسکیمز:\n",
        "                Kisan Card Program:\n",
        "                - 25% subsidy on fertilizers\n",
        "                - 20% discount on certified seeds\n",
        "                - Easy loan access through banks\n",
        "\n",
        "                Solar Tube Well Scheme:\n",
        "                - 60% government subsidy\n",
        "                - Remaining 40% through easy installments\n",
        "                - Electricity bill savings: Rs. 50,000+ annually\n",
        "\n",
        "                Crop Insurance Program:\n",
        "                - Premium: 5% of sum insured\n",
        "                - Government pays 75% of premium\n",
        "                - Coverage: Natural disasters, pest attacks\n",
        "\n",
        "                کسان ڈویلپمنٹ پروگرام سے مفت تربیت اور مشورے\"\"\",\n",
        "                \"metadata\": {\"type\": \"government_schemes\", \"schemes\": \"kisan_card,solar_tubewell,crop_insurance\", \"language\": \"mixed\"},\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        docs = []\n",
        "        for item in seed:\n",
        "            content = normalize_mixed_text(item[\"content\"])\n",
        "            meta = dict(item.get(\"metadata\") or {})\n",
        "            nums = extract_numerical_data(content)\n",
        "            if nums:\n",
        "                meta.update(nums)\n",
        "            docs.append(Document(page_content=content, metadata=meta))\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"], length_function=len\n",
        "        )\n",
        "        pieces = splitter.split_documents(docs)\n",
        "        self.vector_store = FAISS.from_documents(pieces, self.embeddings)\n",
        "        print(\"✅ Seed agricultural knowledge initialized with\", len(pieces), \"chunks.\")\n",
        "\n",
        "    def _auto_process_predefined_pdfs(self):\n",
        "        if not PREDEFINED_PDF_LINKS:\n",
        "            print(\"ℹ️ No predefined PDFs configured.\")\n",
        "            return\n",
        "\n",
        "        print(f\"🚀 Auto-processing {len(PREDEFINED_PDF_LINKS)} Google Drive PDF(s)...\")\n",
        "        ok = 0\n",
        "        for i, link in enumerate(PREDEFINED_PDF_LINKS, start=1):\n",
        "            try:\n",
        "                blob, msg = self.gdrive.download_pdf_from_gdrive(link)\n",
        "                if blob is None:\n",
        "                    print(f\"❌ Doc {i}: {msg}\")\n",
        "                    continue\n",
        "\n",
        "                text, pages = self.gdrive.extract_text_from_pdf(blob)\n",
        "                if \"pdf text extraction error\" in safe_str(text).lower():\n",
        "                    print(f\"❌ Doc {i}: {text}\")\n",
        "                    continue\n",
        "\n",
        "                if len(safe_str(text).strip()) < 100:\n",
        "                    print(f\"ℹ️ Doc {i}: low native text — OCR likely used.\")\n",
        "\n",
        "                processed = normalize_mixed_text(text)\n",
        "                numbers = extract_numerical_data(processed)\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=processed,\n",
        "                    metadata={\n",
        "                        \"type\": \"auto_processed_pdf\",\n",
        "                        \"source\": f\"Auto PDF {i}\",\n",
        "                        \"pages\": pages,\n",
        "                        \"numerical_data\": numbers,\n",
        "                        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                        \"original_link\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"۔\", \".\", \":\", \";\", \" \"]\n",
        "                )\n",
        "                chunks = splitter.split_documents([doc])\n",
        "\n",
        "                if self.vector_store:\n",
        "                    self.vector_store.add_documents(chunks)\n",
        "                else:\n",
        "                    self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": pages, \"chunks\": len(chunks), \"source\": doc.metadata[\"original_link\"], \"status\": \"✅ Success\"}\n",
        "                )\n",
        "                print(f\"✅ Doc {i}: {pages} pages → {len(chunks)} chunks\")\n",
        "                ok += 1\n",
        "            except Exception as e:\n",
        "                self.processed_documents.append(\n",
        "                    {\"id\": i, \"pages\": 0, \"chunks\": 0, \"source\": link[:50] + \"...\" if len(link) > 50 else link, \"status\": f\"❌ Error: {e}\"}\n",
        "                )\n",
        "                print(f\"❌ Doc {i}: {e}\")\n",
        "\n",
        "        print(f\"🎉 Finished: {ok}/{len(PREDEFINED_PDF_LINKS)} document(s) processed.\")\n",
        "\n",
        "    def get_stats_html(self) -> str:\n",
        "        if not self.processed_documents:\n",
        "            return \"📊 Knowledge Base: Seed Pakistani agricultural data only (no PDFs yet)\"\n",
        "        total_chunks = sum(d.get(\"chunks\", 0) for d in self.processed_documents)\n",
        "        total_pages = sum(d.get(\"pages\", 0) for d in self.processed_documents)\n",
        "        return f\"\"\"📊 Knowledge Base Statistics:\n",
        "\n",
        "🗂️ Auto-processed Documents: {len(self.processed_documents)}\n",
        "📄 Total Pages Processed: {total_pages}\n",
        "🧩 Total Text Chunks: {total_chunks}\n",
        "📚 Seed Knowledge: Pakistani agriculture (Urdu + English)\n",
        "🔍 Search Capability: Multilingual (English + Urdu)\n",
        "✅ Status: Ready for queries\n",
        "\"\"\"\n",
        "\n",
        "    def get_relevant_info(self, query: str, k: int = K_RETRIEVE) -> str:\n",
        "        if not self.vector_store:\n",
        "            return \"Knowledge base not available\"\n",
        "        try:\n",
        "            q = safe_str(query)\n",
        "            hits = self.vector_store.similarity_search(q, k=k)\n",
        "\n",
        "            snippets = []\n",
        "            nums_summary = []\n",
        "\n",
        "            for i, doc in enumerate(hits, start=1):\n",
        "                body = _limit_chars(doc.page_content, PER_DOC_CHARS)\n",
        "                snippets.append(f\"معلومات {i}: {body}\")\n",
        "\n",
        "                meta = doc.metadata or {}\n",
        "                if not isinstance(meta, dict):\n",
        "                    meta = {}\n",
        "                nd = meta.get(\"numerical_data\")\n",
        "                if isinstance(nd, dict):\n",
        "                    meta = {**meta, **nd}\n",
        "\n",
        "                if isinstance(meta.get(\"prices\"), list) and meta[\"prices\"]:\n",
        "                    nums_summary.append(f\"💰 قیمتیں: {', '.join(map(safe_str, meta['prices']))}\")\n",
        "                if isinstance(meta.get(\"percentages\"), list) and meta[\"percentages\"]:\n",
        "                    nums_summary.append(f\"📊 فیصد: {', '.join(map(safe_str, meta['percentages']))}%\")\n",
        "                if isinstance(meta.get(\"yields\"), list) and meta[\"yields\"]:\n",
        "                    y_fmt = []\n",
        "                    for y in meta[\"yields\"]:\n",
        "                        try:\n",
        "                            val, unit, per = y\n",
        "                            y_fmt.append(f\"{val} {unit} per {per}\")\n",
        "                        except Exception:\n",
        "                            y_fmt.append(safe_str(y))\n",
        "                    nums_summary.append(f\"🌾 پیداوار: {', '.join(y_fmt)}\")\n",
        "\n",
        "            context = \"\\n\\n\".join(snippets)\n",
        "            if nums_summary:\n",
        "                context = \"📈 اہم اعداد و شمار:\\n\" + \"\\n\".join(nums_summary) + \"\\n\\n\" + context\n",
        "\n",
        "            return _clip_context([context], MAX_CONTEXT_CHARS) or \"No relevant information found.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving information: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 🚀 Initialize RAG\n",
        "# =========================\n",
        "print(\"🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\")\n",
        "pak_agri_rag = AdvancedPakistaniAgriRAG()\n",
        "\n",
        "# =========================\n",
        "# 🎙️ Voice, 🌦️ Weather, 🤝 AI\n",
        "# =========================\n",
        "def voice_to_text(audio_file_path):\n",
        "    if not audio_file_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_file_path, language=\"ur\")\n",
        "        return normalize_mixed_text(result.get(\"text\", \"\"))\n",
        "    except Exception as e:\n",
        "        return f\"آواز سمجھ نہیں آئی: {e}\"\n",
        "\n",
        "def get_weather_with_farming_advice(city=\"Lahore\"):\n",
        "    try:\n",
        "        city = safe_str(city).strip() or \"Lahore\"\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric\"\n",
        "        resp = requests.get(url, timeout=20)\n",
        "        try:\n",
        "            data = resp.json()\n",
        "        except Exception:\n",
        "            return \"موسمی JSON درست نہیں۔\"\n",
        "\n",
        "        main = data.get(\"main\") or {}\n",
        "        wind = data.get(\"wind\") or {}\n",
        "        weather_l = data.get(\"weather\") or [{}]\n",
        "\n",
        "        temp = main.get(\"temp\")\n",
        "        humidity = main.get(\"humidity\")\n",
        "        wind_speed = wind.get(\"speed\")\n",
        "        description = weather_l[0].get(\"description\", \"\")\n",
        "\n",
        "        if any(v is None for v in (temp, humidity, wind_speed)):\n",
        "            return \"موسمی معلومات مکمل نہیں مل سکیں۔\"\n",
        "\n",
        "        if temp > 35:\n",
        "            advice = f\"⚠️ زیادہ گرمی ({temp}°C): صبح 6-8 بجے پانی دیں، دوپہر میں نہیں۔ پانی کی مقدار 20% بڑھائیں۔\"\n",
        "        elif humidity > 80:\n",
        "            advice = f\"🌧️ زیادہ نمی ({humidity}%): فنگیسائیڈ سپرے کریں۔ Mancozeb 2g/لیٹر یا Copper Oxychloride 3g/لیٹر۔\"\n",
        "        elif temp < 10:\n",
        "            advice = f\"❄️ سردی ({temp}°C): پودوں کو ڈھانپیں، پانی 50% کم دیں۔ Frost protection ضروری۔\"\n",
        "        elif wind_speed > 5:\n",
        "            advice = f\"💨 تیز ہوا ({wind_speed} m/s): کیڑے مار دوا کا سپرے نہ کریں۔ Wind barriers لگائیں۔\"\n",
        "        else:\n",
        "            advice = f\"✅ موسم اچھا ہے ({temp}°C, {humidity}% نمی): کھیتی کے کام کر سکتے ہیں۔\"\n",
        "\n",
        "        return f\"آج {city} میں {temp}°C، نمی {humidity}%، ہوا {wind_speed} m/s، موسم {description}\\n\\n{advice}\"\n",
        "    except Exception as e:\n",
        "        return f\"موسمی معلومات نہیں مل سکیں: {e}\"\n",
        "\n",
        "def text_to_voice(text):\n",
        "    try:\n",
        "        clean = normalize_mixed_text(text)\n",
        "        if len(clean) > 500:\n",
        "            clean = clean[:500] + \"... مکمل جواب اوپر پڑھیں\"\n",
        "        tts = gTTS(text=clean, lang=\"ur\", slow=False)\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n",
        "            tts.save(tmp.name)\n",
        "            return tmp.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_enhanced_ai_response(user_message: str, location: str = \"\") -> str:\n",
        "    relevant_context = pak_agri_rag.get_relevant_info(user_message)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are Zameen Dost, a Pakistani agriculture advisor. \"\n",
        "        \"Answer in simple Urdu, start with 'بھائی', use numbers when available, \"\n",
        "        \"and keep it concise and actionable. If weather is included, integrate it. \"\n",
        "        \"Only use the provided context; do not invent facts.\"\n",
        "    )\n",
        "\n",
        "    prompt_user = (\n",
        "        f\"Context:\\n{relevant_context}\\n\\n\"\n",
        "        f\"Location: {safe_str(location)}\\n\"\n",
        "        f\"Question: {safe_str(user_message)}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        chat = groq_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt_user},\n",
        "            ],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            max_tokens=MAX_OUTPUT_TOKENS,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        return chat.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        msg = safe_str(e)\n",
        "        if (\"rate_limit\" in msg) or (\"tokens per minute\" in msg) or (\"Request too large\" in msg):\n",
        "            return \"معذرت، پیغام بڑا تھا یا رفتار حد سے زیادہ تھی۔ براہِ کرم چھوٹا سوال کریں، یا دوبارہ کوشش کریں۔\"\n",
        "        return f\"معذرت، AI سے رابطہ نہیں ہو سکا: {e}\"\n",
        "\n",
        "# =========================\n",
        "# 💬 Main chat handler\n",
        "# =========================\n",
        "def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):\n",
        "    user_message = \"\"\n",
        "    input_display = \"\"\n",
        "\n",
        "    if audio_input:\n",
        "        user_message = voice_to_text(audio_input)\n",
        "        input_display = f\"💬 آپ نے کہا: {user_message}\"\n",
        "    elif text_input:\n",
        "        user_message = safe_str(text_input)\n",
        "        input_display = f\"💬 آپ نے لکھا: {user_message}\"\n",
        "\n",
        "    if not isinstance(user_message, str) or not user_message.strip():\n",
        "        return \"کرپیا کوئی سوال پوچھیں\", None, \"❌ کوئی سوال نہیں ملا\"\n",
        "\n",
        "    enhanced = user_message\n",
        "    if focus_area and safe_str(focus_area) != \"عام سوال\":\n",
        "        enhanced += f\" (کسان کی دلچسپی: {focus_area})\"\n",
        "\n",
        "    terms = [\"موسم\", \"بارش\", \"پانی\", \"weather\", \"irrigation\", \"spray\", \"سپرے\"]\n",
        "    if isinstance(user_message, str) and any(t in user_message for t in terms):\n",
        "        weather_info = get_weather_with_farming_advice(city_name or \"Lahore\")\n",
        "        enhanced += f\"\\n\\nموسمی حالات: {weather_info}\"\n",
        "\n",
        "    ai_response = get_enhanced_ai_response(enhanced, city_name or \"\")\n",
        "    voice_response = text_to_voice(ai_response)\n",
        "    return input_display, voice_response, ai_response\n",
        "\n",
        "# =========================\n",
        "# 🖥️ UI\n",
        "# =========================\n",
        "with gr.Blocks(\n",
        "    title=\"Smart Zameen Dost - زمین دوست\",\n",
        "    theme=gr.themes.Base(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container { background: linear-gradient(135deg, #f8fdff 0%, #e8f7f8 100%); font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }\n",
        "    .header-box { background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin: 10px 0; border-left: 4px solid #2E8B57; }\n",
        "    .stats-box { background: linear-gradient(45deg, #e8f5e8, #f0f8e8); padding: 15px; border-radius: 8px; border: 1px solid #c8e6c9; margin: 10px 0; font-size: 0.9em; }\n",
        "    \"\"\"\n",
        ") as app:\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class='header-box'>\n",
        "          <div style='text-align: center;'>\n",
        "            <h1 style='color: #2E8B57; font-size: 2.2em; margin: 0 0 8px 0;'>🌾 Smart Zameen Dost</h1>\n",
        "            <p style='color: #666; font-size: 1.1em; margin: 0;'>پاکستانی کسانوں کا ذہین مشیر</p>\n",
        "          </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🎤 اپنا سوال پوچھیں\")\n",
        "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"آواز میں پوچھیں\")\n",
        "            text_input = gr.Textbox(label=\"یا یہاں لکھیں (اردو/English)\", placeholder=\"مثال: کون سی فصل زیادہ منافع دے گی؟\", lines=2)\n",
        "            with gr.Row():\n",
        "                city_input = gr.Textbox(label=\"آپ کا شہر\", placeholder=\"Lahore, Karachi, Faisalabad\", value=\"Lahore\", scale=1)\n",
        "                focus_area = gr.Dropdown(\n",
        "                    label=\"دلچسپی کا شعبہ\",\n",
        "                    choices=[\"عام سوال\",\"برآمدی فصلیں\",\"گندم کی کاشت\",\"چاول کی کاشت\",\"کپاس کی کاشت\",\"سبزیوں کی کاشت\",\"پھلوں کی کاشت\",\"کھاد اور بیج\",\"بیماریوں کا علاج\",\"حکومتی اسکیمز\",\"منڈی کی قیمتیں\"],\n",
        "                    value=\"عام سوال\",\n",
        "                    scale=1,\n",
        "                )\n",
        "            chat_btn = gr.Button(\"🚀 جواب حاصل کریں\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🧠 ذہین جواب\")\n",
        "            input_display = gr.Textbox(label=\"آپ کا سوال\", lines=2, interactive=False)\n",
        "            audio_output = gr.Audio(label=\"🔊 آواز میں جواب\")\n",
        "            text_output = gr.Textbox(label=\"📝 تفصیلی جواب\", lines=10, interactive=False, show_copy_button=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        kb_stats = gr.HTML(value=pak_agri_rag.get_stats_html(), elem_classes=[\"stats-box\"])\n",
        "\n",
        "    chat_btn.click(\n",
        "        zameen_dost_advanced_chat,\n",
        "        inputs=[audio_input, text_input, city_input, focus_area],\n",
        "        outputs=[input_display, audio_output, text_output],\n",
        "    )\n",
        "\n",
        "print(\"🎉 App ready!\")\n",
        "print(f\"✅ Auto-processed {len(PREDEFINED_PDF_LINKS)} Google Drive PDF link(s)\")\n",
        "print(\"🔍 Multilingual RAG + Voice + Weather integrated\")\n",
        "\n",
        "gr.close_all()\n",
        "app.launch(share=True, debug=True, show_api=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "OxO0jk4rzd4D",
        "outputId": "21696a98-550e-4718-afbc-3976723fadd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Loading Whisper model...\n",
            "✅ Whisper model loaded.\n",
            "🔤 Loading multilingual sentence embeddings...\n",
            "✅ Multilingual embeddings loaded.\n",
            "🧠 Initializing Advanced Pakistani Agricultural Knowledge Base...\n",
            "✅ Seed agricultural knowledge initialized with 3 chunks.\n",
            "🚀 Auto-processing 4 Google Drive PDF(s)...\n",
            "✅ Doc 1: 322 pages → 1516 chunks\n",
            "✅ Doc 2: 4 pages → 17 chunks\n",
            "✅ Doc 3: 4 pages → 10 chunks\n",
            "✅ Doc 4: 4 pages → 16 chunks\n",
            "🎉 Finished: 4/4 document(s) processed.\n",
            "🎉 App ready!\n",
            "✅ Auto-processed 4 Google Drive PDF link(s)\n",
            "🔍 Multilingual RAG + Voice + Weather integrated\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://14e474dde1316cb860.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://14e474dde1316cb860.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jh6f23dXzxth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}