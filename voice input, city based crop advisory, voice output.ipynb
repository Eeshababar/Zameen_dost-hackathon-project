{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "import groq\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import json\n",
        "import io\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urlparse\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "# RAG and ML Components\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Roboflow for plant disease detection\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "ROBOFLOW_CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"zL85GrIxufrU2bfcS2Sj\"\n",
        ")\n",
        "\n",
        "WEATHER_API_KEY = \"64be775708b8774817ae621feb910017\"\n",
        "WEATHER_URL = \"https://api.openweathermap.org/data/2.5\"\n",
        "\n",
        "# Initialize services\n",
        "print(\"ğŸ¤– Loading Whisper model...\")\n",
        "import whisper\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "print(\"âœ… Whisper model loaded!\")\n",
        "\n",
        "print(\"ğŸ¤– Loading multilingual embedding model...\")\n",
        "multilingual_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "print(\"âœ… Multilingual embeddings loaded!\")\n",
        "\n",
        "# PREDEFINED GOOGLE DRIVE PDF LINKS\n",
        "PREDEFINED_PDF_LINKS = [\n",
        "    \"https://drive.google.com/file/d/1H7b-1PG2SLB99gjogfSl7QTOmLd1iGX0/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1Dzj18fyJXfS4_7hISa6vR1uDYibKne3m/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1xizmdkdG76SR7tJh3SnK6kviBLeA8KzX/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/11x54pyujVtTerunzb-tcKrkaYKsLa3lb/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1jt5I5qSThcnYoGb2mD-zdRIUK-orV9UZ/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1M9_6NNUjd-TEpMmBF-Yoh85B7_Qd3FeK/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1HSwTjGxV5A5ThmIIXxZxGEZ--maOUJSd/view?usp=sharing\",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "class GoogleDrivePDFProcessor:\n",
        "    \"\"\"Handle Google Drive PDF extraction and processing\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_gdrive_link(share_link):\n",
        "        \"\"\"Convert Google Drive share link to direct download link\"\"\"\n",
        "        try:\n",
        "            # Extract file ID from various Google Drive link formats\n",
        "            patterns = [\n",
        "                r'/file/d/([a-zA-Z0-9-_]+)',\n",
        "                r'id=([a-zA-Z0-9-_]+)',\n",
        "                r'/d/([a-zA-Z0-9-_]+)',\n",
        "            ]\n",
        "\n",
        "            file_id = None\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, share_link)\n",
        "                if match:\n",
        "                    file_id = match.group(1)\n",
        "                    break\n",
        "\n",
        "            if not file_id:\n",
        "                return None\n",
        "\n",
        "            # Create direct download link\n",
        "            download_link = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "            return download_link\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting Google Drive link: {e}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pdf_from_gdrive(gdrive_link):\n",
        "        \"\"\"Download PDF from Google Drive link\"\"\"\n",
        "        try:\n",
        "            download_link = GoogleDrivePDFProcessor.convert_gdrive_link(gdrive_link)\n",
        "            if not download_link:\n",
        "                return None, \"Invalid Google Drive link format\"\n",
        "\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(download_link, headers=headers, stream=True)\n",
        "\n",
        "            # Handle Google Drive download confirmation\n",
        "            if 'confirm=' in response.url:\n",
        "                confirm_token = re.search(r'confirm=([^&]+)', response.url)\n",
        "                if confirm_token:\n",
        "                    confirmed_link = f\"{download_link}&confirm={confirm_token.group(1)}\"\n",
        "                    response = requests.get(confirmed_link, headers=headers, stream=True)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.content, \"Success\"\n",
        "            else:\n",
        "                return None, f\"Download failed: Status {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Download error: {str(e)}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_content):\n",
        "        \"\"\"Extract text from PDF content using PyPDF2\"\"\"\n",
        "        try:\n",
        "            # Create PDF reader from bytes\n",
        "            pdf_stream = io.BytesIO(pdf_content)\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_stream)\n",
        "\n",
        "            extracted_text = \"\"\n",
        "            page_count = len(pdf_reader.pages)\n",
        "\n",
        "            for page_num in range(page_count):\n",
        "                try:\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text = page.extract_text()\n",
        "                    if text.strip():  # Only add non-empty pages\n",
        "                        extracted_text += f\"\\n--- Page {page_num + 1} ---\\n{text}\\n\"\n",
        "                except Exception as page_error:\n",
        "                    print(f\"Error extracting page {page_num + 1}: {page_error}\")\n",
        "                    extracted_text += f\"\\n--- Page {page_num + 1} (Error) ---\\n\"\n",
        "\n",
        "            return extracted_text, page_count\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"PDF text extraction error: {str(e)}\", 0\n",
        "\n",
        "class AdvancedPakistaniAgriRAG:\n",
        "    def __init__(self):\n",
        "        self.embeddings = multilingual_embeddings\n",
        "        self.vector_store = None\n",
        "        self.numerical_data = {}\n",
        "        self.gdrive_processor = GoogleDrivePDFProcessor()\n",
        "        self.processed_documents = []  # Track processed documents\n",
        "        self.setup_knowledge_base()\n",
        "        # Automatically process predefined PDFs on initialization\n",
        "        self.auto_process_predefined_pdfs()\n",
        "\n",
        "    def process_mixed_language_text(self, text):\n",
        "        \"\"\"Process text that contains both English and Urdu\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        cleaned_text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()-]', ' ', text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def extract_numerical_data(self, text):\n",
        "        \"\"\"Extract prices, yields, percentages from text\"\"\"\n",
        "        numerical_info = {}\n",
        "\n",
        "        prices = re.findall(r'[\\$Rs\\.]\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)', text)\n",
        "        if prices:\n",
        "            numerical_info['prices'] = prices\n",
        "\n",
        "        percentages = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*%', text)\n",
        "        if percentages:\n",
        "            numerical_info['percentages'] = percentages\n",
        "\n",
        "        yields = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*(tons?|kg|quintals?)\\s*(?:per|/)?\\s*(acre|hectare|Ø§ÛŒÚ©Ú‘)', text, re.IGNORECASE)\n",
        "        if yields:\n",
        "            numerical_info['yields'] = yields\n",
        "\n",
        "        return numerical_info\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Create comprehensive Pakistani agricultural knowledge base\"\"\"\n",
        "\n",
        "        pakistani_agri_knowledge = [\n",
        "            {\n",
        "                \"content\": \"\"\"Punjab Wheat Varieties for Export:\n",
        "                Ø§Ø¹Ù„ÛŒÙ° Ù‚Ø³Ù… Ú©ÛŒ Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ø§Ù‚Ø³Ø§Ù…:\n",
        "                - Anmol-91: Yield 45-50 maunds/acre, Export price $320-350/ton\n",
        "                - Faisalabad-2008: High protein 12-14%, Premium export variety\n",
        "                - Galaxy-2013: Disease resistant, Suitable for UAE market\n",
        "                - Punjab-2011: Good for bread making, Export to Afghanistan\n",
        "                Urdu: ÛŒÛ Ø§Ù‚Ø³Ø§Ù… Ø¨Ø±Ø¢Ù…Ø¯ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ† ÛÛŒÚº Ø§ÙˆØ± Ø²ÛŒØ§Ø¯Û Ù‚ÛŒÙ…Øª Ù…Ù„ØªÛŒ ÛÛ’\"\"\",\n",
        "                \"metadata\": {\"type\": \"crop_varieties\", \"region\": \"Punjab\", \"crop\": \"wheat\", \"language\": \"mixed\"}\n",
        "            },\n",
        "            # Other knowledge entries...\n",
        "        ]\n",
        "\n",
        "        documents = []\n",
        "        for item in pakistani_agri_knowledge:\n",
        "            processed_content = self.process_mixed_language_text(item[\"content\"])\n",
        "            numerical_info = self.extract_numerical_data(processed_content)\n",
        "\n",
        "            metadata = item[\"metadata\"].copy()\n",
        "            if numerical_info:\n",
        "                metadata.update(numerical_info)\n",
        "\n",
        "            doc = Document(page_content=processed_content, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800,\n",
        "            chunk_overlap=100,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \":\", \";\", \" \"],\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        split_docs = text_splitter.split_documents(documents)\n",
        "        self.vector_store = FAISS.from_documents(split_docs, self.embeddings)\n",
        "\n",
        "        print(\"âœ… Advanced Pakistani Agricultural Knowledge Base Created!\")\n",
        "        print(f\"ğŸ“š Loaded {len(split_docs)} knowledge chunks with mixed language support\")\n",
        "\n",
        "    def auto_process_predefined_pdfs(self):\n",
        "        \"\"\"Automatically process predefined Google Drive PDFs\"\"\"\n",
        "        if not PREDEFINED_PDF_LINKS:\n",
        "            print(\"ğŸ“‹ No predefined PDF links found\")\n",
        "            return\n",
        "\n",
        "        print(f\"ğŸš€ Auto-processing {len(PREDEFINED_PDF_LINKS)} predefined PDF documents...\")\n",
        "        successfully_processed = 0\n",
        "\n",
        "        for i, link in enumerate(PREDEFINED_PDF_LINKS, 1):\n",
        "            try:\n",
        "                print(f\"ğŸ“¥ Processing document {i}/{len(PREDEFINED_PDF_LINKS)}...\")\n",
        "\n",
        "                # Download PDF\n",
        "                pdf_content, download_status = self.gdrive_processor.download_pdf_from_gdrive(link)\n",
        "\n",
        "                if pdf_content is None:\n",
        "                    print(f\"âŒ Document {i}: {download_status}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract text using PyPDF2\n",
        "                extracted_text, page_count = self.gdrive_processor.extract_text_from_pdf(pdf_content)\n",
        "\n",
        "                if \"error\" in extracted_text.lower():\n",
        "                    print(f\"âŒ Document {i}: {extracted_text}\")\n",
        "                    continue\n",
        "\n",
        "                # Check if we got meaningful text\n",
        "                if len(extracted_text.strip()) < 100:\n",
        "                    print(f\"âš ï¸ Document {i}: PDF may be image-based or encrypted. Limited text extracted.\")\n",
        "\n",
        "                # Process and add to knowledge base\n",
        "                processed_text = self.process_mixed_language_text(extracted_text)\n",
        "                numerical_info = self.extract_numerical_data(processed_text)\n",
        "\n",
        "                # Create document with metadata\n",
        "                doc = Document(\n",
        "                    page_content=processed_text,\n",
        "                    metadata={\n",
        "                        \"type\": \"auto_processed_pdf\",\n",
        "                        \"source\": f\"Auto PDF {i}\",\n",
        "                        \"pages\": page_count,\n",
        "                        \"numerical_data\": numerical_info,\n",
        "                        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                        \"original_link\": link[:50] + \"...\" if len(link) > 50 else link\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Split into chunks\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=800,\n",
        "                    chunk_overlap=100,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \":\", \";\", \" \"]\n",
        "                )\n",
        "\n",
        "                split_docs = text_splitter.split_documents([doc])\n",
        "\n",
        "                # Add to vector store\n",
        "                if self.vector_store:\n",
        "                    self.vector_store.add_documents(split_docs)\n",
        "                else:\n",
        "                    self.vector_store = FAISS.from_documents(split_docs, self.embeddings)\n",
        "\n",
        "                # Track processed document\n",
        "                self.processed_documents.append({\n",
        "                    \"id\": i,\n",
        "                    \"pages\": page_count,\n",
        "                    \"chunks\": len(split_docs),\n",
        "                    \"source\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    \"status\": \"âœ… Success\"\n",
        "                })\n",
        "\n",
        "                print(f\"âœ… Document {i}: {page_count} pages â†’ {len(split_docs)} chunks\")\n",
        "                successfully_processed += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Document {i}: Error - {str(e)}\")\n",
        "                self.processed_documents.append({\n",
        "                    \"id\": i,\n",
        "                    \"pages\": 0,\n",
        "                    \"chunks\": 0,\n",
        "                    \"source\": link[:50] + \"...\" if len(link) > 50 else link,\n",
        "                    \"status\": f\"âŒ Error: {str(e)}\"\n",
        "                })\n",
        "\n",
        "        print(f\"ğŸ‰ Auto-processing complete! {successfully_processed}/{len(PREDEFINED_PDF_LINKS)} documents processed successfully\")\n",
        "\n",
        "    def get_knowledge_base_stats(self):\n",
        "        \"\"\"Get current knowledge base statistics\"\"\"\n",
        "        total_chunks = sum(doc['chunks'] for doc in self.processed_documents if 'chunks' in doc)\n",
        "        total_pages = sum(doc['pages'] for doc in self.processed_documents if 'pages' in doc)\n",
        "\n",
        "        if not self.processed_documents:\n",
        "            return \"ğŸ“Š Knowledge Base: Default Pakistani agricultural data only\"\n",
        "\n",
        "        stats = f\"\"\"ğŸ“Š Knowledge Base Statistics:\n",
        "\n",
        "ğŸ—‚ï¸ Auto-processed Documents: {len(self.processed_documents)}\n",
        "ğŸ“„ Total Pages Processed: {total_pages}\n",
        "ğŸ§© Total Text Chunks: {total_chunks}\n",
        "ğŸ“š Default Knowledge: Pakistani agricultural data\n",
        "ğŸ” Search Capability: Multilingual (English + Urdu)\n",
        "âœ… Status: Ready for queries\n",
        "\"\"\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def get_relevant_info(self, query, k=4):\n",
        "        \"\"\"Get relevant information with numerical data\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return \"Knowledge base not available\"\n",
        "\n",
        "        try:\n",
        "            relevant_docs = self.vector_store.similarity_search(query, k=k)\n",
        "\n",
        "            context = \"\"\n",
        "            numerical_summary = \"\"\n",
        "\n",
        "            for i, doc in enumerate(relevant_docs, 1):\n",
        "                context += f\"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª {i}: {doc.page_content}\\n\\n\"\n",
        "\n",
        "                metadata = doc.metadata\n",
        "                if 'prices' in metadata:\n",
        "                    numerical_summary += f\"ğŸ’° Ù‚ÛŒÙ…ØªÛŒÚº: {', '.join(metadata['prices'])}\\n\"\n",
        "                if 'percentages' in metadata:\n",
        "                    numerical_summary += f\"ğŸ“Š ÙÛŒØµØ¯: {', '.join(metadata['percentages'])}%\\n\"\n",
        "                if 'yields' in metadata:\n",
        "                    numerical_summary += f\"ğŸŒ¾ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø±: {metadata['yields']}\\n\"\n",
        "\n",
        "            if numerical_summary:\n",
        "                context = f\"ğŸ“ˆ Ø§ÛÙ… Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø´Ù…Ø§Ø±:\\n{numerical_summary}\\n\\n{context}\"\n",
        "\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving information: {str(e)}\"\n",
        "\n",
        "# Initialize advanced RAG system\n",
        "print(\"ğŸ§  Initializing Advanced Pakistani Agricultural Knowledge Base...\")\n",
        "pak_agri_rag = AdvancedPakistaniAgriRAG()\n",
        "\n",
        "def voice_to_text(audio_file):\n",
        "    \"\"\"Convert farmer's voice to text with better error handling\"\"\"\n",
        "    if audio_file is None:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_file, language=\"ur\")\n",
        "        transcribed_text = result[\"text\"]\n",
        "        transcribed_text = pak_agri_rag.process_mixed_language_text(transcribed_text)\n",
        "        return transcribed_text\n",
        "    except Exception as e:\n",
        "        return f\"Ø¢ÙˆØ§Ø² Ø³Ù…Ø¬Ú¾ Ù†ÛÛŒÚº Ø¢Ø¦ÛŒ: {str(e)}\"\n",
        "\n",
        "def get_enhanced_ai_response(user_message, location=\"\"):\n",
        "    \"\"\"Get enhanced AI response with multilingual Pakistani knowledge\"\"\"\n",
        "\n",
        "    relevant_context = pak_agri_rag.get_relevant_info(user_message)\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    Ø¢Ù¾ \"Ø²Ù…ÛŒÙ† Ø¯ÙˆØ³Øª\" ÛÛŒÚº - Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ú©Ø³Ø§Ù†ÙˆÚº Ú©Û’ Ù…Ø§ÛØ± Ù…Ø´ÛŒØ±Û”\n",
        "\n",
        "    Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ø²Ø±Ø§Ø¹Øª Ú©ÛŒ ØªØ§Ø²Û ØªØ±ÛŒÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÛÛŒÚº (English Ø§ÙˆØ± Urdu Ù…ÛŒÚº):\n",
        "    {relevant_context}\n",
        "\n",
        "    Ú©Ø³Ø§Ù† Ú©Ø§ Ø¹Ù„Ø§Ù‚Û: {location}\n",
        "\n",
        "    Ø¢Ù¾ Ú©Ø§ Ú©Ø§Ù…:\n",
        "    1. Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ø­Ø§Ù„Ø§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ù…Ø´ÙˆØ±Û Ø¯ÛŒÙ†Ø§\n",
        "    2. Ø¨Ø±Ø¢Ù…Ø¯ÛŒ ÙØµÙ„ÙˆÚº Ú©ÛŒ ØªØ¬ÙˆÛŒØ² Ø¯ÛŒÙ†Ø§ (numerical data Ú©Û’ Ø³Ø§ØªÚ¾)\n",
        "    3. Ù…Ù‚Ø§Ù…ÛŒ Ø§Ù‚Ø³Ø§Ù… Ø§ÙˆØ± Ù‚ÛŒÙ…ØªÙˆÚº Ú©Ø§ Ø°Ú©Ø± Ú©Ø±Ù†Ø§\n",
        "    4. Ø­Ú©ÙˆÙ…ØªÛŒ Ø§Ø³Ú©ÛŒÙ…ÙˆÚº Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÛŒÙ†Ø§\n",
        "    5. Ù†Ù‚ØµØ§Ù† Ø³Û’ Ø¨Ú†Ø§Ø¤ Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ Ø¨ØªØ§Ù†Ø§\n",
        "    6. Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø´Ù…Ø§Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ (prices, yields, percentages)\n",
        "\n",
        "    Guidelines:\n",
        "    - ÛÙ…ÛŒØ´Û \"Ø¨Ú¾Ø§Ø¦ÛŒ\" Ú©ÛÛ Ú©Ø± Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº\n",
        "    - Ø¢Ø³Ø§Ù† Ø§Ø±Ø¯Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº\n",
        "    - Numbers Ø§ÙˆØ± prices Ø¶Ø±ÙˆØ± Ø¨ØªØ§Ø¦ÛŒÚº\n",
        "    - Export opportunities highlight Ú©Ø±ÛŒÚº\n",
        "    - Government schemes mention Ú©Ø±ÛŒÚº\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            model=\"llama3-8b-8192\",\n",
        "            max_tokens=1200,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "        return chat_completion.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Ù…Ø¹Ø°Ø±ØªØŒ AI Ø³Û’ Ø±Ø§Ø¨Ø·Û Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©Ø§: {str(e)}\"\n",
        "\n",
        "def get_weather_with_farming_advice(city=\"Lahore\"):\n",
        "    \"\"\"Enhanced weather with numerical farming advice\"\"\"\n",
        "    try:\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},PK&appid={WEATHER_API_KEY}&units=metric\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        temp = data['main']['temp']\n",
        "        humidity = data['main']['humidity']\n",
        "        wind_speed = data['wind']['speed']\n",
        "        description = data['weather'][0]['description']\n",
        "\n",
        "        farming_advice = \"\"\n",
        "        if temp > 35:\n",
        "            farming_advice = f\"âš ï¸ Ø²ÛŒØ§Ø¯Û Ú¯Ø±Ù…ÛŒ ({temp}Â°C): ØµØ¨Ø­ 6-8 Ø¨Ø¬Û’ Ù¾Ø§Ù†ÛŒ Ø¯ÛŒÚºØŒ Ø¯ÙˆÙ¾ÛØ± Ù…ÛŒÚº Ù†ÛÛŒÚºÛ” Ù¾Ø§Ù†ÛŒ Ú©ÛŒ Ù…Ù‚Ø¯Ø§Ø± 20% Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚºÛ”\"\n",
        "        elif humidity > 80:\n",
        "            farming_advice = f\"ğŸŒ§ï¸ Ø²ÛŒØ§Ø¯Û Ù†Ù…ÛŒ ({humidity}%): ÙÙ†Ú¯ÛŒØ³Ø§Ø¦ÛŒÚˆ Ø³Ù¾Ø±Û’ Ú©Ø±ÛŒÚºÛ” Mancozeb 2g/Ù„ÛŒÙ¹Ø± ÛŒØ§ Copper Oxychloride 3g/Ù„ÛŒÙ¹Ø±Û”\"\n",
        "        elif temp < 10:\n",
        "            farming_advice = f\"â„ï¸ Ø³Ø±Ø¯ÛŒ ({temp}Â°C): Ù¾ÙˆØ¯ÙˆÚº Ú©Ùˆ ÚˆÚ¾Ø§Ù†Ù¾ÛŒÚºØŒ Ù¾Ø§Ù†ÛŒ 50% Ú©Ù… Ø¯ÛŒÚºÛ” Frost protection Ø¶Ø±ÙˆØ±ÛŒÛ”\"\n",
        "        elif wind_speed > 5:\n",
        "            farming_advice = f\"ğŸ’¨ ØªÛŒØ² ÛÙˆØ§ ({wind_speed} m/s): Ú©ÛŒÚ‘Û’ Ù…Ø§Ø± Ø¯ÙˆØ§ Ú©Ø§ Ø³Ù¾Ø±Û’ Ù†Û Ú©Ø±ÛŒÚºÛ” Wind barriers Ù„Ú¯Ø§Ø¦ÛŒÚºÛ”\"\n",
        "        else:\n",
        "            farming_advice = f\"âœ… Ù…ÙˆØ³Ù… Ø§Ú†Ú¾Ø§ ÛÛ’ ({temp}Â°C, {humidity}% Ù†Ù…ÛŒ): Ú©Ú¾ÛŒØªÛŒ Ú©Û’ Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”\"\n",
        "\n",
        "        return f\"Ø¢Ø¬ {city} Ù…ÛŒÚº {temp}Â°CØŒ Ù†Ù…ÛŒ {humidity}%ØŒ ÛÙˆØ§ {wind_speed}m/sØŒ Ù…ÙˆØ³Ù… {description}\\n\\n{farming_advice}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Ù…ÙˆØ³Ù…ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÛÛŒÚº Ù…Ù„ Ø³Ú©ÛŒÚº: {str(e)}\"\n",
        "\n",
        "def text_to_voice(text):\n",
        "    \"\"\"Enhanced text to voice with better Urdu pronunciation\"\"\"\n",
        "    try:\n",
        "        clean_text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\w\\s.,;:!?()-]', ' ', text)\n",
        "        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "\n",
        "        if len(clean_text) > 500:\n",
        "            clean_text = clean_text[:500] + \"... Ù…Ú©Ù…Ù„ Ø¬ÙˆØ§Ø¨ Ø§ÙˆÙ¾Ø± Ù¾Ú‘Ú¾ÛŒÚº\"\n",
        "\n",
        "        tts = gTTS(text=clean_text, lang='ur', slow=False)\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:\n",
        "            tts.save(tmp_file.name)\n",
        "            return tmp_file.name\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def zameen_dost_advanced_chat(audio_input, text_input, city_name, focus_area):\n",
        "    \"\"\"Advanced main function with multilingual RAG and numerical data\"\"\"\n",
        "\n",
        "    user_message = \"\"\n",
        "    input_display = \"\"\n",
        "\n",
        "    if audio_input is not None:\n",
        "        user_message = voice_to_text(audio_input)\n",
        "        input_display = f\"ğŸ’¬ Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: {user_message}\"\n",
        "    elif text_input:\n",
        "        user_message = text_input\n",
        "        input_display = f\"ğŸ’¬ Ø¢Ù¾ Ù†Û’ Ù„Ú©Ú¾Ø§: {user_message}\"\n",
        "\n",
        "    if not user_message.strip():\n",
        "        return \"Ú©Ø±Ù¾ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ†Ú¾ÛŒÚº\", None, \"âŒ Ú©ÙˆØ¦ÛŒ Ø³ÙˆØ§Ù„ Ù†ÛÛŒÚº Ù…Ù„Ø§\"\n",
        "\n",
        "    enhanced_message = user_message\n",
        "    if focus_area != \"Ø¹Ø§Ù… Ø³ÙˆØ§Ù„\":\n",
        "        enhanced_message += f\" (Ú©Ø³Ø§Ù† Ú©ÛŒ Ø¯Ù„Ú†Ø³Ù¾ÛŒ: {focus_area})\"\n",
        "\n",
        "    if any(word in user_message for word in [\"Ù…ÙˆØ³Ù…\", \"Ø¨Ø§Ø±Ø´\", \"Ù¾Ø§Ù†ÛŒ\", \"weather\", \"irrigation\", \"spray\", \"Ø³Ù¾Ø±Û’\"]):\n",
        "        weather_info = get_weather_with_farming_advice(city_name)\n",
        "        enhanced_message += f\"\\n\\nÙ…ÙˆØ³Ù…ÛŒ Ø­Ø§Ù„Ø§Øª: {weather_info}\"\n",
        "\n",
        "    ai_response = get_enhanced_ai_response(enhanced_message, city_name)\n",
        "    voice_response = text_to_voice(ai_response)\n",
        "\n",
        "    return input_display, voice_response, ai_response\n",
        "\n",
        "# Plant disease detection function\n",
        "def predict_disease(image_path):\n",
        "    try:\n",
        "        result = ROBOFLOW_CLIENT.infer(image_path, model_id=\"plant-disease-detection-v2-2nclk/1\")\n",
        "        if not result.get(\"predictions\"):\n",
        "            return \"âŒ Ú©ÙˆØ¦ÛŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù…Ø¹Ù„ÙˆÙ… Ù†ÛÛŒÚº ÛÙˆØ¦ÛŒ\"\n",
        "        pred = result[\"predictions\"][0]\n",
        "        return f\"ğŸ¦  Ø¨ÛŒÙ…Ø§Ø±ÛŒ: {pred['class']}\\nØ§Ø¹ØªÙ…Ø§Ø¯: {pred['confidence']*100:.2f}%\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Roboflow Error: {e}\"\n",
        "\n",
        "# Weather functions\n",
        "def get_current_weather(city):\n",
        "    try:\n",
        "        params = {\"q\": city, \"appid\": WEATHER_API_KEY, \"units\": \"metric\", \"lang\": \"ur\"}\n",
        "        res = requests.get(f\"{WEATHER_URL}/weather\", params=params).json()\n",
        "        if res.get(\"cod\") != 200:\n",
        "            return \"âŒ Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù… Ø¯Ø±Ø³Øª Ù†ÛÛŒÚº\"\n",
        "        return (\n",
        "            f\"ğŸŒ¤ï¸ Ù…ÙˆØ³Ù…: {res['weather'][0]['description']}\\n\"\n",
        "            f\"ğŸŒ¡ï¸ Ø¯Ø±Ø¬Û Ø­Ø±Ø§Ø±Øª: {res['main']['temp']}Â°C (Ù…Ø­Ø³ÙˆØ³: {res['main']['feels_like']}Â°C)\\n\"\n",
        "            f\"ğŸ’§ Ù†Ù…ÛŒ: {res['main']['humidity']}%\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Weather API Error: {e}\"\n",
        "\n",
        "def get_tomorrow_weather(city):\n",
        "    try:\n",
        "        params = {\"q\": city, \"appid\": WEATHER_API_KEY, \"units\": \"metric\", \"lang\": \"ur\"}\n",
        "        res = requests.get(f\"{WEATHER_URL}/forecast\", params=params).json()\n",
        "        if res.get(\"cod\") != \"200\":\n",
        "            return \"âŒ Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù… Ø¯Ø±Ø³Øª Ù†ÛÛŒÚº\"\n",
        "\n",
        "        tomorrow = (datetime.utcnow() + timedelta(days=1)).date()\n",
        "        for entry in res[\"list\"]:\n",
        "            dt_txt = entry[\"dt_txt\"]\n",
        "            if str(tomorrow) in dt_txt and \"12:00:00\" in dt_txt:\n",
        "                description = entry[\"weather\"][0][\"description\"]\n",
        "                return (\n",
        "                    f\"ğŸ—“ï¸ Ú©Ù„ Ú©Ø§ Ù…ÙˆØ³Ù…: {description}\\n\"\n",
        "                    f\"ğŸŒ¡ï¸ Ø¯Ø±Ø¬Û Ø­Ø±Ø§Ø±Øª: {entry['main']['temp']}Â°C\\n\"\n",
        "                    f\"ğŸ’§ Ù†Ù…ÛŒ: {entry['main']['humidity']}%\"\n",
        "                )\n",
        "        return \"âŒ Ú©Ù„ Ú©ÛŒ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ Ù†ÛÛŒÚº Ù…Ù„ÛŒ\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ API Error: {e}\"\n",
        "\n",
        "# Create the enhanced app with lighter UI\n",
        "with gr.Blocks(\n",
        "    title=\"Smart Zameen Dost - Ø²Ù…ÛŒÙ† Ø¯ÙˆØ³Øª\",\n",
        "    theme=gr.themes.Base(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container {\n",
        "        background: linear-gradient(135deg, #f8fdff 0%, #e8f7f8 100%);\n",
        "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "    }\n",
        "    .header-box {\n",
        "        background: white;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
        "        margin: 10px 0;\n",
        "        border-left: 4px solid #2E8B57;\n",
        "    }\n",
        "    .stats-box {\n",
        "        background: linear-gradient(45deg, #e8f5e8, #f0f8e8);\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #c8e6c9;\n",
        "        margin: 10px 0;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as app:\n",
        "\n",
        "    # Clean and simple header\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class='header-box'>\n",
        "        <div style='text-align: center;'>\n",
        "            <h1 style='color: #2E8B57; font-size: 2.2em; margin: 0 0 8px 0;'>ğŸŒ¾ Smart Zameen Dost</h1>\n",
        "            <p style='color: #666; font-size: 1.1em; margin: 0;'>Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ú©Ø³Ø§Ù†ÙˆÚº Ú©Ø§ Ø°ÛÛŒÙ† Ù…Ø´ÛŒØ±</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ¤ Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ†Ú¾ÛŒÚº\")\n",
        "\n",
        "            audio_input = gr.Audio(\n",
        "                sources=[\"microphone\"],\n",
        "                type=\"filepath\",\n",
        "                label=\"Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ù¾ÙˆÚ†Ú¾ÛŒÚº\"\n",
        "            )\n",
        "\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"ÛŒØ§ ÛŒÛØ§Úº Ù„Ú©Ú¾ÛŒÚº (Ø§Ø±Ø¯Ùˆ/English)\",\n",
        "                placeholder=\"Ù…Ø«Ø§Ù„: Ú©ÙˆÙ† Ø³ÛŒ ÙØµÙ„ Ø²ÛŒØ§Ø¯Û Ù…Ù†Ø§ÙØ¹ Ø¯Û’ Ú¯ÛŒØŸ\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                city_input = gr.Textbox(\n",
        "                    label=\"Ø¢Ù¾ Ú©Ø§ Ø´ÛØ±\",\n",
        "                    placeholder=\"Lahore, Karachi, Faisalabad\",\n",
        "                    value=\"Lahore\",\n",
        "                    scale=1\n",
        "                )\n",
        "\n",
        "                focus_area = gr.Dropdown(\n",
        "                    label=\"Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ú©Ø§ Ø´Ø¹Ø¨Û\",\n",
        "                    choices=[\n",
        "                        \"Ø¹Ø§Ù… Ø³ÙˆØ§Ù„\",\n",
        "                        \"Ø¨Ø±Ø¢Ù…Ø¯ÛŒ ÙØµÙ„ÛŒÚº\",\n",
        "                        \"Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ú©Ø§Ø´Øª\",\n",
        "                        \"Ú†Ø§ÙˆÙ„ Ú©ÛŒ Ú©Ø§Ø´Øª\",\n",
        "                        \"Ú©Ù¾Ø§Ø³ Ú©ÛŒ Ú©Ø§Ø´Øª\",\n",
        "                        \"Ø³Ø¨Ø²ÛŒÙˆÚº Ú©ÛŒ Ú©Ø§Ø´Øª\",\n",
        "                        \"Ù¾Ú¾Ù„ÙˆÚº Ú©ÛŒ Ú©Ø§Ø´Øª\",\n",
        "                        \"Ú©Ú¾Ø§Ø¯ Ø§ÙˆØ± Ø¨ÛŒØ¬\",\n",
        "                        \"Ø¨ÛŒÙ…Ø§Ø±ÛŒÙˆÚº Ú©Ø§ Ø¹Ù„Ø§Ø¬\",\n",
        "                        \"Ø­Ú©ÙˆÙ…ØªÛŒ Ø§Ø³Ú©ÛŒÙ…Ø²\",\n",
        "                        \"Ù…Ù†ÚˆÛŒ Ú©ÛŒ Ù‚ÛŒÙ…ØªÛŒÚº\"\n",
        "                    ],\n",
        "                    value=\"Ø¹Ø§Ù… Ø³ÙˆØ§Ù„\",\n",
        "                    scale=1\n",
        "                )\n",
        "\n",
        "            chat_btn = gr.Button(\"ğŸš€ Ø¬ÙˆØ§Ø¨ Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ§  Ø°ÛÛŒÙ† Ø¬ÙˆØ§Ø¨\")\n",
        "\n",
        "            input_display = gr.Textbox(\n",
        "                label=\"Ø¢Ù¾ Ú©Ø§ Ø³ÙˆØ§Ù„\",\n",
        "                lines=2,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            audio_output = gr.Audio(\n",
        "                label=\"ğŸ”Š Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨\"\n",
        "            )\n",
        "\n",
        "            text_output = gr.Textbox(\n",
        "                label=\"ğŸ“ ØªÙØµÛŒÙ„ÛŒ Ø¬ÙˆØ§Ø¨\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    # Knowledge base statistics\n",
        "    with gr.Row():\n",
        "        kb_stats = gr.HTML(\n",
        "            value=pak_agri_rag.get_knowledge_base_stats(),\n",
        "            elem_classes=[\"stats-box\"]\n",
        "        )\n",
        "\n",
        "    # Connect chat function\n",
        "    chat_btn.click(\n",
        "        zameen_dost_advanced_chat,\n",
        "        inputs=[audio_input, text_input, city_input, focus_area],\n",
        "        outputs=[input_display, audio_output, text_output]\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"ğŸŒ¿ Ù¾ÙˆØ¯ÙˆÚº Ú©ÛŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ\"):\n",
        "        img_input = gr.Image(type=\"filepath\", label=\"ğŸ“· Ù¾ØªÛ’ Ú©ÛŒ ØªØµÙˆÛŒØ±\")\n",
        "        disease_output = gr.Textbox(label=\"ğŸ” Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©ÛŒ ØªÙØµÛŒÙ„\")\n",
        "        gr.Button(\"ğŸ” Ø´Ù†Ø§Ø®Øª Ú©Ø±ÛŒÚº\").click(fn=predict_disease, inputs=img_input, outputs=disease_output)\n",
        "\n",
        "    with gr.Tab(\"ğŸŒ¦ï¸ Ø¢Ø¬ Ú©Ø§ Ù…ÙˆØ³Ù…\"):\n",
        "        city_now = gr.Textbox(label=\"ğŸ™ï¸ Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù…\")\n",
        "        weather_now = gr.Textbox(label=\"â˜ï¸ Ù…ÙˆØ³Ù…\")\n",
        "        gr.Button(\"Ù…ÙˆØ³Ù… Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº\").click(fn=get_current_weather, inputs=city_now, outputs=weather_now)\n",
        "\n",
        "    with gr.Tab(\"ğŸ“… Ú©Ù„ Ú©Ø§ Ù…ÙˆØ³Ù…\"):\n",
        "        city_tomorrow = gr.Textbox(label=\"ğŸ™ï¸ Ø´ÛØ± Ú©Ø§ Ù†Ø§Ù…\")\n",
        "        weather_tomorrow = gr.Textbox(label=\"ğŸŒ§ï¸ Ù…ÙˆØ³Ù… Ú©ÛŒ ØªÙØµÛŒÙ„\")\n",
        "        gr.Button(\"Ú©Ù„ Ú©Ø§ Ù…ÙˆØ³Ù… Ø¯ÛŒÚ©Ú¾ÛŒÚº\").click(fn=get_tomorrow_weather, inputs=city_tomorrow, outputs=weather_tomorrow)\n",
        "\n",
        "    # Simple footer\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style='text-align: center; padding: 15px; margin-top: 20px; background: rgba(46,139,87,0.1); border-radius: 10px;'>\n",
        "        <p style='color: #2E8B57; margin: 0; font-size: 1em; font-weight: bold;'>Smart Zameen Dost - Made for Pakistani Farmers ğŸ‡µğŸ‡°</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "print(\"ğŸ‰ Smart Zameen Dost with Auto PDF Processing is ready!\")\n",
        "print(f\"\\nâœ… Auto-processed {len(PREDEFINED_PDF_LINKS)} PDF documents from Google Drive\")\n",
        "print(\"ğŸ“Š Knowledge base includes default Pakistani agricultural data + your PDFs\")\n",
        "print(\"ğŸ” Multilingual search capability (English + Urdu)\")\n",
        "print(\"ğŸ¤ Voice input and output support\")\n",
        "print(\"ğŸŒ¡ï¸ Weather-based farming advice\")\n",
        "print(\"ğŸ’° Export market data with prices\")\n",
        "print(\"\\nğŸ“‹ To add more PDFs:\")\n",
        "print(\"  1. Add Google Drive share links to PREDEFINED_PDF_LINKS list\")\n",
        "print(\"  2. Restart the application\")\n",
        "print(\"  3. PDFs will be automatically processed and added to knowledge base\")\n",
        "print(\"\\nğŸš€ Application is ready to serve Pakistani farmers!\")\n",
        "\n",
        "# Launch the enhanced app\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V4FBk-8ovEE0",
        "outputId": "96d4e816-83c0-4a49-b2f3-6ceb53d8bb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Loading Whisper model...\n",
            "âœ… Whisper model loaded!\n",
            "ğŸ¤– Loading multilingual embedding model...\n",
            "âœ… Multilingual embeddings loaded!\n",
            "ğŸ§  Initializing Advanced Pakistani Agricultural Knowledge Base...\n",
            "âœ… Advanced Pakistani Agricultural Knowledge Base Created!\n",
            "ğŸ“š Loaded 1 knowledge chunks with mixed language support\n",
            "ğŸš€ Auto-processing 8 predefined PDF documents...\n",
            "ğŸ“¥ Processing document 1/8...\n",
            "âœ… Document 1: 322 pages â†’ 1516 chunks\n",
            "ğŸ“¥ Processing document 2/8...\n",
            "âœ… Document 2: 1 pages â†’ 2 chunks\n",
            "ğŸ“¥ Processing document 3/8...\n",
            "âœ… Document 3: 1 pages â†’ 2 chunks\n",
            "ğŸ“¥ Processing document 4/8...\n",
            "âœ… Document 4: 1 pages â†’ 1 chunks\n",
            "ğŸ“¥ Processing document 5/8...\n",
            "âœ… Document 5: 1 pages â†’ 3 chunks\n",
            "ğŸ“¥ Processing document 6/8...\n",
            "âœ… Document 6: 1 pages â†’ 2 chunks\n",
            "ğŸ“¥ Processing document 7/8...\n",
            "âœ… Document 7: 1 pages â†’ 3 chunks\n",
            "ğŸ“¥ Processing document 8/8...\n",
            "âŒ Document 8: Invalid Google Drive link format\n",
            "ğŸ‰ Auto-processing complete! 7/8 documents processed successfully\n",
            "ğŸ‰ Smart Zameen Dost with Auto PDF Processing is ready!\n",
            "\n",
            "âœ… Auto-processed 8 PDF documents from Google Drive\n",
            "ğŸ“Š Knowledge base includes default Pakistani agricultural data + your PDFs\n",
            "ğŸ” Multilingual search capability (English + Urdu)\n",
            "ğŸ¤ Voice input and output support\n",
            "ğŸŒ¡ï¸ Weather-based farming advice\n",
            "ğŸ’° Export market data with prices\n",
            "\n",
            "ğŸ“‹ To add more PDFs:\n",
            "  1. Add Google Drive share links to PREDEFINED_PDF_LINKS list\n",
            "  2. Restart the application\n",
            "  3. PDFs will be automatically processed and added to knowledge base\n",
            "\n",
            "ğŸš€ Application is ready to serve Pakistani farmers!\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://2d3e03b94f1f7f35b3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2d3e03b94f1f7f35b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}